{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import torch\n",
    "sys.path.append(\"../src/\")\n",
    "from run_pdebench_finetuning import get_args, get_model, build_pdebench_dataset\n",
    "from engine_for_pdebench_finetuning import get_targets, unnorm_batch\n",
    "import utils\n",
    "from einops import rearrange\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'pdebench_finetuning/k400_b/final_runs/k400_b_rand_128_0.08'\n",
    "# model_dir = 'pdebench_finetuning/k400_s/k400_s_turb_512_4chan_test_2'\n",
    "args_json = os.path.join(utils.get_ceph_dir(), model_dir, \"args.json\")\n",
    "args = utils.load_args(args_json)\n",
    "\n",
    "args.num_workers = 1\n",
    "args.device = 'cuda:0'\n",
    "args.checkpoint = os.path.join(model_dir, 'checkpoint-499')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\t\t compNS_rand\n",
      "Fields:\t\t\t ['Vx', 'Vy', 'density', 'pressure']\n",
      "Model:\t\t\t pretrain_videomae_base_patch16_128_4chan_18f\n",
      "Checkpoint:\t\t pdebench_finetuning/k400_b/final_runs/k400_b_rand_128_0.08/checkpoint-499\n",
      "Batch size:\t\t 1\n",
      "Number of workers:\t 1\n",
      "Mask type:\t\t last_frame\n",
      "Mask ratio:\t\t 0.9\n",
      "Norm target mode:\t last_frame\n",
      "Num frames:\t\t 18\n",
      "Device:\t\t\t cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset:\\t\\t\", args.data_set)\n",
    "print(\"Fields:\\t\\t\\t\", args.fields)\n",
    "print(\"Model:\\t\\t\\t\", args.model)\n",
    "print(\"Checkpoint:\\t\\t\", args.checkpoint)\n",
    "print(\"Batch size:\\t\\t\", args.batch_size)\n",
    "print(\"Number of workers:\\t\", args.num_workers)\n",
    "print(\"Mask type:\\t\\t\", args.mask_type)\n",
    "print(\"Mask ratio:\\t\\t\", args.mask_ratio)\n",
    "print(\"Norm target mode:\\t\", args.norm_target_mode)\n",
    "print(\"Num frames:\\t\\t\", args.num_frames)\n",
    "print(\"Device:\\t\\t\\t\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: pretrain_videomae_base_patch16_128_4chan_18f\n",
      "Position interpolate from 8x14x14 to 9x8x8\n",
      "Position interpolate from 8x14x14 to 9x8x8\n",
      "Adapting checkpoint for PDEBench\n",
      "Model loaded\n",
      "number of params: 94.80128 M\n",
      "Loading dataset file /mnt/home/gkrawezik/ceph/AI_DATASETS/PDEBench/2D/CFD/2D_Train_Rand/2D_CFD_Rand_M0.1_Eta0.01_Zeta0.01_periodic_128_Train.hdf5\n",
      "Raw dataset compNS_rand has 10000 samples of shape (128, 128) and 21 timesteps.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "# Load model\n",
    "model = get_model(args)\n",
    "model.to(device)\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Model loaded\")\n",
    "print('number of params: {} M'.format(n_parameters / 1e6))\n",
    "\n",
    "# Load dataset\n",
    "dataset = build_pdebench_dataset(args, set_type='test')\n",
    "data_norm_tf = dataset.transform.transform.transforms[1] # CustomNormalize object to unnormalize data\n",
    "dataset.timesteps = 21\n",
    "dataset.random_start = False\n",
    "\n",
    "# Data loader\n",
    "# sampler = torch.utils.data.RandomSampler(dataset)\n",
    "sampler = torch.utils.data.SequentialSampler(dataset)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, sampler=sampler,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "        worker_init_fn=utils.seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_ouput(output):\n",
    "    p0, p1, p2 = 2, args.patch_size[0], args.patch_size[1]\n",
    "    c = len(args.fields)\n",
    "    t = 1 # For last frame prediction\n",
    "    h, w = args.window_size[-2:]\n",
    "    output = rearrange(output, 'b (t h w) (p0 p1 p2 c) -> b t c p0 (h p1) (w p2)', p0=p0, p1=p1, p2=p2, c=c, t=t, h=h, w=w)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_mse = nn.MSELoss()\n",
    "\n",
    "def loss_func_nmse(input, target, mean_dim=None):\n",
    "    x = torch.mean(torch.square(input - target), dim=(-1, -2)) / torch.mean(torch.square(target) + 1e-7, dim=(-1, -2))\n",
    "    return x.mean(dim=mean_dim)\n",
    "\n",
    "def loss_func_nrmse(input, target, mean_dim=None):\n",
    "    x = torch.sqrt(torch.mean(torch.square(input - target), dim=(-1, -2)) / torch.mean(torch.square(target) + 1e-7, dim=(-1, -2)))\n",
    "    return x.mean(dim=mean_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1-step predictions\n",
    "\n",
    "# losses_mse = []\n",
    "# losses_nmse = []\n",
    "# losses_nrmse = []\n",
    "# losses_nrmse_per_field = []\n",
    "\n",
    "# model.eval()\n",
    "# for samples, masks in data_loader:\n",
    "#     samples = samples.to(device, non_blocking=True)\n",
    "#     samples_unnorm = data_norm_tf.unnormalize(samples.cpu())\n",
    "    \n",
    "#     bool_masked_pos = masks.to(device, non_blocking=True).flatten(1).to(torch.bool)\n",
    "\n",
    "#     p0, p1, p2 = 2, args.patch_size[0], args.patch_size[1]\n",
    "#     nchan = samples.shape[1]\n",
    "#     target = get_targets(samples, bool_masked_pos, args.norm_target_mode, p0=p0, p1=p1, p2=p2)\n",
    "#     target_unnorm = samples_unnorm[:, :, -2:, :, :].squeeze()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(samples, bool_masked_pos)\n",
    "#         outputs_unnorm = unnorm_batch(outputs,\n",
    "#                                       norm_mode=args.norm_target_mode,\n",
    "#                                       patch_size=(p0, p1, p2),\n",
    "#                                       context=samples,\n",
    "#                                       bool_masked_pos=bool_masked_pos)\n",
    "#         outputs_unnorm = data_norm_tf.unnormalize(rearrange_ouput(outputs_unnorm.cpu())).squeeze()\n",
    "\n",
    "#         # Only keep first frame\n",
    "#         outputs_unnorm = outputs_unnorm[:, :1]\n",
    "#         target_unnorm = target_unnorm[:, :1]\n",
    "\n",
    "#         loss_mse = loss_func_mse(input=outputs_unnorm, target=target_unnorm)\n",
    "#         loss_nmse = loss_func_nmse(input=outputs_unnorm, target=target_unnorm)\n",
    "#         loss_nrmse = loss_func_nrmse(input=outputs_unnorm, target=target_unnorm)\n",
    "#         loss_nrmse_per_field = loss_func_nrmse(input=outputs_unnorm, target=target_unnorm, mean_dim=1)\n",
    "        \n",
    "#         loss_mse_value = loss_mse.item()\n",
    "#         loss_nmse_value = loss_nmse.item()\n",
    "#         loss_nrmse_value = loss_nrmse.item()\n",
    "#         loss_nrmse_per_field_value = loss_nrmse_per_field.numpy()\n",
    "\n",
    "#         losses_mse.append(loss_mse_value)\n",
    "#         losses_nmse.append(loss_nmse_value)\n",
    "#         losses_nrmse.append(loss_nrmse_value)\n",
    "#         losses_nrmse_per_field.append(loss_nrmse_per_field_value)\n",
    "\n",
    "# losses_mse = np.array(losses_mse)\n",
    "# losses_nmse = np.array(losses_nmse)\n",
    "# losses_nrmse = np.array(losses_nrmse)\n",
    "# losses_nrmse_per_field = np.array(losses_nrmse_per_field)\n",
    "\n",
    "# print(f\"MSE: {np.mean(losses_mse):.4f} +/- {np.std(losses_mse):.4f}\")\n",
    "# print(f\"NMSE: {np.mean(losses_nmse):.4f} +/- {np.std(losses_nmse):.4f}\")\n",
    "# print(f\"NRMSE: {np.mean(losses_nrmse):.4f} +/- {np.std(losses_nrmse):.4f}\")\n",
    "# for i in range(len(args.fields)):\n",
    "#     print(f\"NRMSE {args.fields[i]}: {np.mean(losses_nrmse_per_field[:, i]):.4f} +/- {np.std(losses_nrmse_per_field[:, i]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 1 step: 0.0014 +/- 0.0020\n",
      "MSE 5 step: 0.0023 +/- 0.0030\n",
      "NMSE 1 step: 0.0023 +/- 0.0037\n",
      "NMSE 5 step: 0.0070 +/- 0.0182\n",
      "NRMSE 1 step: 0.0330 +/- 0.0163\n",
      "NRMSE 5 step: 0.0495 +/- 0.0291\n",
      "NRMSE 1 step Vx: 0.0576 +/- 0.0290\n",
      "NRMSE 5 step Vx: 0.0892 +/- 0.0543\n",
      "NRMSE 1 step Vy: 0.0605 +/- 0.0339\n",
      "NRMSE 5 step Vy: 0.0921 +/- 0.0576\n",
      "NRMSE 1 step density: 0.0122 +/- 0.0085\n",
      "NRMSE 5 step density: 0.0150 +/- 0.0097\n",
      "NRMSE 1 step pressure: 0.0015 +/- 0.0011\n",
      "NRMSE 5 step pressure: 0.0018 +/- 0.0015\n"
     ]
    }
   ],
   "source": [
    "## 1 and 5 step prediction\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "n_pred_frames = 5\n",
    "\n",
    "losses_mse_1_step = []\n",
    "losses_mse_5_step = []\n",
    "losses_nmse_1_step = []\n",
    "losses_nmse_5_step = []\n",
    "losses_nrmse_1_step = []\n",
    "losses_nrmse_5_step = []\n",
    "losses_nrmse_per_field_1_step = []\n",
    "losses_nrmse_per_field_5_step = []\n",
    "\n",
    "model.eval()\n",
    "for samples_base, masks in data_loader:\n",
    "    samples_base = samples_base.to(device, non_blocking=True)\n",
    "    samples_truth_unnorm = data_norm_tf.unnormalize(samples_base.cpu())\n",
    "\n",
    "    bool_masked_pos = masks.to(device, non_blocking=True).flatten(1).to(torch.bool)\n",
    "\n",
    "    p0, p1, p2 = 2, args.patch_size[0], args.patch_size[1]\n",
    "    h, w = args.input_size // p1, args.input_size // p2\n",
    "    nchan = samples_base.shape[1]\n",
    "    \n",
    "    # Extend samples_base\n",
    "    samples_base = torch.cat([samples_base,\n",
    "                              samples_base[:, :, -1:].repeat(1, 1, 1, 1, 1)], dim=2)\n",
    "\n",
    "    for i in range(args.num_frames, args.num_frames + n_pred_frames):\n",
    "        samples = samples_base[:, :, i-args.num_frames:i]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(samples, bool_masked_pos)\n",
    "            outputs_unnorm = unnorm_batch(outputs,\n",
    "                                        norm_mode=args.norm_target_mode,\n",
    "                                        patch_size=(p0, p1, p2),\n",
    "                                        context=samples,\n",
    "                                        bool_masked_pos=bool_masked_pos)\n",
    "            \n",
    "            recon_full = rearrange(samples, 'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)', p0=p0, p1=p1, p2=p2)\n",
    "            recon_full[bool_masked_pos] = outputs_unnorm.flatten(start_dim=0, end_dim=1)\n",
    "            recon_full = rearrange(recon_full, 'b (t h w) (p0 p1 p2 c) -> b c (t p0) (h p1) (w p2)', p0=p0, p1=p1, p2=p2, h=h, w=w)\n",
    "            samples_base[:, :, i-2] = recon_full[:, :, -2]\n",
    "        \n",
    "    samples_base = samples_base[:, :, :-1]\n",
    "\n",
    "    target_unnorm = samples_truth_unnorm[:, :, -n_pred_frames:].squeeze()\n",
    "    outputs_unnorm = samples_base[:, :, -n_pred_frames:]\n",
    "    outputs_unnorm = data_norm_tf.unnormalize(outputs_unnorm.cpu()).squeeze()\n",
    "    \n",
    "    loss_mse_1_step = loss_func_mse(input=outputs_unnorm[:, :1], target=target_unnorm[:, :1])\n",
    "    loss_mse_5_step = loss_func_mse(input=outputs_unnorm, target=target_unnorm)\n",
    "    loss_nmse_1_step = loss_func_nmse(input=outputs_unnorm[:, :1], target=target_unnorm[:, :1])\n",
    "    loss_nmse_5_step = loss_func_nmse(input=outputs_unnorm, target=target_unnorm)\n",
    "    loss_nrmse_1_step = loss_func_nrmse(input=outputs_unnorm[:, :1], target=target_unnorm[:, :1])\n",
    "    loss_nrmse_5_step = loss_func_nrmse(input=outputs_unnorm, target=target_unnorm)\n",
    "    loss_nrmse_per_field_1_step = loss_func_nrmse(input=outputs_unnorm[:, :1], target=target_unnorm[:, :1], mean_dim=1)\n",
    "    loss_nrmse_per_field_5_step = loss_func_nrmse(input=outputs_unnorm, target=target_unnorm, mean_dim=1)\n",
    "\n",
    "    loss_mse_1_step_value = loss_mse_1_step.item()\n",
    "    loss_mse_5_step_value = loss_mse_5_step.item()\n",
    "    loss_nmse_1_step_value = loss_nmse_1_step.item()\n",
    "    loss_nmse_5_step_value = loss_nmse_5_step.item()\n",
    "    loss_nrmse_1_step_value = loss_nrmse_1_step.item()\n",
    "    loss_nrmse_5_step_value = loss_nrmse_5_step.item()\n",
    "    loss_nrmse_per_field_1_step_value = loss_nrmse_per_field_1_step.numpy()\n",
    "    loss_nrmse_per_field_5_step_value = loss_nrmse_per_field_5_step.numpy()\n",
    "\n",
    "    losses_mse_1_step.append(loss_mse_1_step_value)\n",
    "    losses_mse_5_step.append(loss_mse_5_step_value)\n",
    "    losses_nmse_1_step.append(loss_nmse_1_step_value)\n",
    "    losses_nmse_5_step.append(loss_nmse_5_step_value)\n",
    "    losses_nrmse_1_step.append(loss_nrmse_1_step_value)\n",
    "    losses_nrmse_5_step.append(loss_nrmse_5_step_value)\n",
    "    losses_nrmse_per_field_1_step.append(loss_nrmse_per_field_1_step_value)\n",
    "    losses_nrmse_per_field_5_step.append(loss_nrmse_per_field_5_step_value)\n",
    "\n",
    "losses_mse_1_step = np.array(losses_mse_1_step)\n",
    "losses_mse_5_step = np.array(losses_mse_5_step)\n",
    "losses_nmse_1_step = np.array(losses_nmse_1_step)\n",
    "losses_nmse_5_step = np.array(losses_nmse_5_step)\n",
    "losses_nrmse_1_step = np.array(losses_nrmse_1_step)\n",
    "losses_nrmse_5_step = np.array(losses_nrmse_5_step)\n",
    "losses_nrmse_per_field_1_step = np.array(losses_nrmse_per_field_1_step)\n",
    "losses_nrmse_per_field_5_step = np.array(losses_nrmse_per_field_5_step)\n",
    "\n",
    "print(f\"MSE 1 step: {np.mean(losses_mse_1_step):.4f} +/- {np.std(losses_mse_1_step):.4f}\")\n",
    "print(f\"MSE 5 step: {np.mean(losses_mse_5_step):.4f} +/- {np.std(losses_mse_5_step):.4f}\")\n",
    "print(f\"NMSE 1 step: {np.mean(losses_nmse_1_step):.4f} +/- {np.std(losses_nmse_1_step):.4f}\")\n",
    "print(f\"NMSE 5 step: {np.mean(losses_nmse_5_step):.4f} +/- {np.std(losses_nmse_5_step):.4f}\")\n",
    "print(f\"NRMSE 1 step: {np.mean(losses_nrmse_1_step):.4f} +/- {np.std(losses_nrmse_1_step):.4f}\")\n",
    "print(f\"NRMSE 5 step: {np.mean(losses_nrmse_5_step):.4f} +/- {np.std(losses_nrmse_5_step):.4f}\")\n",
    "for i in range(len(args.fields)):\n",
    "    print(f\"NRMSE 1 step {args.fields[i]}: {np.mean(losses_nrmse_per_field_1_step[:, i]):.4f} +/- {np.std(losses_nrmse_per_field_1_step[:, i]):.4f}\")\n",
    "    print(f\"NRMSE 5 step {args.fields[i]}: {np.mean(losses_nrmse_per_field_5_step[:, i]):.4f} +/- {np.std(losses_nrmse_per_field_5_step[:, i]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('videomae')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78f739402b6efd8f9c4cbadeb4bb1722c2c6b5c02615425ba8de04008e590e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
