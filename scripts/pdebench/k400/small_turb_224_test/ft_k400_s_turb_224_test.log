Finetuning VideoMAE on PDEBENCH dataset
Python path: /mnt/home/bregaldosaintblancard/venvs/videomae/bin/python3
Output dir: /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
Finetuning model k400_vit-s on compNS_turb dataset
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
| distributed init (rank 0): env://, gpu 0
nccl env:// 4 0
| distributed init (rank 1): env://, gpu 1
nccl env:// 4 1
| distributed init (rank 3): env://, gpu 3
nccl env:// 4 3
| distributed init (rank 2): env://, gpu 2
nccl env:// 4 2
Distributed mode initialized
Namespace(batch_size=4, epochs=100, save_ckpt_freq=25, model='pretrain_videomae_small_patch16_224', checkpoint='k400_vit-s', decoder_depth=4, mask_type='last_frame', mask_ratio=0.9, input_size=224, drop_path=0.0, norm_target_mode='last_frame', wb_project='videomae_finetuning', wb_group='Calibration', wb_name='k400_s_turb_224_test', opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.001, warmup_lr=1e-06, min_lr=1e-05, warmup_epochs=5, warmup_steps=-1, use_checkpoint=False, color_jitter=0.0, train_interpolation='bicubic', data_set='compNS_turb', fields=['Vx', 'Vy', 'density'], data_tmp_copy=False, imagenet_default_mean_and_std=True, num_frames=16, sampling_rate=1, output_dir='/mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/', log_dir='/mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/', device='cuda', seed=0, resume='', auto_resume=True, start_epoch=0, num_workers=4, pin_mem=True, world_size=4, local_rank=-1, dist_on_itp=False, dist_url='env://', rank=0, gpu=0, distributed=True, dist_backend='nccl')
wandb: Currently logged in as: bregaldo (flatiron-scipt). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.8
wandb: Run data is saved locally in /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/wandb/run-20230902_123755-gsc5kaof
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run k400_s_turb_224_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/flatiron-scipt/videomae_finetuning
wandb: üöÄ View run at https://wandb.ai/flatiron-scipt/videomae_finetuning/runs/gsc5kaof
Creating model: pretrain_videomae_small_patch16_224
Model loaded
Patch size = (16, 16)
output_dir= /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
log_dir /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
Raw dataset compNS_turb has 1000 samples of shape (512, 512) and 21 timesteps.
Raw dataset compNS_turb has 1000 samples of shape (512, 512) and 21 timesteps.
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ffce40770d0>
Sampler_test = <torch.utils.data.distributed.DistributedSampler object at 0x7ffce4077070>
Model = PretrainVisionTransformer(
  (encoder): PretrainVisionTransformerEncoder(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 384, kernel_size=(2, 16, 16), stride=(2, 16, 16))
    )
    (blocks): ModuleList(
      (0-11): 12 x Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (head): Identity()
  )
  (decoder): PretrainVisionTransformerDecoder(
    (blocks): ModuleList(
      (0-3): 4 x Block(
        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=192, out_features=576, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
    (head): Linear(in_features=192, out_features=1536, bias=True)
  )
  (encoder_to_decoder): Linear(in_features=384, out_features=192, bias=False)
)
number of params: 24.029376 M
LR = 0.00006250
Batch size = 16
Number of training steps = 50
Number of training examples per epoch = 800
Param groups = {
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "mask_token",
      "encoder.patch_embed.proj.bias",
      "encoder.blocks.0.norm1.weight",
      "encoder.blocks.0.norm1.bias",
      "encoder.blocks.0.attn.q_bias",
      "encoder.blocks.0.attn.v_bias",
      "encoder.blocks.0.attn.proj.bias",
      "encoder.blocks.0.norm2.weight",
      "encoder.blocks.0.norm2.bias",
      "encoder.blocks.0.mlp.fc1.bias",
      "encoder.blocks.0.mlp.fc2.bias",
      "encoder.blocks.1.norm1.weight",
      "encoder.blocks.1.norm1.bias",
      "encoder.blocks.1.attn.q_bias",
      "encoder.blocks.1.attn.v_bias",
      "encoder.blocks.1.attn.proj.bias",
      "encoder.blocks.1.norm2.weight",
      "encoder.blocks.1.norm2.bias",
      "encoder.blocks.1.mlp.fc1.bias",
      "encoder.blocks.1.mlp.fc2.bias",
      "encoder.blocks.2.norm1.weight",
      "encoder.blocks.2.norm1.bias",
      "encoder.blocks.2.attn.q_bias",
      "encoder.blocks.2.attn.v_bias",
      "encoder.blocks.2.attn.proj.bias",
      "encoder.blocks.2.norm2.weight",
      "encoder.blocks.2.norm2.bias",
      "encoder.blocks.2.mlp.fc1.bias",
      "encoder.blocks.2.mlp.fc2.bias",
      "encoder.blocks.3.norm1.weight",
      "encoder.blocks.3.norm1.bias",
      "encoder.blocks.3.attn.q_bias",
      "encoder.blocks.3.attn.v_bias",
      "encoder.blocks.3.attn.proj.bias",
      "encoder.blocks.3.norm2.weight",
      "encoder.blocks.3.norm2.bias",
      "encoder.blocks.3.mlp.fc1.bias",
      "encoder.blocks.3.mlp.fc2.bias",
      "encoder.blocks.4.norm1.weight",
      "encoder.blocks.4.norm1.bias",
      "encoder.blocks.4.attn.q_bias",
      "encoder.blocks.4.attn.v_bias",
      "encoder.blocks.4.attn.proj.bias",
      "encoder.blocks.4.norm2.weight",
      "encoder.blocks.4.norm2.bias",
      "encoder.blocks.4.mlp.fc1.bias",
      "encoder.blocks.4.mlp.fc2.bias",
      "encoder.blocks.5.norm1.weight",
      "encoder.blocks.5.norm1.bias",
      "encoder.blocks.5.attn.q_bias",
      "encoder.blocks.5.attn.v_bias",
      "encoder.blocks.5.attn.proj.bias",
      "encoder.blocks.5.norm2.weight",
      "encoder.blocks.5.norm2.bias",
      "encoder.blocks.5.mlp.fc1.bias",
      "encoder.blocks.5.mlp.fc2.bias",
      "encoder.blocks.6.norm1.weight",
      "encoder.blocks.6.norm1.bias",
      "encoder.blocks.6.attn.q_bias",
      "encoder.blocks.6.attn.v_bias",
      "encoder.blocks.6.attn.proj.bias",
      "encoder.blocks.6.norm2.weight",
      "encoder.blocks.6.norm2.bias",
      "encoder.blocks.6.mlp.fc1.bias",
      "encoder.blocks.6.mlp.fc2.bias",
      "encoder.blocks.7.norm1.weight",
      "encoder.blocks.7.norm1.bias",
      "encoder.blocks.7.attn.q_bias",
      "encoder.blocks.7.attn.v_bias",
      "encoder.blocks.7.attn.proj.bias",
      "encoder.blocks.7.norm2.weight",
      "encoder.blocks.7.norm2.bias",
      "encoder.blocks.7.mlp.fc1.bias",
      "encoder.blocks.7.mlp.fc2.bias",
      "encoder.blocks.8.norm1.weight",
      "encoder.blocks.8.norm1.bias",
      "encoder.blocks.8.attn.q_bias",
      "encoder.blocks.8.attn.v_bias",
      "encoder.blocks.8.attn.proj.bias",
      "encoder.blocks.8.norm2.weight",
      "encoder.blocks.8.norm2.bias",
      "encoder.blocks.8.mlp.fc1.bias",
      "encoder.blocks.8.mlp.fc2.bias",
      "encoder.blocks.9.norm1.weight",
      "encoder.blocks.9.norm1.bias",
      "encoder.blocks.9.attn.q_bias",
      "encoder.blocks.9.attn.v_bias",
      "encoder.blocks.9.attn.proj.bias",
      "encoder.blocks.9.norm2.weight",
      "encoder.blocks.9.norm2.bias",
      "encoder.blocks.9.mlp.fc1.bias",
      "encoder.blocks.9.mlp.fc2.bias",
      "encoder.blocks.10.norm1.weight",
      "encoder.blocks.10.norm1.bias",
      "encoder.blocks.10.attn.q_bias",
      "encoder.blocks.10.attn.v_bias",
      "encoder.blocks.10.attn.proj.bias",
      "encoder.blocks.10.norm2.weight",
      "encoder.blocks.10.norm2.bias",
      "encoder.blocks.10.mlp.fc1.bias",
      "encoder.blocks.10.mlp.fc2.bias",
      "encoder.blocks.11.norm1.weight",
      "encoder.blocks.11.norm1.bias",
      "encoder.blocks.11.attn.q_bias",
      "encoder.blocks.11.attn.v_bias",
      "encoder.blocks.11.attn.proj.bias",
      "encoder.blocks.11.norm2.weight",
      "encoder.blocks.11.norm2.bias",
      "encoder.blocks.11.mlp.fc1.bias",
      "encoder.blocks.11.mlp.fc2.bias",
      "encoder.norm.weight",
      "encoder.norm.bias",
      "decoder.blocks.0.norm1.weight",
      "decoder.blocks.0.norm1.bias",
      "decoder.blocks.0.attn.q_bias",
      "decoder.blocks.0.attn.v_bias",
      "decoder.blocks.0.attn.proj.bias",
      "decoder.blocks.0.norm2.weight",
      "decoder.blocks.0.norm2.bias",
      "decoder.blocks.0.mlp.fc1.bias",
      "decoder.blocks.0.mlp.fc2.bias",
      "decoder.blocks.1.norm1.weight",
      "decoder.blocks.1.norm1.bias",
      "decoder.blocks.1.attn.q_bias",
      "decoder.blocks.1.attn.v_bias",
      "decoder.blocks.1.attn.proj.bias",
      "decoder.blocks.1.norm2.weight",
      "decoder.blocks.1.norm2.bias",
      "decoder.blocks.1.mlp.fc1.bias",
      "decoder.blocks.1.mlp.fc2.bias",
      "decoder.blocks.2.norm1.weight",
      "decoder.blocks.2.norm1.bias",
      "decoder.blocks.2.attn.q_bias",
      "decoder.blocks.2.attn.v_bias",
      "decoder.blocks.2.attn.proj.bias",
      "decoder.blocks.2.norm2.weight",
      "decoder.blocks.2.norm2.bias",
      "decoder.blocks.2.mlp.fc1.bias",
      "decoder.blocks.2.mlp.fc2.bias",
      "decoder.blocks.3.norm1.weight",
      "decoder.blocks.3.norm1.bias",
      "decoder.blocks.3.attn.q_bias",
      "decoder.blocks.3.attn.v_bias",
      "decoder.blocks.3.attn.proj.bias",
      "decoder.blocks.3.norm2.weight",
      "decoder.blocks.3.norm2.bias",
      "decoder.blocks.3.mlp.fc1.bias",
      "decoder.blocks.3.mlp.fc2.bias",
      "decoder.norm.weight",
      "decoder.norm.bias",
      "decoder.head.bias"
    ],
    "lr_scale": 1.0
  },
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "encoder.patch_embed.proj.weight",
      "encoder.blocks.0.attn.qkv.weight",
      "encoder.blocks.0.attn.proj.weight",
      "encoder.blocks.0.mlp.fc1.weight",
      "encoder.blocks.0.mlp.fc2.weight",
      "encoder.blocks.1.attn.qkv.weight",
      "encoder.blocks.1.attn.proj.weight",
      "encoder.blocks.1.mlp.fc1.weight",
      "encoder.blocks.1.mlp.fc2.weight",
      "encoder.blocks.2.attn.qkv.weight",
      "encoder.blocks.2.attn.proj.weight",
      "encoder.blocks.2.mlp.fc1.weight",
      "encoder.blocks.2.mlp.fc2.weight",
      "encoder.blocks.3.attn.qkv.weight",
      "encoder.blocks.3.attn.proj.weight",
      "encoder.blocks.3.mlp.fc1.weight",
      "encoder.blocks.3.mlp.fc2.weight",
      "encoder.blocks.4.attn.qkv.weight",
      "encoder.blocks.4.attn.proj.weight",
      "encoder.blocks.4.mlp.fc1.weight",
      "encoder.blocks.4.mlp.fc2.weight",
      "encoder.blocks.5.attn.qkv.weight",
      "encoder.blocks.5.attn.proj.weight",
      "encoder.blocks.5.mlp.fc1.weight",
      "encoder.blocks.5.mlp.fc2.weight",
      "encoder.blocks.6.attn.qkv.weight",
      "encoder.blocks.6.attn.proj.weight",
      "encoder.blocks.6.mlp.fc1.weight",
      "encoder.blocks.6.mlp.fc2.weight",
      "encoder.blocks.7.attn.qkv.weight",
      "encoder.blocks.7.attn.proj.weight",
      "encoder.blocks.7.mlp.fc1.weight",
      "encoder.blocks.7.mlp.fc2.weight",
      "encoder.blocks.8.attn.qkv.weight",
      "encoder.blocks.8.attn.proj.weight",
      "encoder.blocks.8.mlp.fc1.weight",
      "encoder.blocks.8.mlp.fc2.weight",
      "encoder.blocks.9.attn.qkv.weight",
      "encoder.blocks.9.attn.proj.weight",
      "encoder.blocks.9.mlp.fc1.weight",
      "encoder.blocks.9.mlp.fc2.weight",
      "encoder.blocks.10.attn.qkv.weight",
      "encoder.blocks.10.attn.proj.weight",
      "encoder.blocks.10.mlp.fc1.weight",
      "encoder.blocks.10.mlp.fc2.weight",
      "encoder.blocks.11.attn.qkv.weight",
      "encoder.blocks.11.attn.proj.weight",
      "encoder.blocks.11.mlp.fc1.weight",
      "encoder.blocks.11.mlp.fc2.weight",
      "decoder.blocks.0.attn.qkv.weight",
      "decoder.blocks.0.attn.proj.weight",
      "decoder.blocks.0.mlp.fc1.weight",
      "decoder.blocks.0.mlp.fc2.weight",
      "decoder.blocks.1.attn.qkv.weight",
      "decoder.blocks.1.attn.proj.weight",
      "decoder.blocks.1.mlp.fc1.weight",
      "decoder.blocks.1.mlp.fc2.weight",
      "decoder.blocks.2.attn.qkv.weight",
      "decoder.blocks.2.attn.proj.weight",
      "decoder.blocks.2.mlp.fc1.weight",
      "decoder.blocks.2.mlp.fc2.weight",
      "decoder.blocks.3.attn.qkv.weight",
      "decoder.blocks.3.attn.proj.weight",
      "decoder.blocks.3.mlp.fc1.weight",
      "decoder.blocks.3.mlp.fc2.weight",
      "decoder.head.weight",
      "encoder_to_decoder.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 6.25e-05, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR & WD scheduler!
Set warmup steps = 250
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
Auto resume checkpoint: 
Start training for 100 epochs
NaN or Inf found in input tensor.
Epoch: [0] (train)  [ 0/50]  eta: 0:02:33  lr: 0.000000  min_lr: 0.000000  loss: 3.2990 (3.2990)  loss_scale: 32768.0000 (32768.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: nan (nan)  time: 3.0613  data: 2.1270  max mem: 5404
NaN or Inf found in input tensor.
NaN or Inf found in input tensor.
NaN or Inf found in input tensor.
NaN or Inf found in input tensor.
NaN or Inf found in input tensor.
NaN or Inf found in input tensor.
Epoch: [0] (train)  [10/50]  eta: 0:00:16  lr: 0.000003  min_lr: 0.000003  loss: 2.6415 (2.8766)  loss_scale: 2048.0000 (6237.0909)  weight_decay: 0.0500 (0.0500)  grad_norm: nan (nan)  time: 0.4005  data: 0.1950  max mem: 5587
Epoch: [0] (train)  [20/50]  eta: 0:00:08  lr: 0.000005  min_lr: 0.000005  loss: 2.6415 (2.9742)  loss_scale: 512.0000 (3510.8571)  weight_decay: 0.0500 (0.0500)  grad_norm: nan (nan)  time: 0.1397  data: 0.0026  max mem: 5588
Epoch: [0] (train)  [30/50]  eta: 0:00:04  lr: 0.000008  min_lr: 0.000008  loss: 2.8607 (3.0177)  loss_scale: 512.0000 (2543.4839)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2745 (nan)  time: 0.1484  data: 0.0051  max mem: 5588
Epoch: [0] (train)  [40/50]  eta: 0:00:02  lr: 0.000010  min_lr: 0.000010  loss: 2.7630 (2.9955)  loss_scale: 512.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3623 (nan)  time: 0.1552  data: 0.0079  max mem: 5588
Epoch: [0] (train)  [49/50]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000012  loss: 2.7630 (2.9977)  loss_scale: 512.0000 (1771.5200)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4454 (nan)  time: 0.1253  data: 0.0058  max mem: 5588
Epoch: [0] (train) Total time: 0:00:09 (0.1964 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000012  loss: 2.7630 (2.9223)  loss_scale: 512.0000 (1771.5200)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4454 (nan)
Epoch: [0] (test)  [0/6]  eta: 0:00:02  loss: 2.3119 (2.3119)  time: 0.4023  data: 0.3745  max mem: 5588
Epoch: [0] (test)  [5/6]  eta: 0:00:00  loss: 2.7766 (2.7328)  time: 0.1475  data: 0.1148  max mem: 5588
Epoch: [0] (test) Total time: 0:00:00 (0.1558 s / it)
Averaged stats: loss: 2.7766 (2.7802)
Epoch: [1] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000013  min_lr: 0.000013  loss: 2.5539 (2.5539)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.4892 (15.4892)  time: 0.9317  data: 0.6951  max mem: 5588
Epoch: [1] (train)  [10/50]  eta: 0:00:09  lr: 0.000015  min_lr: 0.000015  loss: 2.5539 (2.7635)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 23.4239 (22.9936)  time: 0.2421  data: 0.0662  max mem: 5588
Epoch: [1] (train)  [20/50]  eta: 0:00:06  lr: 0.000018  min_lr: 0.000018  loss: 2.8617 (2.8185)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 19.0201 (20.8970)  time: 0.1672  data: 0.0027  max mem: 5589
Epoch: [1] (train)  [30/50]  eta: 0:00:03  lr: 0.000020  min_lr: 0.000020  loss: 2.8598 (2.8657)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.8798 (19.7243)  time: 0.1589  data: 0.0014  max mem: 5589
Epoch: [1] (train)  [40/50]  eta: 0:00:01  lr: 0.000023  min_lr: 0.000023  loss: 2.6858 (2.8306)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.1129 (18.6738)  time: 0.1653  data: 0.0013  max mem: 5589
Epoch: [1] (train)  [49/50]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000025  loss: 2.5104 (2.7617)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8846 (17.6275)  time: 0.1441  data: 0.0011  max mem: 5589
Epoch: [1] (train) Total time: 0:00:08 (0.1703 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000025  loss: 2.5104 (2.7641)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8846 (17.6275)
Epoch: [1] (test)  [0/6]  eta: 0:00:02  loss: 2.4061 (2.4061)  time: 0.3604  data: 0.3278  max mem: 5589
Epoch: [1] (test)  [5/6]  eta: 0:00:00  loss: 2.4061 (2.6077)  time: 0.1338  data: 0.1040  max mem: 5589
Epoch: [1] (test) Total time: 0:00:00 (0.1441 s / it)
Averaged stats: loss: 2.4061 (2.6770)
Epoch: [2] (train)  [ 0/50]  eta: 0:00:48  lr: 0.000025  min_lr: 0.000025  loss: 2.7813 (2.7813)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1069 (12.1069)  time: 0.9687  data: 0.5235  max mem: 5589
Epoch: [2] (train)  [10/50]  eta: 0:00:09  lr: 0.000028  min_lr: 0.000028  loss: 2.3827 (2.4881)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.2084 (11.8642)  time: 0.2479  data: 0.0516  max mem: 5589
Epoch: [2] (train)  [20/50]  eta: 0:00:05  lr: 0.000030  min_lr: 0.000030  loss: 2.3152 (2.4771)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.2934 (14.5186)  time: 0.1553  data: 0.0025  max mem: 5589
Epoch: [2] (train)  [30/50]  eta: 0:00:03  lr: 0.000033  min_lr: 0.000033  loss: 2.3675 (2.4296)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 15.1338 (14.8791)  time: 0.1507  data: 0.0031  max mem: 5589
Epoch: [2] (train)  [40/50]  eta: 0:00:01  lr: 0.000035  min_lr: 0.000035  loss: 2.4676 (2.4640)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.0430 (14.9705)  time: 0.1735  data: 0.0042  max mem: 5589
Epoch: [2] (train)  [49/50]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000037  loss: 2.5348 (2.4460)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9066 (14.9322)  time: 0.1393  data: 0.0016  max mem: 5589
Epoch: [2] (train) Total time: 0:00:08 (0.1691 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000037  loss: 2.5348 (2.5507)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9066 (14.9322)
Epoch: [2] (test)  [0/6]  eta: 0:00:03  loss: 2.4672 (2.4672)  time: 0.6645  data: 0.6424  max mem: 5589
Epoch: [2] (test)  [5/6]  eta: 0:00:00  loss: 2.4672 (2.6254)  time: 0.1646  data: 0.1416  max mem: 5589
Epoch: [2] (test) Total time: 0:00:01 (0.1725 s / it)
Averaged stats: loss: 2.4672 (2.5901)
Epoch: [3] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000038  min_lr: 0.000038  loss: 2.2291 (2.2291)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3330 (8.3330)  time: 0.9292  data: 0.3658  max mem: 5589
Epoch: [3] (train)  [10/50]  eta: 0:00:09  lr: 0.000040  min_lr: 0.000040  loss: 2.3881 (2.4912)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.7995 (15.1713)  time: 0.2409  data: 0.0408  max mem: 5589
Epoch: [3] (train)  [20/50]  eta: 0:00:06  lr: 0.000043  min_lr: 0.000043  loss: 2.3881 (2.5059)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.3560 (15.4017)  time: 0.1685  data: 0.0073  max mem: 5589
Epoch: [3] (train)  [30/50]  eta: 0:00:03  lr: 0.000045  min_lr: 0.000045  loss: 2.3478 (2.4663)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9282 (15.4296)  time: 0.1531  data: 0.0049  max mem: 5589
Epoch: [3] (train)  [40/50]  eta: 0:00:01  lr: 0.000048  min_lr: 0.000048  loss: 2.2152 (2.3984)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0317 (14.5806)  time: 0.1604  data: 0.0035  max mem: 5589
Epoch: [3] (train)  [49/50]  eta: 0:00:00  lr: 0.000050  min_lr: 0.000050  loss: 2.1134 (2.3774)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0297 (14.4225)  time: 0.1326  data: 0.0020  max mem: 5589
Epoch: [3] (train) Total time: 0:00:08 (0.1667 s / it)
Averaged stats: lr: 0.000050  min_lr: 0.000050  loss: 2.1134 (2.3334)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0297 (14.4225)
Epoch: [3] (test)  [0/6]  eta: 0:00:03  loss: 1.8439 (1.8439)  time: 0.5795  data: 0.5575  max mem: 5589
Epoch: [3] (test)  [5/6]  eta: 0:00:00  loss: 2.0079 (2.1540)  time: 0.1451  data: 0.1222  max mem: 5589
Epoch: [3] (test) Total time: 0:00:00 (0.1528 s / it)
Averaged stats: loss: 2.0079 (2.2126)
Epoch: [4] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000050  min_lr: 0.000050  loss: 1.9194 (1.9194)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9284 (11.9284)  time: 0.9863  data: 0.7793  max mem: 5589
Epoch: [4] (train)  [10/50]  eta: 0:00:09  lr: 0.000053  min_lr: 0.000053  loss: 1.9740 (2.1609)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.9284 (12.1587)  time: 0.2333  data: 0.0734  max mem: 5589
Epoch: [4] (train)  [20/50]  eta: 0:00:05  lr: 0.000055  min_lr: 0.000055  loss: 2.1029 (2.1718)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9970 (13.6031)  time: 0.1573  data: 0.0043  max mem: 5589
Epoch: [4] (train)  [30/50]  eta: 0:00:03  lr: 0.000058  min_lr: 0.000058  loss: 2.2111 (2.1686)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.9970 (13.6938)  time: 0.1529  data: 0.0039  max mem: 5589
Epoch: [4] (train)  [40/50]  eta: 0:00:01  lr: 0.000060  min_lr: 0.000060  loss: 2.0183 (2.1305)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0813 (12.9585)  time: 0.1619  data: 0.0020  max mem: 5589
Epoch: [4] (train)  [49/50]  eta: 0:00:00  lr: 0.000063  min_lr: 0.000063  loss: 1.9841 (2.1236)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6021 (12.5781)  time: 0.1306  data: 0.0011  max mem: 5589
Epoch: [4] (train) Total time: 0:00:08 (0.1643 s / it)
Averaged stats: lr: 0.000063  min_lr: 0.000063  loss: 1.9841 (2.2113)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6021 (12.5781)
Epoch: [4] (test)  [0/6]  eta: 0:00:03  loss: 2.0301 (2.0301)  time: 0.6536  data: 0.6078  max mem: 5589
Epoch: [4] (test)  [5/6]  eta: 0:00:00  loss: 1.9458 (2.0683)  time: 0.1597  data: 0.1256  max mem: 5589
Epoch: [4] (test) Total time: 0:00:00 (0.1662 s / it)
Averaged stats: loss: 1.9458 (2.2812)
Epoch: [5] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000063  min_lr: 0.000063  loss: 2.0749 (2.0749)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8629 (7.8629)  time: 1.0352  data: 0.9044  max mem: 5589
Epoch: [5] (train)  [10/50]  eta: 0:00:09  lr: 0.000062  min_lr: 0.000062  loss: 2.0749 (2.1471)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3035 (11.7689)  time: 0.2286  data: 0.0845  max mem: 5589
Epoch: [5] (train)  [20/50]  eta: 0:00:05  lr: 0.000062  min_lr: 0.000062  loss: 2.0567 (2.0937)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5320 (10.8342)  time: 0.1551  data: 0.0021  max mem: 5589
Epoch: [5] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 2.0392 (2.0984)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2541 (10.9655)  time: 0.1632  data: 0.0022  max mem: 5589
Epoch: [5] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 2.0442 (2.1139)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4142 (10.6453)  time: 0.1494  data: 0.0025  max mem: 5590
Epoch: [5] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.9770 (2.0860)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8752 (10.2048)  time: 0.1215  data: 0.0015  max mem: 5590
Epoch: [5] (train) Total time: 0:00:08 (0.1636 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.9770 (2.1178)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8752 (10.2048)
Epoch: [5] (test)  [0/6]  eta: 0:00:03  loss: 1.7307 (1.7307)  time: 0.5670  data: 0.5284  max mem: 5590
Epoch: [5] (test)  [5/6]  eta: 0:00:00  loss: 2.0272 (2.0357)  time: 0.1466  data: 0.1144  max mem: 5590
Epoch: [5] (test) Total time: 0:00:00 (0.1561 s / it)
Averaged stats: loss: 2.0272 (2.1791)
Epoch: [6] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000062  min_lr: 0.000062  loss: 2.0559 (2.0559)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4345 (10.4345)  time: 0.9396  data: 0.8225  max mem: 5590
Epoch: [6] (train)  [10/50]  eta: 0:00:08  lr: 0.000062  min_lr: 0.000062  loss: 1.9162 (2.0089)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 16.6356 (17.9382)  time: 0.2176  data: 0.0771  max mem: 5590
Epoch: [6] (train)  [20/50]  eta: 0:00:05  lr: 0.000062  min_lr: 0.000062  loss: 1.8947 (1.9982)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.5795 (13.9460)  time: 0.1529  data: 0.0044  max mem: 5590
Epoch: [6] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 1.9218 (1.9959)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7525 (12.4151)  time: 0.1697  data: 0.0072  max mem: 5591
Epoch: [6] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 1.9705 (1.9867)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7525 (11.9000)  time: 0.1721  data: 0.0141  max mem: 5591
Epoch: [6] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.9799 (2.0021)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8684 (11.8889)  time: 0.1293  data: 0.0102  max mem: 5591
Epoch: [6] (train) Total time: 0:00:08 (0.1671 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.9799 (2.0776)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8684 (11.8889)
Epoch: [6] (test)  [0/6]  eta: 0:00:02  loss: 2.1194 (2.1194)  time: 0.3540  data: 0.3301  max mem: 5591
Epoch: [6] (test)  [5/6]  eta: 0:00:00  loss: 1.8807 (1.8991)  time: 0.1498  data: 0.1277  max mem: 5591
Epoch: [6] (test) Total time: 0:00:00 (0.1552 s / it)
Averaged stats: loss: 1.8807 (2.0167)
Epoch: [7] (train)  [ 0/50]  eta: 0:00:42  lr: 0.000062  min_lr: 0.000062  loss: 1.8254 (1.8254)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9418 (8.9418)  time: 0.8568  data: 0.6888  max mem: 5591
Epoch: [7] (train)  [10/50]  eta: 0:00:09  lr: 0.000062  min_lr: 0.000062  loss: 2.0231 (1.9922)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.7249 (11.0647)  time: 0.2494  data: 0.0663  max mem: 5591
Epoch: [7] (train)  [20/50]  eta: 0:00:06  lr: 0.000062  min_lr: 0.000062  loss: 2.0560 (2.0181)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1357 (11.4435)  time: 0.1753  data: 0.0047  max mem: 5591
Epoch: [7] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 2.0560 (2.0329)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0864 (11.1196)  time: 0.1437  data: 0.0040  max mem: 5591
Epoch: [7] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 2.0575 (2.0342)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3659 (11.2239)  time: 0.1488  data: 0.0041  max mem: 5591
Epoch: [7] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.9037 (1.9941)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3659 (11.1896)  time: 0.1335  data: 0.0038  max mem: 5591
Epoch: [7] (train) Total time: 0:00:08 (0.1642 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.9037 (1.9988)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3659 (11.1896)
Epoch: [7] (test)  [0/6]  eta: 0:00:03  loss: 1.8207 (1.8207)  time: 0.6579  data: 0.6186  max mem: 5591
Epoch: [7] (test)  [5/6]  eta: 0:00:00  loss: 1.6326 (1.7437)  time: 0.1622  data: 0.1300  max mem: 5591
Epoch: [7] (test) Total time: 0:00:01 (0.1698 s / it)
Averaged stats: loss: 1.6326 (1.8761)
Epoch: [8] (train)  [ 0/50]  eta: 0:00:43  lr: 0.000062  min_lr: 0.000062  loss: 1.9855 (1.9855)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8982 (6.8982)  time: 0.8622  data: 0.4643  max mem: 5591
Epoch: [8] (train)  [10/50]  eta: 0:00:08  lr: 0.000062  min_lr: 0.000062  loss: 1.9201 (1.9458)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1573 (12.4149)  time: 0.2232  data: 0.0452  max mem: 5591
Epoch: [8] (train)  [20/50]  eta: 0:00:05  lr: 0.000062  min_lr: 0.000062  loss: 1.9201 (1.9230)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7728 (11.7049)  time: 0.1647  data: 0.0041  max mem: 5591
Epoch: [8] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 1.7288 (1.8684)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3708 (11.1772)  time: 0.1741  data: 0.0038  max mem: 5591
Epoch: [8] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 1.7579 (1.8947)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9495 (11.2652)  time: 0.1616  data: 0.0034  max mem: 5591
Epoch: [8] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.8467 (1.8958)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8350 (11.6120)  time: 0.1220  data: 0.0023  max mem: 5591
Epoch: [8] (train) Total time: 0:00:08 (0.1669 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.8467 (1.9338)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8350 (11.6120)
Epoch: [8] (test)  [0/6]  eta: 0:00:02  loss: 1.9969 (1.9969)  time: 0.3855  data: 0.3549  max mem: 5591
Epoch: [8] (test)  [5/6]  eta: 0:00:00  loss: 1.9853 (1.9176)  time: 0.1285  data: 0.1023  max mem: 5591
Epoch: [8] (test) Total time: 0:00:00 (0.1379 s / it)
Averaged stats: loss: 1.9853 (1.9395)
Epoch: [9] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000062  min_lr: 0.000062  loss: 1.6433 (1.6433)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1995 (6.1995)  time: 0.9002  data: 0.7857  max mem: 5591
Epoch: [9] (train)  [10/50]  eta: 0:00:09  lr: 0.000062  min_lr: 0.000062  loss: 1.7993 (1.7030)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0482 (9.2640)  time: 0.2314  data: 0.0818  max mem: 5591
Epoch: [9] (train)  [20/50]  eta: 0:00:06  lr: 0.000062  min_lr: 0.000062  loss: 1.8156 (1.8735)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3955 (11.3728)  time: 0.1685  data: 0.0080  max mem: 5591
Epoch: [9] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 1.8632 (1.8919)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.3763 (11.9325)  time: 0.1537  data: 0.0033  max mem: 5591
Epoch: [9] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 1.7796 (1.8381)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7451 (11.2192)  time: 0.1540  data: 0.0020  max mem: 5591
Epoch: [9] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.7511 (1.8302)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3935 (11.6236)  time: 0.1439  data: 0.0014  max mem: 5591
Epoch: [9] (train) Total time: 0:00:08 (0.1690 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.7511 (1.8008)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3935 (11.6236)
Epoch: [9] (test)  [0/6]  eta: 0:00:03  loss: 1.7157 (1.7157)  time: 0.6197  data: 0.5738  max mem: 5591
Epoch: [9] (test)  [5/6]  eta: 0:00:00  loss: 1.5876 (1.7309)  time: 0.1577  data: 0.1240  max mem: 5591
Epoch: [9] (test) Total time: 0:00:00 (0.1638 s / it)
Averaged stats: loss: 1.5876 (1.8454)
Epoch: [10] (train)  [ 0/50]  eta: 0:00:39  lr: 0.000062  min_lr: 0.000062  loss: 1.3828 (1.3828)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1800 (6.1800)  time: 0.7900  data: 0.6614  max mem: 5591
Epoch: [10] (train)  [10/50]  eta: 0:00:09  lr: 0.000062  min_lr: 0.000062  loss: 1.7060 (1.6027)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2550 (10.3641)  time: 0.2273  data: 0.0851  max mem: 5591
Epoch: [10] (train)  [20/50]  eta: 0:00:05  lr: 0.000062  min_lr: 0.000062  loss: 1.7060 (1.6485)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4310 (11.0926)  time: 0.1648  data: 0.0231  max mem: 5591
Epoch: [10] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 1.7520 (1.6973)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3412 (11.9270)  time: 0.1512  data: 0.0109  max mem: 5591
Epoch: [10] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 1.7520 (1.6859)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.3412 (12.0281)  time: 0.1555  data: 0.0096  max mem: 5591
Epoch: [10] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.6754 (1.7071)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5101 (12.2732)  time: 0.1287  data: 0.0083  max mem: 5591
Epoch: [10] (train) Total time: 0:00:08 (0.1612 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.6754 (1.7300)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5101 (12.2732)
Epoch: [10] (test)  [0/6]  eta: 0:00:04  loss: 1.5151 (1.5151)  time: 0.7497  data: 0.7241  max mem: 5591
Epoch: [10] (test)  [5/6]  eta: 0:00:00  loss: 1.7269 (1.7417)  time: 0.1651  data: 0.1384  max mem: 5591
Epoch: [10] (test) Total time: 0:00:01 (0.1717 s / it)
Averaged stats: loss: 1.7269 (1.8521)
Epoch: [11] (train)  [ 0/50]  eta: 0:00:36  lr: 0.000062  min_lr: 0.000062  loss: 1.5873 (1.5873)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3520 (10.3520)  time: 0.7234  data: 0.6122  max mem: 5591
Epoch: [11] (train)  [10/50]  eta: 0:00:09  lr: 0.000062  min_lr: 0.000062  loss: 1.7138 (1.7874)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9267 (11.3965)  time: 0.2357  data: 0.0613  max mem: 5591
Epoch: [11] (train)  [20/50]  eta: 0:00:06  lr: 0.000062  min_lr: 0.000062  loss: 1.5815 (1.7622)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9267 (12.3027)  time: 0.1763  data: 0.0045  max mem: 5591
Epoch: [11] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 1.5815 (1.7570)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4150 (12.1327)  time: 0.1634  data: 0.0032  max mem: 5591
Epoch: [11] (train)  [40/50]  eta: 0:00:01  lr: 0.000062  min_lr: 0.000062  loss: 1.5965 (1.7283)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4150 (11.9154)  time: 0.1534  data: 0.0029  max mem: 5591
Epoch: [11] (train)  [49/50]  eta: 0:00:00  lr: 0.000062  min_lr: 0.000062  loss: 1.6195 (1.7563)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8323 (12.1834)  time: 0.1292  data: 0.0021  max mem: 5591
Epoch: [11] (train) Total time: 0:00:08 (0.1675 s / it)
Averaged stats: lr: 0.000062  min_lr: 0.000062  loss: 1.6195 (1.6747)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8323 (12.1834)
Epoch: [11] (test)  [0/6]  eta: 0:00:03  loss: 1.7086 (1.7086)  time: 0.6363  data: 0.6146  max mem: 5591
Epoch: [11] (test)  [5/6]  eta: 0:00:00  loss: 1.7151 (1.7419)  time: 0.1565  data: 0.1365  max mem: 5591
Epoch: [11] (test) Total time: 0:00:00 (0.1666 s / it)
Averaged stats: loss: 1.7151 (1.7764)
Epoch: [12] (train)  [ 0/50]  eta: 0:00:34  lr: 0.000062  min_lr: 0.000062  loss: 1.8729 (1.8729)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.0650 (13.0650)  time: 0.6930  data: 0.3390  max mem: 5591
Epoch: [12] (train)  [10/50]  eta: 0:00:10  lr: 0.000062  min_lr: 0.000062  loss: 1.6199 (1.6592)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8438 (12.1425)  time: 0.2525  data: 0.0413  max mem: 5591
Epoch: [12] (train)  [20/50]  eta: 0:00:06  lr: 0.000062  min_lr: 0.000062  loss: 1.5118 (1.6149)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7289 (12.0797)  time: 0.1833  data: 0.0067  max mem: 5591
Epoch: [12] (train)  [30/50]  eta: 0:00:03  lr: 0.000062  min_lr: 0.000062  loss: 1.4950 (1.6174)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.4885 (12.0808)  time: 0.1568  data: 0.0034  max mem: 5591
Epoch: [12] (train)  [40/50]  eta: 0:00:01  lr: 0.000061  min_lr: 0.000061  loss: 1.6133 (1.6461)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9639 (11.7817)  time: 0.1519  data: 0.0037  max mem: 5591
Epoch: [12] (train)  [49/50]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000061  loss: 1.4931 (1.6177)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7437 (11.4604)  time: 0.1305  data: 0.0019  max mem: 5591
Epoch: [12] (train) Total time: 0:00:08 (0.1692 s / it)
Averaged stats: lr: 0.000061  min_lr: 0.000061  loss: 1.4931 (1.6127)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7437 (11.4604)
Epoch: [12] (test)  [0/6]  eta: 0:00:02  loss: 1.4481 (1.4481)  time: 0.3992  data: 0.3480  max mem: 5591
Epoch: [12] (test)  [5/6]  eta: 0:00:00  loss: 1.3514 (1.5806)  time: 0.1465  data: 0.1213  max mem: 5591
Epoch: [12] (test) Total time: 0:00:00 (0.1529 s / it)
Averaged stats: loss: 1.3514 (1.6203)
Epoch: [13] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000061  min_lr: 0.000061  loss: 1.7078 (1.7078)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 14.4091 (14.4091)  time: 0.9346  data: 0.6384  max mem: 5591
Epoch: [13] (train)  [10/50]  eta: 0:00:09  lr: 0.000061  min_lr: 0.000061  loss: 1.5530 (1.5501)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.6445 (12.6691)  time: 0.2271  data: 0.0756  max mem: 5591
Epoch: [13] (train)  [20/50]  eta: 0:00:06  lr: 0.000061  min_lr: 0.000061  loss: 1.5460 (1.5404)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6116 (12.3506)  time: 0.1667  data: 0.0169  max mem: 5591
Epoch: [13] (train)  [30/50]  eta: 0:00:03  lr: 0.000061  min_lr: 0.000061  loss: 1.4718 (1.5306)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3618 (12.5071)  time: 0.1606  data: 0.0090  max mem: 5591
Epoch: [13] (train)  [40/50]  eta: 0:00:01  lr: 0.000061  min_lr: 0.000061  loss: 1.4718 (1.5371)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5166 (12.6248)  time: 0.1531  data: 0.0020  max mem: 5591
Epoch: [13] (train)  [49/50]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000061  loss: 1.4540 (1.5298)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6071 (12.6804)  time: 0.1317  data: 0.0010  max mem: 5591
Epoch: [13] (train) Total time: 0:00:08 (0.1660 s / it)
Averaged stats: lr: 0.000061  min_lr: 0.000061  loss: 1.4540 (1.5463)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.6071 (12.6804)
Epoch: [13] (test)  [0/6]  eta: 0:00:03  loss: 1.2218 (1.2218)  time: 0.6127  data: 0.5851  max mem: 5591
Epoch: [13] (test)  [5/6]  eta: 0:00:00  loss: 1.3426 (1.4923)  time: 0.1466  data: 0.1235  max mem: 5591
Epoch: [13] (test) Total time: 0:00:00 (0.1540 s / it)
Averaged stats: loss: 1.3426 (1.5727)
Epoch: [14] (train)  [ 0/50]  eta: 0:00:42  lr: 0.000061  min_lr: 0.000061  loss: 1.4322 (1.4322)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0792 (8.0792)  time: 0.8486  data: 0.5619  max mem: 5591
Epoch: [14] (train)  [10/50]  eta: 0:00:09  lr: 0.000061  min_lr: 0.000061  loss: 1.2917 (1.4233)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3420 (12.2034)  time: 0.2320  data: 0.0780  max mem: 5591
Epoch: [14] (train)  [20/50]  eta: 0:00:05  lr: 0.000061  min_lr: 0.000061  loss: 1.2917 (1.4268)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3062 (12.0240)  time: 0.1668  data: 0.0265  max mem: 5591
Epoch: [14] (train)  [30/50]  eta: 0:00:03  lr: 0.000061  min_lr: 0.000061  loss: 1.4085 (1.4466)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3550 (11.3260)  time: 0.1557  data: 0.0181  max mem: 5591
Epoch: [14] (train)  [40/50]  eta: 0:00:01  lr: 0.000061  min_lr: 0.000061  loss: 1.4732 (1.4613)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6437 (11.7827)  time: 0.1539  data: 0.0097  max mem: 5591
Epoch: [14] (train)  [49/50]  eta: 0:00:00  lr: 0.000061  min_lr: 0.000061  loss: 1.4881 (1.4885)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8007 (11.6412)  time: 0.1289  data: 0.0084  max mem: 5591
Epoch: [14] (train) Total time: 0:00:08 (0.1638 s / it)
Averaged stats: lr: 0.000061  min_lr: 0.000061  loss: 1.4881 (1.5224)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8007 (11.6412)
Epoch: [14] (test)  [0/6]  eta: 0:00:03  loss: 1.5913 (1.5913)  time: 0.6019  data: 0.5611  max mem: 5591
Epoch: [14] (test)  [5/6]  eta: 0:00:00  loss: 1.5913 (1.6757)  time: 0.1510  data: 0.1143  max mem: 5591
Epoch: [14] (test) Total time: 0:00:00 (0.1565 s / it)
Averaged stats: loss: 1.5913 (1.6564)
Epoch: [15] (train)  [ 0/50]  eta: 0:00:44  lr: 0.000061  min_lr: 0.000061  loss: 1.3243 (1.3243)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5255 (12.5255)  time: 0.8957  data: 0.7973  max mem: 5591
Epoch: [15] (train)  [10/50]  eta: 0:00:09  lr: 0.000061  min_lr: 0.000061  loss: 1.3842 (1.3959)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5089 (10.3609)  time: 0.2344  data: 0.0757  max mem: 5591
Epoch: [15] (train)  [20/50]  eta: 0:00:06  lr: 0.000061  min_lr: 0.000061  loss: 1.4015 (1.4500)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5089 (11.3445)  time: 0.1703  data: 0.0028  max mem: 5591
Epoch: [15] (train)  [30/50]  eta: 0:00:03  lr: 0.000061  min_lr: 0.000061  loss: 1.5267 (1.5045)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.1464 (12.2815)  time: 0.1597  data: 0.0021  max mem: 5591
Epoch: [15] (train)  [40/50]  eta: 0:00:01  lr: 0.000061  min_lr: 0.000061  loss: 1.4772 (1.4961)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2773 (12.2599)  time: 0.1567  data: 0.0057  max mem: 5591
Epoch: [15] (train)  [49/50]  eta: 0:00:00  lr: 0.000060  min_lr: 0.000060  loss: 1.4404 (1.4966)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0530 (11.9677)  time: 0.1292  data: 0.0049  max mem: 5591
Epoch: [15] (train) Total time: 0:00:08 (0.1664 s / it)
Averaged stats: lr: 0.000060  min_lr: 0.000060  loss: 1.4404 (1.5215)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0530 (11.9677)
Epoch: [15] (test)  [0/6]  eta: 0:00:02  loss: 1.4082 (1.4082)  time: 0.4997  data: 0.4678  max mem: 5591
Epoch: [15] (test)  [5/6]  eta: 0:00:00  loss: 1.3740 (1.5120)  time: 0.1515  data: 0.1207  max mem: 5591
Epoch: [15] (test) Total time: 0:00:00 (0.1570 s / it)
Averaged stats: loss: 1.3740 (1.5472)
Epoch: [16] (train)  [ 0/50]  eta: 0:00:48  lr: 0.000060  min_lr: 0.000060  loss: 1.3715 (1.3715)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5311 (9.5311)  time: 0.9691  data: 0.6992  max mem: 5591
Epoch: [16] (train)  [10/50]  eta: 0:00:09  lr: 0.000060  min_lr: 0.000060  loss: 1.4464 (1.4382)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.6223 (12.3698)  time: 0.2393  data: 0.0654  max mem: 5591
Epoch: [16] (train)  [20/50]  eta: 0:00:06  lr: 0.000060  min_lr: 0.000060  loss: 1.4318 (1.4180)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3705 (12.3340)  time: 0.1703  data: 0.0020  max mem: 5591
Epoch: [16] (train)  [30/50]  eta: 0:00:03  lr: 0.000060  min_lr: 0.000060  loss: 1.3990 (1.4213)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6016 (11.7772)  time: 0.1683  data: 0.0024  max mem: 5591
Epoch: [16] (train)  [40/50]  eta: 0:00:01  lr: 0.000060  min_lr: 0.000060  loss: 1.4189 (1.4738)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9957 (11.9700)  time: 0.1531  data: 0.0017  max mem: 5591
Epoch: [16] (train)  [49/50]  eta: 0:00:00  lr: 0.000060  min_lr: 0.000060  loss: 1.5279 (1.4954)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7826 (11.9623)  time: 0.1240  data: 0.0005  max mem: 5591
Epoch: [16] (train) Total time: 0:00:08 (0.1689 s / it)
Averaged stats: lr: 0.000060  min_lr: 0.000060  loss: 1.5279 (1.4813)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7826 (11.9623)
Epoch: [16] (test)  [0/6]  eta: 0:00:03  loss: 1.6480 (1.6480)  time: 0.5939  data: 0.5525  max mem: 5591
Epoch: [16] (test)  [5/6]  eta: 0:00:00  loss: 1.6325 (1.5803)  time: 0.1379  data: 0.1087  max mem: 5591
Epoch: [16] (test) Total time: 0:00:00 (0.1421 s / it)
Averaged stats: loss: 1.6325 (1.4784)
Epoch: [17] (train)  [ 0/50]  eta: 0:00:54  lr: 0.000060  min_lr: 0.000060  loss: 2.6123 (2.6123)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 21.8311 (21.8311)  time: 1.0840  data: 0.4885  max mem: 5591
Epoch: [17] (train)  [10/50]  eta: 0:00:09  lr: 0.000060  min_lr: 0.000060  loss: 1.5918 (1.6735)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4782 (14.7822)  time: 0.2416  data: 0.0475  max mem: 5591
Epoch: [17] (train)  [20/50]  eta: 0:00:06  lr: 0.000060  min_lr: 0.000060  loss: 1.4646 (1.5543)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1687 (12.7133)  time: 0.1620  data: 0.0027  max mem: 5591
Epoch: [17] (train)  [30/50]  eta: 0:00:03  lr: 0.000060  min_lr: 0.000060  loss: 1.3655 (1.4778)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3176 (12.6736)  time: 0.1505  data: 0.0026  max mem: 5591
Epoch: [17] (train)  [40/50]  eta: 0:00:01  lr: 0.000060  min_lr: 0.000060  loss: 1.3012 (1.4539)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2280 (11.6941)  time: 0.1567  data: 0.0033  max mem: 5591
Epoch: [17] (train)  [49/50]  eta: 0:00:00  lr: 0.000060  min_lr: 0.000060  loss: 1.3321 (1.4407)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7663 (11.4455)  time: 0.1396  data: 0.0032  max mem: 5591
Epoch: [17] (train) Total time: 0:00:08 (0.1685 s / it)
Averaged stats: lr: 0.000060  min_lr: 0.000060  loss: 1.3321 (1.4793)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7663 (11.4455)
Epoch: [17] (test)  [0/6]  eta: 0:00:03  loss: 1.3872 (1.3872)  time: 0.6386  data: 0.6172  max mem: 5591
Epoch: [17] (test)  [5/6]  eta: 0:00:00  loss: 1.3872 (1.4969)  time: 0.1371  data: 0.1143  max mem: 5591
Epoch: [17] (test) Total time: 0:00:00 (0.1450 s / it)
Averaged stats: loss: 1.3872 (1.5789)
Epoch: [18] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000060  min_lr: 0.000060  loss: 1.2956 (1.2956)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5372 (8.5372)  time: 0.9317  data: 0.3381  max mem: 5591
Epoch: [18] (train)  [10/50]  eta: 0:00:09  lr: 0.000060  min_lr: 0.000060  loss: 1.3962 (1.4880)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5152 (10.9749)  time: 0.2295  data: 0.0335  max mem: 5591
Epoch: [18] (train)  [20/50]  eta: 0:00:05  lr: 0.000060  min_lr: 0.000060  loss: 1.4098 (1.4785)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9850 (10.8581)  time: 0.1593  data: 0.0023  max mem: 5591
Epoch: [18] (train)  [30/50]  eta: 0:00:03  lr: 0.000059  min_lr: 0.000059  loss: 1.4584 (1.4915)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5866 (11.2100)  time: 0.1608  data: 0.0017  max mem: 5591
Epoch: [18] (train)  [40/50]  eta: 0:00:01  lr: 0.000059  min_lr: 0.000059  loss: 1.4521 (1.4790)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0776 (11.7929)  time: 0.1551  data: 0.0028  max mem: 5591
Epoch: [18] (train)  [49/50]  eta: 0:00:00  lr: 0.000059  min_lr: 0.000059  loss: 1.4762 (1.5192)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2449 (11.9168)  time: 0.1225  data: 0.0022  max mem: 5591
Epoch: [18] (train) Total time: 0:00:08 (0.1638 s / it)
Averaged stats: lr: 0.000059  min_lr: 0.000059  loss: 1.4762 (1.4301)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.2449 (11.9168)
Epoch: [18] (test)  [0/6]  eta: 0:00:03  loss: 1.3281 (1.3281)  time: 0.6177  data: 0.5810  max mem: 5591
Epoch: [18] (test)  [5/6]  eta: 0:00:00  loss: 1.3213 (1.3962)  time: 0.1460  data: 0.1190  max mem: 5591
Epoch: [18] (test) Total time: 0:00:00 (0.1530 s / it)
Averaged stats: loss: 1.3213 (1.4888)
Epoch: [19] (train)  [ 0/50]  eta: 0:00:55  lr: 0.000059  min_lr: 0.000059  loss: 1.7490 (1.7490)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.5656 (12.5656)  time: 1.1179  data: 0.3491  max mem: 5591
Epoch: [19] (train)  [10/50]  eta: 0:00:09  lr: 0.000059  min_lr: 0.000059  loss: 1.3891 (1.4061)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3634 (11.2751)  time: 0.2448  data: 0.0385  max mem: 5591
Epoch: [19] (train)  [20/50]  eta: 0:00:05  lr: 0.000059  min_lr: 0.000059  loss: 1.3714 (1.4148)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8391 (11.3657)  time: 0.1538  data: 0.0052  max mem: 5591
Epoch: [19] (train)  [30/50]  eta: 0:00:03  lr: 0.000059  min_lr: 0.000059  loss: 1.4292 (1.4510)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9706 (12.4892)  time: 0.1542  data: 0.0038  max mem: 5591
Epoch: [19] (train)  [40/50]  eta: 0:00:01  lr: 0.000059  min_lr: 0.000059  loss: 1.4775 (1.4579)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3925 (12.1443)  time: 0.1652  data: 0.0040  max mem: 5591
Epoch: [19] (train)  [49/50]  eta: 0:00:00  lr: 0.000059  min_lr: 0.000059  loss: 1.2973 (1.4111)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4223 (11.5589)  time: 0.1367  data: 0.0019  max mem: 5591
Epoch: [19] (train) Total time: 0:00:08 (0.1687 s / it)
Averaged stats: lr: 0.000059  min_lr: 0.000059  loss: 1.2973 (1.4169)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4223 (11.5589)
Epoch: [19] (test)  [0/6]  eta: 0:00:02  loss: 1.1486 (1.1486)  time: 0.4206  data: 0.3832  max mem: 5591
Epoch: [19] (test)  [5/6]  eta: 0:00:00  loss: 1.1486 (1.3513)  time: 0.1125  data: 0.0854  max mem: 5591
Epoch: [19] (test) Total time: 0:00:00 (0.1194 s / it)
Averaged stats: loss: 1.1486 (1.4602)
Epoch: [20] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000059  min_lr: 0.000059  loss: 1.4867 (1.4867)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4411 (6.4411)  time: 0.9817  data: 0.7143  max mem: 5591
Epoch: [20] (train)  [10/50]  eta: 0:00:09  lr: 0.000059  min_lr: 0.000059  loss: 1.4006 (1.3395)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1979 (8.9466)  time: 0.2388  data: 0.0689  max mem: 5591
Epoch: [20] (train)  [20/50]  eta: 0:00:06  lr: 0.000059  min_lr: 0.000059  loss: 1.3670 (1.3602)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6308 (10.0660)  time: 0.1629  data: 0.0031  max mem: 5591
Epoch: [20] (train)  [30/50]  eta: 0:00:03  lr: 0.000058  min_lr: 0.000058  loss: 1.2726 (1.3353)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6704 (9.9576)  time: 0.1544  data: 0.0047  max mem: 5591
Epoch: [20] (train)  [40/50]  eta: 0:00:01  lr: 0.000058  min_lr: 0.000058  loss: 1.2726 (1.3152)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6704 (9.8671)  time: 0.1630  data: 0.0082  max mem: 5591
Epoch: [20] (train)  [49/50]  eta: 0:00:00  lr: 0.000058  min_lr: 0.000058  loss: 1.3555 (1.3456)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2635 (10.0726)  time: 0.1383  data: 0.0080  max mem: 5591
Epoch: [20] (train) Total time: 0:00:08 (0.1683 s / it)
Averaged stats: lr: 0.000058  min_lr: 0.000058  loss: 1.3555 (1.3115)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2635 (10.0726)
Epoch: [20] (test)  [0/6]  eta: 0:00:04  loss: 1.4127 (1.4127)  time: 0.6974  data: 0.6751  max mem: 5591
Epoch: [20] (test)  [5/6]  eta: 0:00:00  loss: 1.4262 (1.4605)  time: 0.1534  data: 0.1331  max mem: 5591
Epoch: [20] (test) Total time: 0:00:00 (0.1601 s / it)
Averaged stats: loss: 1.4262 (1.4407)
Epoch: [21] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000058  min_lr: 0.000058  loss: 1.3569 (1.3569)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4346 (9.4346)  time: 0.9191  data: 0.8204  max mem: 5591
Epoch: [21] (train)  [10/50]  eta: 0:00:09  lr: 0.000058  min_lr: 0.000058  loss: 1.2057 (1.2194)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4985 (10.5706)  time: 0.2279  data: 0.1003  max mem: 5591
Epoch: [21] (train)  [20/50]  eta: 0:00:05  lr: 0.000058  min_lr: 0.000058  loss: 1.2221 (1.2217)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0555 (9.8265)  time: 0.1542  data: 0.0235  max mem: 5591
Epoch: [21] (train)  [30/50]  eta: 0:00:03  lr: 0.000058  min_lr: 0.000058  loss: 1.2391 (1.2420)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3278 (10.0437)  time: 0.1617  data: 0.0120  max mem: 5591
Epoch: [21] (train)  [40/50]  eta: 0:00:01  lr: 0.000058  min_lr: 0.000058  loss: 1.2315 (1.2640)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7174 (10.2920)  time: 0.1632  data: 0.0036  max mem: 5591
Epoch: [21] (train)  [49/50]  eta: 0:00:00  lr: 0.000058  min_lr: 0.000058  loss: 1.3409 (1.2902)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8569 (10.7016)  time: 0.1256  data: 0.0018  max mem: 5591
Epoch: [21] (train) Total time: 0:00:08 (0.1639 s / it)
Averaged stats: lr: 0.000058  min_lr: 0.000058  loss: 1.3409 (1.3015)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8569 (10.7016)
Epoch: [21] (test)  [0/6]  eta: 0:00:03  loss: 1.2553 (1.2553)  time: 0.5635  data: 0.5337  max mem: 5591
Epoch: [21] (test)  [5/6]  eta: 0:00:00  loss: 1.3901 (1.4000)  time: 0.1309  data: 0.1009  max mem: 5591
Epoch: [21] (test) Total time: 0:00:00 (0.1383 s / it)
Averaged stats: loss: 1.3901 (1.4022)
Epoch: [22] (train)  [ 0/50]  eta: 0:00:50  lr: 0.000058  min_lr: 0.000058  loss: 1.3160 (1.3160)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3435 (8.3435)  time: 1.0037  data: 0.3405  max mem: 5591
Epoch: [22] (train)  [10/50]  eta: 0:00:09  lr: 0.000058  min_lr: 0.000058  loss: 1.2874 (1.3149)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.1069 (10.9424)  time: 0.2306  data: 0.0382  max mem: 5591
Epoch: [22] (train)  [20/50]  eta: 0:00:05  lr: 0.000058  min_lr: 0.000058  loss: 1.2579 (1.3027)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8736 (11.1625)  time: 0.1550  data: 0.0047  max mem: 5591
Epoch: [22] (train)  [30/50]  eta: 0:00:03  lr: 0.000057  min_lr: 0.000057  loss: 1.2754 (1.3056)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6679 (11.1932)  time: 0.1545  data: 0.0020  max mem: 5591
Epoch: [22] (train)  [40/50]  eta: 0:00:01  lr: 0.000057  min_lr: 0.000057  loss: 1.2660 (1.2760)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5073 (11.4466)  time: 0.1646  data: 0.0022  max mem: 5591
Epoch: [22] (train)  [49/50]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000057  loss: 1.1734 (1.2659)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2895 (11.1060)  time: 0.1325  data: 0.0016  max mem: 5591
Epoch: [22] (train) Total time: 0:00:08 (0.1652 s / it)
Averaged stats: lr: 0.000057  min_lr: 0.000057  loss: 1.1734 (1.2732)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2895 (11.1060)
Epoch: [22] (test)  [0/6]  eta: 0:00:03  loss: 1.3662 (1.3662)  time: 0.5425  data: 0.5001  max mem: 5591
Epoch: [22] (test)  [5/6]  eta: 0:00:00  loss: 1.3206 (1.4205)  time: 0.1656  data: 0.1397  max mem: 5591
Epoch: [22] (test) Total time: 0:00:01 (0.1719 s / it)
Averaged stats: loss: 1.3206 (1.4873)
Epoch: [23] (train)  [ 0/50]  eta: 0:00:42  lr: 0.000057  min_lr: 0.000057  loss: 1.1013 (1.1013)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.5111 (13.5111)  time: 0.8580  data: 0.6001  max mem: 5591
Epoch: [23] (train)  [10/50]  eta: 0:00:09  lr: 0.000057  min_lr: 0.000057  loss: 1.1538 (1.1974)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9632 (11.2431)  time: 0.2418  data: 0.0592  max mem: 5591
Epoch: [23] (train)  [20/50]  eta: 0:00:06  lr: 0.000057  min_lr: 0.000057  loss: 1.1587 (1.2097)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.9632 (11.4018)  time: 0.1725  data: 0.0036  max mem: 5591
Epoch: [23] (train)  [30/50]  eta: 0:00:03  lr: 0.000057  min_lr: 0.000057  loss: 1.1983 (1.2432)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4199 (11.7601)  time: 0.1542  data: 0.0023  max mem: 5591
Epoch: [23] (train)  [40/50]  eta: 0:00:01  lr: 0.000057  min_lr: 0.000057  loss: 1.2482 (1.2452)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4979 (11.5067)  time: 0.1597  data: 0.0022  max mem: 5591
Epoch: [23] (train)  [49/50]  eta: 0:00:00  lr: 0.000057  min_lr: 0.000057  loss: 1.2239 (1.2491)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6430 (11.0921)  time: 0.1419  data: 0.0012  max mem: 5591
Epoch: [23] (train) Total time: 0:00:08 (0.1687 s / it)
Averaged stats: lr: 0.000057  min_lr: 0.000057  loss: 1.2239 (1.2312)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6430 (11.0921)
Epoch: [23] (test)  [0/6]  eta: 0:00:03  loss: 1.3042 (1.3042)  time: 0.5437  data: 0.5213  max mem: 5591
Epoch: [23] (test)  [5/6]  eta: 0:00:00  loss: 1.4076 (1.4301)  time: 0.1473  data: 0.1212  max mem: 5591
Epoch: [23] (test) Total time: 0:00:00 (0.1548 s / it)
Averaged stats: loss: 1.4076 (1.4491)
Epoch: [24] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000057  min_lr: 0.000057  loss: 1.1653 (1.1653)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8439 (12.8439)  time: 1.0398  data: 0.6911  max mem: 5591
Epoch: [24] (train)  [10/50]  eta: 0:00:09  lr: 0.000056  min_lr: 0.000056  loss: 1.3016 (1.2863)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8815 (12.0541)  time: 0.2462  data: 0.0670  max mem: 5591
Epoch: [24] (train)  [20/50]  eta: 0:00:06  lr: 0.000056  min_lr: 0.000056  loss: 1.2783 (1.2591)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.8446 (11.3923)  time: 0.1603  data: 0.0039  max mem: 5591
Epoch: [24] (train)  [30/50]  eta: 0:00:03  lr: 0.000056  min_lr: 0.000056  loss: 1.2345 (1.2574)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6420 (11.4878)  time: 0.1613  data: 0.0031  max mem: 5591
Epoch: [24] (train)  [40/50]  eta: 0:00:01  lr: 0.000056  min_lr: 0.000056  loss: 1.2345 (1.2489)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7238 (11.3855)  time: 0.1581  data: 0.0034  max mem: 5591
Epoch: [24] (train)  [49/50]  eta: 0:00:00  lr: 0.000056  min_lr: 0.000056  loss: 1.2051 (1.2525)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1674 (11.1448)  time: 0.1232  data: 0.0021  max mem: 5591
Epoch: [24] (train) Total time: 0:00:08 (0.1666 s / it)
Averaged stats: lr: 0.000056  min_lr: 0.000056  loss: 1.2051 (1.2543)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1674 (11.1448)
Epoch: [24] (test)  [0/6]  eta: 0:00:03  loss: 1.1389 (1.1389)  time: 0.5889  data: 0.5345  max mem: 5591
Epoch: [24] (test)  [5/6]  eta: 0:00:00  loss: 1.2231 (1.2147)  time: 0.1507  data: 0.1043  max mem: 5591
Epoch: [24] (test) Total time: 0:00:00 (0.1569 s / it)
Averaged stats: loss: 1.2231 (1.3396)
Saving model at epoch 25 in /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
Epoch: [25] (train)  [ 0/50]  eta: 0:00:31  lr: 0.000056  min_lr: 0.000056  loss: 0.8451 (0.8451)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9569 (9.9569)  time: 0.6369  data: 0.5399  max mem: 5591
Epoch: [25] (train)  [10/50]  eta: 0:00:07  lr: 0.000056  min_lr: 0.000056  loss: 1.1605 (1.1540)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0998 (10.4164)  time: 0.1894  data: 0.0530  max mem: 5591
Epoch: [25] (train)  [20/50]  eta: 0:00:05  lr: 0.000056  min_lr: 0.000056  loss: 1.1815 (1.1730)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0998 (10.2720)  time: 0.1480  data: 0.0206  max mem: 5591
Epoch: [25] (train)  [30/50]  eta: 0:00:03  lr: 0.000056  min_lr: 0.000056  loss: 1.2037 (1.1836)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5597 (10.0996)  time: 0.1609  data: 0.0405  max mem: 5591
Epoch: [25] (train)  [40/50]  eta: 0:00:01  lr: 0.000055  min_lr: 0.000055  loss: 1.1767 (1.1835)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0798 (10.1888)  time: 0.1593  data: 0.0319  max mem: 5591
Epoch: [25] (train)  [49/50]  eta: 0:00:00  lr: 0.000055  min_lr: 0.000055  loss: 1.1528 (1.1853)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6778 (10.3233)  time: 0.1277  data: 0.0129  max mem: 5591
Epoch: [25] (train) Total time: 0:00:07 (0.1557 s / it)
Averaged stats: lr: 0.000055  min_lr: 0.000055  loss: 1.1528 (1.1747)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6778 (10.3233)
Epoch: [25] (test)  [0/6]  eta: 0:00:04  loss: 1.0849 (1.0849)  time: 0.7520  data: 0.7301  max mem: 5591
Epoch: [25] (test)  [5/6]  eta: 0:00:00  loss: 1.2167 (1.3243)  time: 0.1577  data: 0.1373  max mem: 5591
Epoch: [25] (test) Total time: 0:00:00 (0.1653 s / it)
Averaged stats: loss: 1.2167 (1.3344)
Epoch: [26] (train)  [ 0/50]  eta: 0:00:50  lr: 0.000055  min_lr: 0.000055  loss: 1.0948 (1.0948)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 13.8059 (13.8059)  time: 1.0107  data: 0.7861  max mem: 5591
Epoch: [26] (train)  [10/50]  eta: 0:00:09  lr: 0.000055  min_lr: 0.000055  loss: 1.1502 (1.1626)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.8103 (11.9092)  time: 0.2265  data: 0.0736  max mem: 5591
Epoch: [26] (train)  [20/50]  eta: 0:00:05  lr: 0.000055  min_lr: 0.000055  loss: 1.1502 (1.1662)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.7556 (12.2763)  time: 0.1507  data: 0.0134  max mem: 5591
Epoch: [26] (train)  [30/50]  eta: 0:00:03  lr: 0.000055  min_lr: 0.000055  loss: 1.1071 (1.1476)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4051 (11.6737)  time: 0.1610  data: 0.0270  max mem: 5591
Epoch: [26] (train)  [40/50]  eta: 0:00:01  lr: 0.000055  min_lr: 0.000055  loss: 1.0942 (1.1496)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2050 (11.5665)  time: 0.1712  data: 0.0266  max mem: 5591
Epoch: [26] (train)  [49/50]  eta: 0:00:00  lr: 0.000055  min_lr: 0.000055  loss: 1.0942 (1.1514)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2050 (11.2884)  time: 0.1307  data: 0.0158  max mem: 5591
Epoch: [26] (train) Total time: 0:00:08 (0.1658 s / it)
Averaged stats: lr: 0.000055  min_lr: 0.000055  loss: 1.0942 (1.1745)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2050 (11.2884)
Epoch: [26] (test)  [0/6]  eta: 0:00:02  loss: 1.2547 (1.2547)  time: 0.4046  data: 0.3637  max mem: 5591
Epoch: [26] (test)  [5/6]  eta: 0:00:00  loss: 1.1737 (1.3158)  time: 0.1499  data: 0.1248  max mem: 5591
Epoch: [26] (test) Total time: 0:00:00 (0.1572 s / it)
Averaged stats: loss: 1.1737 (1.2989)
Epoch: [27] (train)  [ 0/50]  eta: 0:00:39  lr: 0.000055  min_lr: 0.000055  loss: 1.0032 (1.0032)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0556 (10.0556)  time: 0.7865  data: 0.6751  max mem: 5591
Epoch: [27] (train)  [10/50]  eta: 0:00:09  lr: 0.000055  min_lr: 0.000055  loss: 1.0032 (1.0751)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1045 (11.0216)  time: 0.2447  data: 0.1002  max mem: 5591
Epoch: [27] (train)  [20/50]  eta: 0:00:05  lr: 0.000054  min_lr: 0.000054  loss: 1.1376 (1.1466)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1045 (11.1979)  time: 0.1693  data: 0.0236  max mem: 5591
Epoch: [27] (train)  [30/50]  eta: 0:00:03  lr: 0.000054  min_lr: 0.000054  loss: 1.2503 (1.1739)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4727 (11.8492)  time: 0.1542  data: 0.0152  max mem: 5591
Epoch: [27] (train)  [40/50]  eta: 0:00:01  lr: 0.000054  min_lr: 0.000054  loss: 1.1036 (1.1576)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.4727 (11.7559)  time: 0.1626  data: 0.0274  max mem: 5591
Epoch: [27] (train)  [49/50]  eta: 0:00:00  lr: 0.000054  min_lr: 0.000054  loss: 1.0724 (1.1598)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1347 (11.2375)  time: 0.1318  data: 0.0152  max mem: 5591
Epoch: [27] (train) Total time: 0:00:08 (0.1671 s / it)
Averaged stats: lr: 0.000054  min_lr: 0.000054  loss: 1.0724 (1.1657)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.1347 (11.2375)
Epoch: [27] (test)  [0/6]  eta: 0:00:03  loss: 0.8236 (0.8236)  time: 0.6500  data: 0.6020  max mem: 5591
Epoch: [27] (test)  [5/6]  eta: 0:00:00  loss: 1.2321 (1.1774)  time: 0.1499  data: 0.1239  max mem: 5591
Epoch: [27] (test) Total time: 0:00:00 (0.1576 s / it)
Averaged stats: loss: 1.2321 (1.2495)
Epoch: [28] (train)  [ 0/50]  eta: 0:00:47  lr: 0.000054  min_lr: 0.000054  loss: 0.9824 (0.9824)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3818 (7.3818)  time: 0.9499  data: 0.8584  max mem: 5591
Epoch: [28] (train)  [10/50]  eta: 0:00:09  lr: 0.000054  min_lr: 0.000054  loss: 1.0371 (1.0938)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7679 (8.0366)  time: 0.2278  data: 0.0976  max mem: 5591
Epoch: [28] (train)  [20/50]  eta: 0:00:05  lr: 0.000054  min_lr: 0.000054  loss: 1.0371 (1.0952)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1885 (8.7361)  time: 0.1513  data: 0.0151  max mem: 5591
Epoch: [28] (train)  [30/50]  eta: 0:00:03  lr: 0.000054  min_lr: 0.000054  loss: 1.1000 (1.1236)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2662 (9.0687)  time: 0.1621  data: 0.0102  max mem: 5591
Epoch: [28] (train)  [40/50]  eta: 0:00:01  lr: 0.000053  min_lr: 0.000053  loss: 1.1144 (1.1178)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0241 (9.1214)  time: 0.1699  data: 0.0074  max mem: 5591
Epoch: [28] (train)  [49/50]  eta: 0:00:00  lr: 0.000053  min_lr: 0.000053  loss: 1.1144 (1.1477)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2142 (9.3527)  time: 0.1277  data: 0.0030  max mem: 5591
Epoch: [28] (train) Total time: 0:00:08 (0.1651 s / it)
Averaged stats: lr: 0.000053  min_lr: 0.000053  loss: 1.1144 (1.1151)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2142 (9.3527)
Epoch: [28] (test)  [0/6]  eta: 0:00:03  loss: 1.2180 (1.2180)  time: 0.5744  data: 0.5461  max mem: 5591
Epoch: [28] (test)  [5/6]  eta: 0:00:00  loss: 1.2614 (1.3048)  time: 0.1473  data: 0.1196  max mem: 5591
Epoch: [28] (test) Total time: 0:00:00 (0.1527 s / it)
Averaged stats: loss: 1.2614 (1.2752)
Epoch: [29] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000053  min_lr: 0.000053  loss: 0.7632 (0.7632)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9744 (5.9744)  time: 0.9284  data: 0.3141  max mem: 5591
Epoch: [29] (train)  [10/50]  eta: 0:00:09  lr: 0.000053  min_lr: 0.000053  loss: 0.9774 (1.0327)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3535 (8.6892)  time: 0.2465  data: 0.0328  max mem: 5591
Epoch: [29] (train)  [20/50]  eta: 0:00:06  lr: 0.000053  min_lr: 0.000053  loss: 1.0410 (1.1285)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1048 (10.4300)  time: 0.1653  data: 0.0027  max mem: 5591
Epoch: [29] (train)  [30/50]  eta: 0:00:03  lr: 0.000053  min_lr: 0.000053  loss: 1.1059 (1.1260)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.2927 (10.5989)  time: 0.1546  data: 0.0015  max mem: 5591
Epoch: [29] (train)  [40/50]  eta: 0:00:01  lr: 0.000053  min_lr: 0.000053  loss: 1.0895 (1.1358)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5045 (10.2571)  time: 0.1636  data: 0.0029  max mem: 5591
Epoch: [29] (train)  [49/50]  eta: 0:00:00  lr: 0.000053  min_lr: 0.000053  loss: 1.0895 (1.1379)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7197 (9.9578)  time: 0.1323  data: 0.0020  max mem: 5591
Epoch: [29] (train) Total time: 0:00:08 (0.1682 s / it)
Averaged stats: lr: 0.000053  min_lr: 0.000053  loss: 1.0895 (1.1085)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7197 (9.9578)
Epoch: [29] (test)  [0/6]  eta: 0:00:04  loss: 0.9968 (0.9968)  time: 0.8068  data: 0.7701  max mem: 5591
Epoch: [29] (test)  [5/6]  eta: 0:00:00  loss: 1.2264 (1.2479)  time: 0.1639  data: 0.1349  max mem: 5591
Epoch: [29] (test) Total time: 0:00:01 (0.1691 s / it)
Averaged stats: loss: 1.2264 (1.2690)
Epoch: [30] (train)  [ 0/50]  eta: 0:00:41  lr: 0.000053  min_lr: 0.000053  loss: 1.1392 (1.1392)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6092 (7.6092)  time: 0.8392  data: 0.5122  max mem: 5591
Epoch: [30] (train)  [10/50]  eta: 0:00:09  lr: 0.000052  min_lr: 0.000052  loss: 1.0557 (1.0796)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8839 (7.9987)  time: 0.2338  data: 0.0506  max mem: 5591
Epoch: [30] (train)  [20/50]  eta: 0:00:06  lr: 0.000052  min_lr: 0.000052  loss: 1.0236 (1.0668)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9504 (8.5700)  time: 0.1734  data: 0.0065  max mem: 5591
Epoch: [30] (train)  [30/50]  eta: 0:00:03  lr: 0.000052  min_lr: 0.000052  loss: 1.0169 (1.0408)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6838 (8.4042)  time: 0.1669  data: 0.0084  max mem: 5591
Epoch: [30] (train)  [40/50]  eta: 0:00:01  lr: 0.000052  min_lr: 0.000052  loss: 1.0289 (1.0644)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7170 (8.5532)  time: 0.1540  data: 0.0060  max mem: 5591
Epoch: [30] (train)  [49/50]  eta: 0:00:00  lr: 0.000052  min_lr: 0.000052  loss: 1.0736 (1.0732)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5519 (8.7077)  time: 0.1225  data: 0.0033  max mem: 5591
Epoch: [30] (train) Total time: 0:00:08 (0.1666 s / it)
Averaged stats: lr: 0.000052  min_lr: 0.000052  loss: 1.0736 (1.0698)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5519 (8.7077)
Epoch: [30] (test)  [0/6]  eta: 0:00:03  loss: 1.3452 (1.3452)  time: 0.5889  data: 0.5487  max mem: 5591
Epoch: [30] (test)  [5/6]  eta: 0:00:00  loss: 1.2161 (1.2742)  time: 0.1393  data: 0.1148  max mem: 5591
Epoch: [30] (test) Total time: 0:00:00 (0.1451 s / it)
Averaged stats: loss: 1.2161 (1.3337)
Epoch: [31] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000052  min_lr: 0.000052  loss: 1.0472 (1.0472)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3872 (8.3872)  time: 0.9064  data: 0.3374  max mem: 5591
Epoch: [31] (train)  [10/50]  eta: 0:00:09  lr: 0.000052  min_lr: 0.000052  loss: 1.0474 (1.0656)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9019 (10.2359)  time: 0.2259  data: 0.0352  max mem: 5591
Epoch: [31] (train)  [20/50]  eta: 0:00:06  lr: 0.000051  min_lr: 0.000051  loss: 1.0687 (1.0709)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4129 (9.7846)  time: 0.1695  data: 0.0048  max mem: 5591
Epoch: [31] (train)  [30/50]  eta: 0:00:03  lr: 0.000051  min_lr: 0.000051  loss: 1.0718 (1.0615)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2005 (9.5534)  time: 0.1628  data: 0.0026  max mem: 5591
Epoch: [31] (train)  [40/50]  eta: 0:00:01  lr: 0.000051  min_lr: 0.000051  loss: 1.0349 (1.0551)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8769 (9.7574)  time: 0.1505  data: 0.0005  max mem: 5591
Epoch: [31] (train)  [49/50]  eta: 0:00:00  lr: 0.000051  min_lr: 0.000051  loss: 1.0349 (1.0606)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8769 (9.5651)  time: 0.1272  data: 0.0005  max mem: 5591
Epoch: [31] (train) Total time: 0:00:08 (0.1643 s / it)
Averaged stats: lr: 0.000051  min_lr: 0.000051  loss: 1.0349 (1.0470)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8769 (9.5651)
Epoch: [31] (test)  [0/6]  eta: 0:00:03  loss: 1.3281 (1.3281)  time: 0.6645  data: 0.6404  max mem: 5591
Epoch: [31] (test)  [5/6]  eta: 0:00:00  loss: 1.1581 (1.2565)  time: 0.1576  data: 0.1356  max mem: 5591
Epoch: [31] (test) Total time: 0:00:01 (0.1669 s / it)
Averaged stats: loss: 1.1581 (1.2662)
Epoch: [32] (train)  [ 0/50]  eta: 0:00:44  lr: 0.000051  min_lr: 0.000051  loss: 0.9208 (0.9208)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1898 (7.1898)  time: 0.8917  data: 0.7230  max mem: 5591
Epoch: [32] (train)  [10/50]  eta: 0:00:09  lr: 0.000051  min_lr: 0.000051  loss: 0.9556 (0.9880)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2146 (7.9181)  time: 0.2284  data: 0.0769  max mem: 5591
Epoch: [32] (train)  [20/50]  eta: 0:00:05  lr: 0.000051  min_lr: 0.000051  loss: 1.1227 (1.1613)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5386 (10.2920)  time: 0.1630  data: 0.0206  max mem: 5591
Epoch: [32] (train)  [30/50]  eta: 0:00:03  lr: 0.000050  min_lr: 0.000050  loss: 1.1416 (1.1631)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6069 (11.0131)  time: 0.1665  data: 0.0333  max mem: 5591
Epoch: [32] (train)  [40/50]  eta: 0:00:01  lr: 0.000050  min_lr: 0.000050  loss: 1.1435 (1.1736)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.6095 (11.1472)  time: 0.1587  data: 0.0221  max mem: 5591
Epoch: [32] (train)  [49/50]  eta: 0:00:00  lr: 0.000050  min_lr: 0.000050  loss: 1.1630 (1.1772)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7139 (11.1820)  time: 0.1260  data: 0.0046  max mem: 5591
Epoch: [32] (train) Total time: 0:00:08 (0.1652 s / it)
Averaged stats: lr: 0.000050  min_lr: 0.000050  loss: 1.1630 (1.1227)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.7139 (11.1820)
Epoch: [32] (test)  [0/6]  eta: 0:00:02  loss: 1.3167 (1.3167)  time: 0.3683  data: 0.3465  max mem: 5591
Epoch: [32] (test)  [5/6]  eta: 0:00:00  loss: 1.3083 (1.3345)  time: 0.1400  data: 0.1197  max mem: 5591
Epoch: [32] (test) Total time: 0:00:00 (0.1477 s / it)
Averaged stats: loss: 1.3083 (1.3249)
Epoch: [33] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000050  min_lr: 0.000050  loss: 1.0810 (1.0810)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.1861 (9.1861)  time: 1.0301  data: 0.6923  max mem: 5591
Epoch: [33] (train)  [10/50]  eta: 0:00:09  lr: 0.000050  min_lr: 0.000050  loss: 1.0302 (1.1318)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.8161 (13.0426)  time: 0.2376  data: 0.0646  max mem: 5591
Epoch: [33] (train)  [20/50]  eta: 0:00:05  lr: 0.000050  min_lr: 0.000050  loss: 1.0266 (1.0861)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 12.0042 (12.2084)  time: 0.1565  data: 0.0030  max mem: 5591
Epoch: [33] (train)  [30/50]  eta: 0:00:03  lr: 0.000050  min_lr: 0.000050  loss: 1.0325 (1.0979)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5359 (11.6299)  time: 0.1516  data: 0.0047  max mem: 5591
Epoch: [33] (train)  [40/50]  eta: 0:00:01  lr: 0.000049  min_lr: 0.000049  loss: 1.1977 (1.2364)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3618 (13.3331)  time: 0.1538  data: 0.0038  max mem: 5591
Epoch: [33] (train)  [49/50]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000049  loss: 1.3110 (1.2414)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3954 (12.8720)  time: 0.1307  data: 0.0021  max mem: 5591
Epoch: [33] (train) Total time: 0:00:08 (0.1643 s / it)
Averaged stats: lr: 0.000049  min_lr: 0.000049  loss: 1.3110 (1.2084)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.3954 (12.8720)
Epoch: [33] (test)  [0/6]  eta: 0:00:03  loss: 1.1814 (1.1814)  time: 0.6136  data: 0.5613  max mem: 5591
Epoch: [33] (test)  [5/6]  eta: 0:00:00  loss: 1.1676 (1.2750)  time: 0.1423  data: 0.1121  max mem: 5591
Epoch: [33] (test) Total time: 0:00:00 (0.1518 s / it)
Averaged stats: loss: 1.1676 (1.3465)
Epoch: [34] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000049  min_lr: 0.000049  loss: 1.0065 (1.0065)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3694 (8.3694)  time: 0.9116  data: 0.3375  max mem: 5591
Epoch: [34] (train)  [10/50]  eta: 0:00:09  lr: 0.000049  min_lr: 0.000049  loss: 1.1469 (1.1783)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1434 (8.1499)  time: 0.2291  data: 0.0471  max mem: 5591
Epoch: [34] (train)  [20/50]  eta: 0:00:05  lr: 0.000049  min_lr: 0.000049  loss: 1.1329 (1.1148)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0423 (8.0655)  time: 0.1582  data: 0.0105  max mem: 5591
Epoch: [34] (train)  [30/50]  eta: 0:00:03  lr: 0.000049  min_lr: 0.000049  loss: 1.0437 (1.1136)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4142 (8.5742)  time: 0.1626  data: 0.0026  max mem: 5591
Epoch: [34] (train)  [40/50]  eta: 0:00:01  lr: 0.000049  min_lr: 0.000049  loss: 1.0127 (1.0786)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5336 (8.9786)  time: 0.1752  data: 0.0023  max mem: 5591
Epoch: [34] (train)  [49/50]  eta: 0:00:00  lr: 0.000049  min_lr: 0.000049  loss: 1.0127 (1.0775)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0711 (9.4510)  time: 0.1327  data: 0.0031  max mem: 5591
Epoch: [34] (train) Total time: 0:00:08 (0.1673 s / it)
Averaged stats: lr: 0.000049  min_lr: 0.000049  loss: 1.0127 (1.0720)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.0711 (9.4510)
Epoch: [34] (test)  [0/6]  eta: 0:00:04  loss: 1.0670 (1.0670)  time: 0.6668  data: 0.6284  max mem: 5591
Epoch: [34] (test)  [5/6]  eta: 0:00:00  loss: 1.0670 (1.1750)  time: 0.1474  data: 0.1195  max mem: 5591
Epoch: [34] (test) Total time: 0:00:00 (0.1576 s / it)
Averaged stats: loss: 1.0670 (1.2127)
Epoch: [35] (train)  [ 0/50]  eta: 0:00:47  lr: 0.000048  min_lr: 0.000048  loss: 1.0318 (1.0318)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5332 (9.5332)  time: 0.9580  data: 0.3362  max mem: 5591
Epoch: [35] (train)  [10/50]  eta: 0:00:09  lr: 0.000048  min_lr: 0.000048  loss: 1.0973 (1.0894)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6719 (11.0221)  time: 0.2427  data: 0.0422  max mem: 5591
Epoch: [35] (train)  [20/50]  eta: 0:00:06  lr: 0.000048  min_lr: 0.000048  loss: 1.0715 (1.0785)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5718 (10.1774)  time: 0.1636  data: 0.0119  max mem: 5591
Epoch: [35] (train)  [30/50]  eta: 0:00:03  lr: 0.000048  min_lr: 0.000048  loss: 0.9672 (1.0372)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2192 (9.4061)  time: 0.1566  data: 0.0071  max mem: 5591
Epoch: [35] (train)  [40/50]  eta: 0:00:01  lr: 0.000048  min_lr: 0.000048  loss: 0.9446 (1.0231)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3466 (8.7599)  time: 0.1618  data: 0.0026  max mem: 5591
Epoch: [35] (train)  [49/50]  eta: 0:00:00  lr: 0.000048  min_lr: 0.000048  loss: 0.9753 (1.0290)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4684 (8.7100)  time: 0.1349  data: 0.0020  max mem: 5591
Epoch: [35] (train) Total time: 0:00:08 (0.1684 s / it)
Averaged stats: lr: 0.000048  min_lr: 0.000048  loss: 0.9753 (1.0105)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4684 (8.7100)
Epoch: [35] (test)  [0/6]  eta: 0:00:03  loss: 1.4476 (1.4476)  time: 0.6430  data: 0.6210  max mem: 5591
Epoch: [35] (test)  [5/6]  eta: 0:00:00  loss: 1.2424 (1.2865)  time: 0.1503  data: 0.1300  max mem: 5591
Epoch: [35] (test) Total time: 0:00:00 (0.1598 s / it)
Averaged stats: loss: 1.2424 (1.2991)
Epoch: [36] (train)  [ 0/50]  eta: 0:00:48  lr: 0.000048  min_lr: 0.000048  loss: 0.9618 (0.9618)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9227 (6.9227)  time: 0.9602  data: 0.4003  max mem: 5591
Epoch: [36] (train)  [10/50]  eta: 0:00:09  lr: 0.000047  min_lr: 0.000047  loss: 1.0336 (1.0492)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9943 (7.9715)  time: 0.2257  data: 0.0447  max mem: 5591
Epoch: [36] (train)  [20/50]  eta: 0:00:05  lr: 0.000047  min_lr: 0.000047  loss: 1.0211 (1.0074)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1193 (8.5968)  time: 0.1572  data: 0.0085  max mem: 5591
Epoch: [36] (train)  [30/50]  eta: 0:00:03  lr: 0.000047  min_lr: 0.000047  loss: 0.9648 (0.9774)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.0905 (8.8599)  time: 0.1668  data: 0.0094  max mem: 5591
Epoch: [36] (train)  [40/50]  eta: 0:00:01  lr: 0.000047  min_lr: 0.000047  loss: 0.9140 (0.9862)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9289 (8.7134)  time: 0.1672  data: 0.0096  max mem: 5591
Epoch: [36] (train)  [49/50]  eta: 0:00:00  lr: 0.000047  min_lr: 0.000047  loss: 0.9598 (0.9846)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1341 (8.6160)  time: 0.1291  data: 0.0050  max mem: 5591
Epoch: [36] (train) Total time: 0:00:08 (0.1665 s / it)
Averaged stats: lr: 0.000047  min_lr: 0.000047  loss: 0.9598 (1.0018)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1341 (8.6160)
Epoch: [36] (test)  [0/6]  eta: 0:00:03  loss: 1.0865 (1.0865)  time: 0.5752  data: 0.5383  max mem: 5591
Epoch: [36] (test)  [5/6]  eta: 0:00:00  loss: 1.0865 (1.1162)  time: 0.1370  data: 0.1093  max mem: 5591
Epoch: [36] (test) Total time: 0:00:00 (0.1484 s / it)
Averaged stats: loss: 1.0865 (1.1984)
Epoch: [37] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000047  min_lr: 0.000047  loss: 1.0343 (1.0343)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2634 (8.2634)  time: 0.9807  data: 0.3475  max mem: 5591
Epoch: [37] (train)  [10/50]  eta: 0:00:09  lr: 0.000047  min_lr: 0.000047  loss: 0.9438 (0.9593)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6386 (9.2611)  time: 0.2386  data: 0.0340  max mem: 5591
Epoch: [37] (train)  [20/50]  eta: 0:00:05  lr: 0.000046  min_lr: 0.000046  loss: 0.9367 (0.9615)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7146 (8.9733)  time: 0.1530  data: 0.0023  max mem: 5591
Epoch: [37] (train)  [30/50]  eta: 0:00:03  lr: 0.000046  min_lr: 0.000046  loss: 0.9029 (0.9471)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1054 (8.6647)  time: 0.1532  data: 0.0029  max mem: 5591
Epoch: [37] (train)  [40/50]  eta: 0:00:01  lr: 0.000046  min_lr: 0.000046  loss: 0.9215 (0.9573)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8630 (9.0385)  time: 0.1673  data: 0.0029  max mem: 5591
Epoch: [37] (train)  [49/50]  eta: 0:00:00  lr: 0.000046  min_lr: 0.000046  loss: 0.9446 (0.9553)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3236 (9.2607)  time: 0.1348  data: 0.0012  max mem: 5591
Epoch: [37] (train) Total time: 0:00:08 (0.1661 s / it)
Averaged stats: lr: 0.000046  min_lr: 0.000046  loss: 0.9446 (0.9734)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.3236 (9.2607)
Epoch: [37] (test)  [0/6]  eta: 0:00:03  loss: 1.0730 (1.0730)  time: 0.5310  data: 0.5093  max mem: 5591
Epoch: [37] (test)  [5/6]  eta: 0:00:00  loss: 1.0802 (1.2633)  time: 0.1444  data: 0.1237  max mem: 5591
Epoch: [37] (test) Total time: 0:00:00 (0.1541 s / it)
Averaged stats: loss: 1.0802 (1.2322)
Epoch: [38] (train)  [ 0/50]  eta: 0:00:40  lr: 0.000046  min_lr: 0.000046  loss: 1.1959 (1.1959)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2353 (9.2353)  time: 0.8170  data: 0.6339  max mem: 5591
Epoch: [38] (train)  [10/50]  eta: 0:00:09  lr: 0.000046  min_lr: 0.000046  loss: 1.0360 (0.9783)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.4145 (9.5647)  time: 0.2315  data: 0.0711  max mem: 5591
Epoch: [38] (train)  [20/50]  eta: 0:00:05  lr: 0.000045  min_lr: 0.000045  loss: 0.9719 (0.9946)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.4611 (9.9957)  time: 0.1634  data: 0.0093  max mem: 5591
Epoch: [38] (train)  [30/50]  eta: 0:00:03  lr: 0.000045  min_lr: 0.000045  loss: 0.9298 (0.9677)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.2623 (9.3903)  time: 0.1501  data: 0.0033  max mem: 5591
Epoch: [38] (train)  [40/50]  eta: 0:00:01  lr: 0.000045  min_lr: 0.000045  loss: 0.9298 (0.9671)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8845 (9.0836)  time: 0.1555  data: 0.0018  max mem: 5591
Epoch: [38] (train)  [49/50]  eta: 0:00:00  lr: 0.000045  min_lr: 0.000045  loss: 0.9572 (0.9684)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4197 (8.8725)  time: 0.1338  data: 0.0039  max mem: 5591
Epoch: [38] (train) Total time: 0:00:08 (0.1634 s / it)
Averaged stats: lr: 0.000045  min_lr: 0.000045  loss: 0.9572 (0.9601)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4197 (8.8725)
Epoch: [38] (test)  [0/6]  eta: 0:00:03  loss: 1.0339 (1.0339)  time: 0.5692  data: 0.5469  max mem: 5591
Epoch: [38] (test)  [5/6]  eta: 0:00:00  loss: 1.0339 (1.1129)  time: 0.1516  data: 0.1303  max mem: 5591
Epoch: [38] (test) Total time: 0:00:00 (0.1635 s / it)
Averaged stats: loss: 1.0339 (1.1676)
Epoch: [39] (train)  [ 0/50]  eta: 0:00:52  lr: 0.000045  min_lr: 0.000045  loss: 0.7728 (0.7728)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9181 (5.9181)  time: 1.0545  data: 0.5787  max mem: 5591
Epoch: [39] (train)  [10/50]  eta: 0:00:09  lr: 0.000045  min_lr: 0.000045  loss: 0.8546 (0.8740)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8180 (8.3677)  time: 0.2383  data: 0.0532  max mem: 5591
Epoch: [39] (train)  [20/50]  eta: 0:00:06  lr: 0.000045  min_lr: 0.000045  loss: 0.8617 (0.8961)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7831 (8.5084)  time: 0.1581  data: 0.0018  max mem: 5591
Epoch: [39] (train)  [30/50]  eta: 0:00:03  lr: 0.000044  min_lr: 0.000044  loss: 0.8798 (0.8969)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6187 (8.5688)  time: 0.1619  data: 0.0038  max mem: 5591
Epoch: [39] (train)  [40/50]  eta: 0:00:01  lr: 0.000044  min_lr: 0.000044  loss: 0.9061 (0.9066)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1975 (8.4632)  time: 0.1681  data: 0.0032  max mem: 5591
Epoch: [39] (train)  [49/50]  eta: 0:00:00  lr: 0.000044  min_lr: 0.000044  loss: 0.8932 (0.9045)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9621 (8.6246)  time: 0.1310  data: 0.0011  max mem: 5591
Epoch: [39] (train) Total time: 0:00:08 (0.1690 s / it)
Averaged stats: lr: 0.000044  min_lr: 0.000044  loss: 0.8932 (0.9402)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9621 (8.6246)
Epoch: [39] (test)  [0/6]  eta: 0:00:03  loss: 1.2103 (1.2103)  time: 0.5552  data: 0.5333  max mem: 5591
Epoch: [39] (test)  [5/6]  eta: 0:00:00  loss: 1.0832 (1.1345)  time: 0.1315  data: 0.1091  max mem: 5591
Epoch: [39] (test) Total time: 0:00:00 (0.1436 s / it)
Averaged stats: loss: 1.0832 (1.2523)
Epoch: [40] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000044  min_lr: 0.000044  loss: 0.9034 (0.9034)  loss_scale: 512.0000 (512.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4231 (7.4231)  time: 0.9257  data: 0.5708  max mem: 5591
Epoch: [40] (train)  [10/50]  eta: 0:00:10  lr: 0.000044  min_lr: 0.000044  loss: 0.8554 (0.8703)  loss_scale: 512.0000 (698.1818)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.8568 (8.9133)  time: 0.2503  data: 0.0572  max mem: 5591
Epoch: [40] (train)  [20/50]  eta: 0:00:06  lr: 0.000044  min_lr: 0.000044  loss: 0.8554 (0.8764)  loss_scale: 1024.0000 (853.3333)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.5067 (8.5329)  time: 0.1732  data: 0.0041  max mem: 5591
Epoch: [40] (train)  [30/50]  eta: 0:00:03  lr: 0.000043  min_lr: 0.000043  loss: 0.8816 (0.8849)  loss_scale: 1024.0000 (908.3871)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1417 (8.4307)  time: 0.1623  data: 0.0021  max mem: 5591
Epoch: [40] (train)  [40/50]  eta: 0:00:01  lr: 0.000043  min_lr: 0.000043  loss: 0.8936 (0.9086)  loss_scale: 1024.0000 (936.5854)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1599 (8.4463)  time: 0.1569  data: 0.0012  max mem: 5591
Epoch: [40] (train)  [49/50]  eta: 0:00:00  lr: 0.000043  min_lr: 0.000043  loss: 0.9116 (0.9091)  loss_scale: 1024.0000 (952.3200)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7696 (8.3563)  time: 0.1245  data: 0.0004  max mem: 5591
Epoch: [40] (train) Total time: 0:00:08 (0.1687 s / it)
Averaged stats: lr: 0.000043  min_lr: 0.000043  loss: 0.9116 (0.9256)  loss_scale: 1024.0000 (952.3200)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7696 (8.3563)
Epoch: [40] (test)  [0/6]  eta: 0:00:03  loss: 1.0746 (1.0746)  time: 0.6378  data: 0.6162  max mem: 5591
Epoch: [40] (test)  [5/6]  eta: 0:00:00  loss: 1.0937 (1.2057)  time: 0.1490  data: 0.1177  max mem: 5591
Epoch: [40] (test) Total time: 0:00:00 (0.1623 s / it)
Averaged stats: loss: 1.0937 (1.2299)
Epoch: [41] (train)  [ 0/50]  eta: 0:00:50  lr: 0.000043  min_lr: 0.000043  loss: 1.0898 (1.0898)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9087 (6.9087)  time: 1.0044  data: 0.4110  max mem: 5591
Epoch: [41] (train)  [10/50]  eta: 0:00:09  lr: 0.000043  min_lr: 0.000043  loss: 0.8241 (0.8108)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3567 (8.1032)  time: 0.2286  data: 0.0436  max mem: 5591
Epoch: [41] (train)  [20/50]  eta: 0:00:05  lr: 0.000043  min_lr: 0.000043  loss: 0.8241 (0.8634)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3567 (8.1008)  time: 0.1556  data: 0.0075  max mem: 5591
Epoch: [41] (train)  [30/50]  eta: 0:00:03  lr: 0.000042  min_lr: 0.000042  loss: 0.8448 (0.8743)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2537 (8.3459)  time: 0.1608  data: 0.0069  max mem: 5591
Epoch: [41] (train)  [40/50]  eta: 0:00:01  lr: 0.000042  min_lr: 0.000042  loss: 0.8721 (0.8860)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1677 (8.2679)  time: 0.1720  data: 0.0068  max mem: 5591
Epoch: [41] (train)  [49/50]  eta: 0:00:00  lr: 0.000042  min_lr: 0.000042  loss: 0.8980 (0.8935)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9739 (8.2302)  time: 0.1288  data: 0.0041  max mem: 5591
Epoch: [41] (train) Total time: 0:00:08 (0.1656 s / it)
Averaged stats: lr: 0.000042  min_lr: 0.000042  loss: 0.8980 (0.9097)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9739 (8.2302)
Epoch: [41] (test)  [0/6]  eta: 0:00:04  loss: 1.1756 (1.1756)  time: 0.7576  data: 0.7349  max mem: 5591
Epoch: [41] (test)  [5/6]  eta: 0:00:00  loss: 1.1756 (1.3317)  time: 0.1611  data: 0.1408  max mem: 5591
Epoch: [41] (test) Total time: 0:00:01 (0.1706 s / it)
Averaged stats: loss: 1.1756 (1.1518)
Epoch: [42] (train)  [ 0/50]  eta: 0:00:53  lr: 0.000042  min_lr: 0.000042  loss: 0.7890 (0.7890)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4670 (6.4670)  time: 1.0685  data: 0.9223  max mem: 5591
Epoch: [42] (train)  [10/50]  eta: 0:00:09  lr: 0.000042  min_lr: 0.000042  loss: 0.8894 (0.9132)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3905 (7.6409)  time: 0.2406  data: 0.0985  max mem: 5591
Epoch: [42] (train)  [20/50]  eta: 0:00:05  lr: 0.000042  min_lr: 0.000042  loss: 0.8894 (0.9143)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2723 (7.5408)  time: 0.1494  data: 0.0120  max mem: 5591
Epoch: [42] (train)  [30/50]  eta: 0:00:03  lr: 0.000042  min_lr: 0.000042  loss: 0.8904 (0.9193)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9074 (7.6885)  time: 0.1513  data: 0.0055  max mem: 5591
Epoch: [42] (train)  [40/50]  eta: 0:00:01  lr: 0.000041  min_lr: 0.000041  loss: 0.8814 (0.9124)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9442 (7.4614)  time: 0.1697  data: 0.0038  max mem: 5591
Epoch: [42] (train)  [49/50]  eta: 0:00:00  lr: 0.000041  min_lr: 0.000041  loss: 0.8736 (0.9028)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6837 (7.5095)  time: 0.1354  data: 0.0026  max mem: 5591
Epoch: [42] (train) Total time: 0:00:08 (0.1672 s / it)
Averaged stats: lr: 0.000041  min_lr: 0.000041  loss: 0.8736 (0.8887)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6837 (7.5095)
Epoch: [42] (test)  [0/6]  eta: 0:00:03  loss: 0.8190 (0.8190)  time: 0.6360  data: 0.6063  max mem: 5591
Epoch: [42] (test)  [5/6]  eta: 0:00:00  loss: 1.0708 (1.1890)  time: 0.1375  data: 0.1095  max mem: 5591
Epoch: [42] (test) Total time: 0:00:00 (0.1472 s / it)
Averaged stats: loss: 1.0708 (1.2270)
Epoch: [43] (train)  [ 0/50]  eta: 0:00:39  lr: 0.000041  min_lr: 0.000041  loss: 0.8337 (0.8337)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2997 (8.2997)  time: 0.7857  data: 0.6236  max mem: 5591
Epoch: [43] (train)  [10/50]  eta: 0:00:10  lr: 0.000041  min_lr: 0.000041  loss: 0.8370 (0.8811)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1519 (7.7226)  time: 0.2500  data: 0.0600  max mem: 5591
Epoch: [43] (train)  [20/50]  eta: 0:00:06  lr: 0.000041  min_lr: 0.000041  loss: 0.8620 (0.8954)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1519 (8.3814)  time: 0.1867  data: 0.0049  max mem: 5591
Epoch: [43] (train)  [30/50]  eta: 0:00:03  lr: 0.000041  min_lr: 0.000041  loss: 0.8582 (0.8784)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8306 (8.0052)  time: 0.1617  data: 0.0042  max mem: 5591
Epoch: [43] (train)  [40/50]  eta: 0:00:01  lr: 0.000040  min_lr: 0.000040  loss: 0.8070 (0.8776)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9203 (8.0785)  time: 0.1501  data: 0.0020  max mem: 5591
Epoch: [43] (train)  [49/50]  eta: 0:00:00  lr: 0.000040  min_lr: 0.000040  loss: 0.8070 (0.8803)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1010 (8.2831)  time: 0.1386  data: 0.0012  max mem: 5591
Epoch: [43] (train) Total time: 0:00:08 (0.1722 s / it)
Averaged stats: lr: 0.000040  min_lr: 0.000040  loss: 0.8070 (0.8799)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1010 (8.2831)
Epoch: [43] (test)  [0/6]  eta: 0:00:02  loss: 1.1113 (1.1113)  time: 0.3626  data: 0.3303  max mem: 5591
Epoch: [43] (test)  [5/6]  eta: 0:00:00  loss: 1.1113 (1.1667)  time: 0.1321  data: 0.1041  max mem: 5591
Epoch: [43] (test) Total time: 0:00:00 (0.1395 s / it)
Averaged stats: loss: 1.1113 (1.1792)
Epoch: [44] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000040  min_lr: 0.000040  loss: 0.8379 (0.8379)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 11.3994 (11.3994)  time: 0.9855  data: 0.5564  max mem: 5591
Epoch: [44] (train)  [10/50]  eta: 0:00:09  lr: 0.000040  min_lr: 0.000040  loss: 0.8925 (0.8860)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.0518 (9.6571)  time: 0.2287  data: 0.0553  max mem: 5591
Epoch: [44] (train)  [20/50]  eta: 0:00:06  lr: 0.000040  min_lr: 0.000040  loss: 0.8937 (0.8822)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.9607 (9.2988)  time: 0.1630  data: 0.0063  max mem: 5591
Epoch: [44] (train)  [30/50]  eta: 0:00:03  lr: 0.000040  min_lr: 0.000040  loss: 0.9171 (0.9009)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.7022 (8.9132)  time: 0.1748  data: 0.0126  max mem: 5591
Epoch: [44] (train)  [40/50]  eta: 0:00:01  lr: 0.000039  min_lr: 0.000039  loss: 0.8911 (0.8967)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1075 (8.8211)  time: 0.1629  data: 0.0137  max mem: 5591
Epoch: [44] (train)  [49/50]  eta: 0:00:00  lr: 0.000039  min_lr: 0.000039  loss: 0.8260 (0.8914)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7595 (8.5904)  time: 0.1275  data: 0.0057  max mem: 5591
Epoch: [44] (train) Total time: 0:00:08 (0.1688 s / it)
Averaged stats: lr: 0.000039  min_lr: 0.000039  loss: 0.8260 (0.8862)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7595 (8.5904)
Epoch: [44] (test)  [0/6]  eta: 0:00:03  loss: 1.0508 (1.0508)  time: 0.5872  data: 0.5532  max mem: 5591
Epoch: [44] (test)  [5/6]  eta: 0:00:00  loss: 1.0508 (1.1389)  time: 0.1514  data: 0.1249  max mem: 5591
Epoch: [44] (test) Total time: 0:00:00 (0.1575 s / it)
Averaged stats: loss: 1.0508 (1.2114)
Epoch: [45] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000039  min_lr: 0.000039  loss: 0.8086 (0.8086)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8717 (6.8717)  time: 0.9972  data: 0.8379  max mem: 5591
Epoch: [45] (train)  [10/50]  eta: 0:00:09  lr: 0.000039  min_lr: 0.000039  loss: 0.8985 (0.8979)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0057 (7.7798)  time: 0.2335  data: 0.0767  max mem: 5591
Epoch: [45] (train)  [20/50]  eta: 0:00:05  lr: 0.000039  min_lr: 0.000039  loss: 0.8325 (0.8681)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.9518 (7.8813)  time: 0.1593  data: 0.0012  max mem: 5591
Epoch: [45] (train)  [30/50]  eta: 0:00:03  lr: 0.000039  min_lr: 0.000039  loss: 0.8159 (0.8684)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1024 (7.5570)  time: 0.1686  data: 0.0041  max mem: 5591
Epoch: [45] (train)  [40/50]  eta: 0:00:01  lr: 0.000038  min_lr: 0.000038  loss: 0.8254 (0.8775)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4721 (7.8502)  time: 0.1659  data: 0.0038  max mem: 5591
Epoch: [45] (train)  [49/50]  eta: 0:00:00  lr: 0.000038  min_lr: 0.000038  loss: 0.8866 (0.8749)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3499 (7.8883)  time: 0.1284  data: 0.0010  max mem: 5591
Epoch: [45] (train) Total time: 0:00:08 (0.1692 s / it)
Averaged stats: lr: 0.000038  min_lr: 0.000038  loss: 0.8866 (0.8598)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.3499 (7.8883)
Epoch: [45] (test)  [0/6]  eta: 0:00:03  loss: 0.8280 (0.8280)  time: 0.6432  data: 0.5890  max mem: 5591
Epoch: [45] (test)  [5/6]  eta: 0:00:00  loss: 1.0557 (1.0112)  time: 0.1539  data: 0.1282  max mem: 5591
Epoch: [45] (test) Total time: 0:00:00 (0.1603 s / it)
Averaged stats: loss: 1.0557 (1.0812)
Epoch: [46] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000038  min_lr: 0.000038  loss: 0.8430 (0.8430)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8976 (7.8976)  time: 1.0332  data: 0.3463  max mem: 5591
Epoch: [46] (train)  [10/50]  eta: 0:00:09  lr: 0.000038  min_lr: 0.000038  loss: 0.8430 (0.8551)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6901 (6.6935)  time: 0.2375  data: 0.0421  max mem: 5591
Epoch: [46] (train)  [20/50]  eta: 0:00:06  lr: 0.000038  min_lr: 0.000038  loss: 0.8238 (0.8442)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3562 (7.4280)  time: 0.1646  data: 0.0070  max mem: 5591
Epoch: [46] (train)  [30/50]  eta: 0:00:03  lr: 0.000038  min_lr: 0.000038  loss: 0.7758 (0.8170)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8455 (7.5648)  time: 0.1611  data: 0.0050  max mem: 5591
Epoch: [46] (train)  [40/50]  eta: 0:00:01  lr: 0.000037  min_lr: 0.000037  loss: 0.7648 (0.8096)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7627 (7.3828)  time: 0.1493  data: 0.0041  max mem: 5591
Epoch: [46] (train)  [49/50]  eta: 0:00:00  lr: 0.000037  min_lr: 0.000037  loss: 0.7629 (0.8101)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6482 (7.4275)  time: 0.1256  data: 0.0005  max mem: 5591
Epoch: [46] (train) Total time: 0:00:08 (0.1664 s / it)
Averaged stats: lr: 0.000037  min_lr: 0.000037  loss: 0.7629 (0.8330)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6482 (7.4275)
Epoch: [46] (test)  [0/6]  eta: 0:00:03  loss: 0.9652 (0.9652)  time: 0.5953  data: 0.5632  max mem: 5591
Epoch: [46] (test)  [5/6]  eta: 0:00:00  loss: 1.0800 (1.1795)  time: 0.1555  data: 0.1298  max mem: 5591
Epoch: [46] (test) Total time: 0:00:00 (0.1627 s / it)
Averaged stats: loss: 1.0800 (1.1606)
Epoch: [47] (train)  [ 0/50]  eta: 0:00:47  lr: 0.000037  min_lr: 0.000037  loss: 0.5833 (0.5833)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6211 (4.6211)  time: 0.9548  data: 0.3504  max mem: 5591
Epoch: [47] (train)  [10/50]  eta: 0:00:09  lr: 0.000037  min_lr: 0.000037  loss: 0.7945 (0.7926)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3589 (7.4988)  time: 0.2400  data: 0.0560  max mem: 5591
Epoch: [47] (train)  [20/50]  eta: 0:00:06  lr: 0.000037  min_lr: 0.000037  loss: 0.7818 (0.8077)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6688 (7.5167)  time: 0.1694  data: 0.0385  max mem: 5591
Epoch: [47] (train)  [30/50]  eta: 0:00:03  lr: 0.000037  min_lr: 0.000037  loss: 0.7817 (0.8166)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6688 (7.4611)  time: 0.1753  data: 0.0542  max mem: 5591
Epoch: [47] (train)  [40/50]  eta: 0:00:01  lr: 0.000036  min_lr: 0.000036  loss: 0.8049 (0.8125)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7902 (7.4593)  time: 0.1712  data: 0.0436  max mem: 5591
Epoch: [47] (train)  [49/50]  eta: 0:00:00  lr: 0.000036  min_lr: 0.000036  loss: 0.8153 (0.8140)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0732 (7.4007)  time: 0.1249  data: 0.0148  max mem: 5591
Epoch: [47] (train) Total time: 0:00:08 (0.1729 s / it)
Averaged stats: lr: 0.000036  min_lr: 0.000036  loss: 0.8153 (0.8257)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0732 (7.4007)
Epoch: [47] (test)  [0/6]  eta: 0:00:03  loss: 0.9309 (0.9309)  time: 0.5676  data: 0.5277  max mem: 5591
Epoch: [47] (test)  [5/6]  eta: 0:00:00  loss: 1.0071 (1.1406)  time: 0.1554  data: 0.1321  max mem: 5591
Epoch: [47] (test) Total time: 0:00:00 (0.1632 s / it)
Averaged stats: loss: 1.0071 (1.2029)
Epoch: [48] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000036  min_lr: 0.000036  loss: 0.9733 (0.9733)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8153 (7.8153)  time: 0.9394  data: 0.6011  max mem: 5591
Epoch: [48] (train)  [10/50]  eta: 0:00:09  lr: 0.000036  min_lr: 0.000036  loss: 0.8461 (0.8663)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1373 (8.6850)  time: 0.2448  data: 0.0605  max mem: 5591
Epoch: [48] (train)  [20/50]  eta: 0:00:06  lr: 0.000036  min_lr: 0.000036  loss: 0.8057 (0.8260)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3552 (7.8771)  time: 0.1699  data: 0.0060  max mem: 5591
Epoch: [48] (train)  [30/50]  eta: 0:00:03  lr: 0.000036  min_lr: 0.000036  loss: 0.7837 (0.8137)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7900 (7.5379)  time: 0.1605  data: 0.0055  max mem: 5591
Epoch: [48] (train)  [40/50]  eta: 0:00:01  lr: 0.000035  min_lr: 0.000035  loss: 0.7895 (0.8134)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4723 (7.2903)  time: 0.1436  data: 0.0077  max mem: 5591
Epoch: [48] (train)  [49/50]  eta: 0:00:00  lr: 0.000035  min_lr: 0.000035  loss: 0.8229 (0.8255)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9142 (7.5265)  time: 0.1216  data: 0.0052  max mem: 5591
Epoch: [48] (train) Total time: 0:00:08 (0.1658 s / it)
Averaged stats: lr: 0.000035  min_lr: 0.000035  loss: 0.8229 (0.8159)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9142 (7.5265)
Epoch: [48] (test)  [0/6]  eta: 0:00:02  loss: 0.7608 (0.7608)  time: 0.3622  data: 0.3329  max mem: 5591
Epoch: [48] (test)  [5/6]  eta: 0:00:00  loss: 0.9647 (1.0613)  time: 0.1608  data: 0.1394  max mem: 5591
Epoch: [48] (test) Total time: 0:00:01 (0.1712 s / it)
Averaged stats: loss: 0.9647 (1.1665)
Epoch: [49] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000035  min_lr: 0.000035  loss: 0.7548 (0.7548)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2051 (5.2051)  time: 0.9022  data: 0.7837  max mem: 5591
Epoch: [49] (train)  [10/50]  eta: 0:00:09  lr: 0.000035  min_lr: 0.000035  loss: 0.7548 (0.7611)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3257 (7.4507)  time: 0.2393  data: 0.0784  max mem: 5591
Epoch: [49] (train)  [20/50]  eta: 0:00:05  lr: 0.000035  min_lr: 0.000035  loss: 0.7586 (0.7784)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3257 (7.5798)  time: 0.1564  data: 0.0050  max mem: 5591
Epoch: [49] (train)  [30/50]  eta: 0:00:03  lr: 0.000035  min_lr: 0.000035  loss: 0.7842 (0.7923)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1791 (7.5763)  time: 0.1435  data: 0.0020  max mem: 5591
Epoch: [49] (train)  [40/50]  eta: 0:00:01  lr: 0.000034  min_lr: 0.000034  loss: 0.8336 (0.8190)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2850 (7.6468)  time: 0.1603  data: 0.0024  max mem: 5591
Epoch: [49] (train)  [49/50]  eta: 0:00:00  lr: 0.000034  min_lr: 0.000034  loss: 0.8270 (0.8121)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0158 (7.5431)  time: 0.1408  data: 0.0018  max mem: 5591
Epoch: [49] (train) Total time: 0:00:08 (0.1664 s / it)
Averaged stats: lr: 0.000034  min_lr: 0.000034  loss: 0.8270 (0.7972)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.0158 (7.5431)
Epoch: [49] (test)  [0/6]  eta: 0:00:02  loss: 1.1251 (1.1251)  time: 0.3544  data: 0.3220  max mem: 5591
Epoch: [49] (test)  [5/6]  eta: 0:00:00  loss: 1.1251 (1.2680)  time: 0.1392  data: 0.1081  max mem: 5591
Epoch: [49] (test) Total time: 0:00:00 (0.1468 s / it)
Averaged stats: loss: 1.1251 (1.1616)
Saving model at epoch 50 in /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
Epoch: [50] (train)  [ 0/50]  eta: 0:00:20  lr: 0.000034  min_lr: 0.000034  loss: 0.7478 (0.7478)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3111 (5.3111)  time: 0.4059  data: 0.3345  max mem: 5591
Epoch: [50] (train)  [10/50]  eta: 0:00:06  lr: 0.000034  min_lr: 0.000034  loss: 0.7606 (0.7918)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7623 (7.4192)  time: 0.1691  data: 0.0389  max mem: 5591
Epoch: [50] (train)  [20/50]  eta: 0:00:04  lr: 0.000034  min_lr: 0.000034  loss: 0.7546 (0.7702)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7623 (7.3237)  time: 0.1496  data: 0.0056  max mem: 5591
Epoch: [50] (train)  [30/50]  eta: 0:00:03  lr: 0.000034  min_lr: 0.000034  loss: 0.7643 (0.7741)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1795 (7.6760)  time: 0.1613  data: 0.0105  max mem: 5591
Epoch: [50] (train)  [40/50]  eta: 0:00:01  lr: 0.000033  min_lr: 0.000033  loss: 0.7906 (0.7911)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0043 (8.3181)  time: 0.1696  data: 0.0263  max mem: 5591
Epoch: [50] (train)  [49/50]  eta: 0:00:00  lr: 0.000033  min_lr: 0.000033  loss: 0.8684 (0.8114)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2196 (8.7671)  time: 0.1402  data: 0.0251  max mem: 5591
Epoch: [50] (train) Total time: 0:00:07 (0.1532 s / it)
Averaged stats: lr: 0.000033  min_lr: 0.000033  loss: 0.8684 (0.8052)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2196 (8.7671)
Epoch: [50] (test)  [0/6]  eta: 0:00:03  loss: 1.1830 (1.1830)  time: 0.5717  data: 0.5501  max mem: 5591
Epoch: [50] (test)  [5/6]  eta: 0:00:00  loss: 0.9950 (1.1113)  time: 0.1392  data: 0.1136  max mem: 5591
Epoch: [50] (test) Total time: 0:00:00 (0.1469 s / it)
Averaged stats: loss: 0.9950 (1.2174)
Epoch: [51] (train)  [ 0/50]  eta: 0:00:37  lr: 0.000033  min_lr: 0.000033  loss: 0.7851 (0.7851)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8832 (7.8832)  time: 0.7494  data: 0.5709  max mem: 5591
Epoch: [51] (train)  [10/50]  eta: 0:00:08  lr: 0.000033  min_lr: 0.000033  loss: 0.8513 (0.8383)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.8832 (7.8312)  time: 0.2194  data: 0.0534  max mem: 5591
Epoch: [51] (train)  [20/50]  eta: 0:00:06  lr: 0.000033  min_lr: 0.000033  loss: 0.8513 (0.8422)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0850 (8.6955)  time: 0.1784  data: 0.0033  max mem: 5591
Epoch: [51] (train)  [30/50]  eta: 0:00:03  lr: 0.000032  min_lr: 0.000032  loss: 0.8088 (0.8302)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1544 (8.7678)  time: 0.1710  data: 0.0035  max mem: 5591
Epoch: [51] (train)  [40/50]  eta: 0:00:01  lr: 0.000032  min_lr: 0.000032  loss: 0.8088 (0.8197)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0928 (8.6577)  time: 0.1446  data: 0.0034  max mem: 5591
Epoch: [51] (train)  [49/50]  eta: 0:00:00  lr: 0.000032  min_lr: 0.000032  loss: 0.8172 (0.8286)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1420 (8.7399)  time: 0.1281  data: 0.0025  max mem: 5591
Epoch: [51] (train) Total time: 0:00:08 (0.1664 s / it)
Averaged stats: lr: 0.000032  min_lr: 0.000032  loss: 0.8172 (0.8518)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.1420 (8.7399)
Epoch: [51] (test)  [0/6]  eta: 0:00:02  loss: 1.1341 (1.1341)  time: 0.4061  data: 0.3674  max mem: 5591
Epoch: [51] (test)  [5/6]  eta: 0:00:00  loss: 1.1180 (1.2587)  time: 0.1467  data: 0.1169  max mem: 5591
Epoch: [51] (test) Total time: 0:00:00 (0.1575 s / it)
Averaged stats: loss: 1.1180 (1.1545)
Epoch: [52] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000032  min_lr: 0.000032  loss: 0.9816 (0.9816)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 10.5963 (10.5963)  time: 0.9315  data: 0.4894  max mem: 5591
Epoch: [52] (train)  [10/50]  eta: 0:00:09  lr: 0.000032  min_lr: 0.000032  loss: 0.7950 (0.7833)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.5543 (9.3801)  time: 0.2272  data: 0.0500  max mem: 5591
Epoch: [52] (train)  [20/50]  eta: 0:00:05  lr: 0.000032  min_lr: 0.000032  loss: 0.7438 (0.7637)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4681 (8.5637)  time: 0.1622  data: 0.0069  max mem: 5591
Epoch: [52] (train)  [30/50]  eta: 0:00:03  lr: 0.000031  min_lr: 0.000031  loss: 0.7262 (0.7631)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3568 (8.1657)  time: 0.1646  data: 0.0063  max mem: 5591
Epoch: [52] (train)  [40/50]  eta: 0:00:01  lr: 0.000031  min_lr: 0.000031  loss: 0.7420 (0.7658)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5703 (8.2312)  time: 0.1477  data: 0.0039  max mem: 5591
Epoch: [52] (train)  [49/50]  eta: 0:00:00  lr: 0.000031  min_lr: 0.000031  loss: 0.7521 (0.7741)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0658 (8.1901)  time: 0.1234  data: 0.0025  max mem: 5591
Epoch: [52] (train) Total time: 0:00:08 (0.1639 s / it)
Averaged stats: lr: 0.000031  min_lr: 0.000031  loss: 0.7521 (0.7992)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.0658 (8.1901)
Epoch: [52] (test)  [0/6]  eta: 0:00:04  loss: 0.9825 (0.9825)  time: 0.6962  data: 0.6746  max mem: 5591
Epoch: [52] (test)  [5/6]  eta: 0:00:00  loss: 1.1090 (1.1043)  time: 0.1503  data: 0.1303  max mem: 5591
Epoch: [52] (test) Total time: 0:00:00 (0.1568 s / it)
Averaged stats: loss: 1.1090 (1.1331)
Epoch: [53] (train)  [ 0/50]  eta: 0:00:50  lr: 0.000031  min_lr: 0.000031  loss: 0.6591 (0.6591)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5535 (5.5535)  time: 1.0038  data: 0.8977  max mem: 5591
Epoch: [53] (train)  [10/50]  eta: 0:00:09  lr: 0.000031  min_lr: 0.000031  loss: 0.7318 (0.7343)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1488 (6.4112)  time: 0.2268  data: 0.0878  max mem: 5591
Epoch: [53] (train)  [20/50]  eta: 0:00:05  lr: 0.000031  min_lr: 0.000031  loss: 0.7318 (0.7473)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5778 (7.0375)  time: 0.1544  data: 0.0073  max mem: 5591
Epoch: [53] (train)  [30/50]  eta: 0:00:03  lr: 0.000030  min_lr: 0.000030  loss: 0.7574 (0.7642)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5891 (6.8369)  time: 0.1573  data: 0.0071  max mem: 5591
Epoch: [53] (train)  [40/50]  eta: 0:00:01  lr: 0.000030  min_lr: 0.000030  loss: 0.7807 (0.7734)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5891 (6.9673)  time: 0.1610  data: 0.0061  max mem: 5591
Epoch: [53] (train)  [49/50]  eta: 0:00:00  lr: 0.000030  min_lr: 0.000030  loss: 0.7728 (0.7719)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2497 (6.8698)  time: 0.1297  data: 0.0040  max mem: 5591
Epoch: [53] (train) Total time: 0:00:08 (0.1639 s / it)
Averaged stats: lr: 0.000030  min_lr: 0.000030  loss: 0.7728 (0.7612)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2497 (6.8698)
Epoch: [53] (test)  [0/6]  eta: 0:00:02  loss: 1.1931 (1.1931)  time: 0.3815  data: 0.3446  max mem: 5591
Epoch: [53] (test)  [5/6]  eta: 0:00:00  loss: 1.1724 (1.2288)  time: 0.1337  data: 0.1101  max mem: 5591
Epoch: [53] (test) Total time: 0:00:00 (0.1432 s / it)
Averaged stats: loss: 1.1724 (1.1748)
Epoch: [54] (train)  [ 0/50]  eta: 0:00:37  lr: 0.000030  min_lr: 0.000030  loss: 0.9256 (0.9256)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5711 (6.5711)  time: 0.7527  data: 0.6253  max mem: 5591
Epoch: [54] (train)  [10/50]  eta: 0:00:08  lr: 0.000030  min_lr: 0.000030  loss: 0.7503 (0.7621)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6060 (7.4120)  time: 0.2128  data: 0.0870  max mem: 5591
Epoch: [54] (train)  [20/50]  eta: 0:00:05  lr: 0.000030  min_lr: 0.000030  loss: 0.6924 (0.7503)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6060 (7.6314)  time: 0.1638  data: 0.0247  max mem: 5591
Epoch: [54] (train)  [30/50]  eta: 0:00:03  lr: 0.000029  min_lr: 0.000029  loss: 0.7590 (0.7616)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6839 (7.2737)  time: 0.1604  data: 0.0146  max mem: 5591
Epoch: [54] (train)  [40/50]  eta: 0:00:01  lr: 0.000029  min_lr: 0.000029  loss: 0.7871 (0.7628)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7713 (7.5812)  time: 0.1516  data: 0.0140  max mem: 5591
Epoch: [54] (train)  [49/50]  eta: 0:00:00  lr: 0.000029  min_lr: 0.000029  loss: 0.7941 (0.7802)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7538 (7.7666)  time: 0.1326  data: 0.0183  max mem: 5591
Epoch: [54] (train) Total time: 0:00:08 (0.1610 s / it)
Averaged stats: lr: 0.000029  min_lr: 0.000029  loss: 0.7941 (0.7659)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.7538 (7.7666)
Epoch: [54] (test)  [0/6]  eta: 0:00:02  loss: 1.2916 (1.2916)  time: 0.3918  data: 0.3538  max mem: 5591
Epoch: [54] (test)  [5/6]  eta: 0:00:00  loss: 1.1671 (1.2388)  time: 0.1471  data: 0.1113  max mem: 5591
Epoch: [54] (test) Total time: 0:00:00 (0.1539 s / it)
Averaged stats: loss: 1.1671 (1.1965)
Epoch: [55] (train)  [ 0/50]  eta: 0:00:59  lr: 0.000029  min_lr: 0.000029  loss: 0.8121 (0.8121)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.9150 (9.9150)  time: 1.1903  data: 1.1202  max mem: 5591
Epoch: [55] (train)  [10/50]  eta: 0:00:09  lr: 0.000029  min_lr: 0.000029  loss: 0.8121 (0.8388)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 8.2439 (8.5586)  time: 0.2395  data: 0.1083  max mem: 5591
Epoch: [55] (train)  [20/50]  eta: 0:00:05  lr: 0.000029  min_lr: 0.000029  loss: 0.7548 (0.7836)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4183 (7.6215)  time: 0.1447  data: 0.0051  max mem: 5591
Epoch: [55] (train)  [30/50]  eta: 0:00:03  lr: 0.000028  min_lr: 0.000028  loss: 0.7028 (0.7700)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6457 (7.4946)  time: 0.1653  data: 0.0021  max mem: 5591
Epoch: [55] (train)  [40/50]  eta: 0:00:01  lr: 0.000028  min_lr: 0.000028  loss: 0.7080 (0.7512)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6806 (7.2171)  time: 0.1661  data: 0.0035  max mem: 5591
Epoch: [55] (train)  [49/50]  eta: 0:00:00  lr: 0.000028  min_lr: 0.000028  loss: 0.7126 (0.7467)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3093 (7.1164)  time: 0.1304  data: 0.0032  max mem: 5591
Epoch: [55] (train) Total time: 0:00:08 (0.1675 s / it)
Averaged stats: lr: 0.000028  min_lr: 0.000028  loss: 0.7126 (0.7495)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3093 (7.1164)
Epoch: [55] (test)  [0/6]  eta: 0:00:04  loss: 1.1154 (1.1154)  time: 0.6822  data: 0.6594  max mem: 5591
Epoch: [55] (test)  [5/6]  eta: 0:00:00  loss: 0.8901 (1.0736)  time: 0.1471  data: 0.1262  max mem: 5591
Epoch: [55] (test) Total time: 0:00:00 (0.1591 s / it)
Averaged stats: loss: 0.8901 (1.1471)
Epoch: [56] (train)  [ 0/50]  eta: 0:00:44  lr: 0.000028  min_lr: 0.000028  loss: 0.8763 (0.8763)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.6862 (9.6862)  time: 0.8853  data: 0.3483  max mem: 5591
Epoch: [56] (train)  [10/50]  eta: 0:00:09  lr: 0.000028  min_lr: 0.000028  loss: 0.7231 (0.7094)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0842 (6.4453)  time: 0.2306  data: 0.0401  max mem: 5591
Epoch: [56] (train)  [20/50]  eta: 0:00:06  lr: 0.000028  min_lr: 0.000028  loss: 0.7081 (0.7120)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5869 (6.2395)  time: 0.1780  data: 0.0057  max mem: 5591
Epoch: [56] (train)  [30/50]  eta: 0:00:03  lr: 0.000027  min_lr: 0.000027  loss: 0.7072 (0.7107)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1879 (6.7301)  time: 0.1698  data: 0.0024  max mem: 5591
Epoch: [56] (train)  [40/50]  eta: 0:00:01  lr: 0.000027  min_lr: 0.000027  loss: 0.7203 (0.7202)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.4450 (7.2549)  time: 0.1534  data: 0.0033  max mem: 5591
Epoch: [56] (train)  [49/50]  eta: 0:00:00  lr: 0.000027  min_lr: 0.000027  loss: 0.7544 (0.7333)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5399 (7.2817)  time: 0.1278  data: 0.0022  max mem: 5591
Epoch: [56] (train) Total time: 0:00:08 (0.1690 s / it)
Averaged stats: lr: 0.000027  min_lr: 0.000027  loss: 0.7544 (0.7410)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.5399 (7.2817)
Epoch: [56] (test)  [0/6]  eta: 0:00:02  loss: 1.0267 (1.0267)  time: 0.3836  data: 0.3462  max mem: 5591
Epoch: [56] (test)  [5/6]  eta: 0:00:00  loss: 1.0267 (1.2075)  time: 0.1714  data: 0.1480  max mem: 5591
Epoch: [56] (test) Total time: 0:00:01 (0.1779 s / it)
Averaged stats: loss: 1.0267 (1.1896)
Epoch: [57] (train)  [ 0/50]  eta: 0:00:48  lr: 0.000027  min_lr: 0.000027  loss: 0.9403 (0.9403)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4554 (5.4554)  time: 0.9729  data: 0.5778  max mem: 5591
Epoch: [57] (train)  [10/50]  eta: 0:00:09  lr: 0.000027  min_lr: 0.000027  loss: 0.7456 (0.7570)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6626 (6.1358)  time: 0.2302  data: 0.0551  max mem: 5591
Epoch: [57] (train)  [20/50]  eta: 0:00:06  lr: 0.000027  min_lr: 0.000027  loss: 0.7824 (0.7852)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7370 (6.3659)  time: 0.1697  data: 0.0023  max mem: 5591
Epoch: [57] (train)  [30/50]  eta: 0:00:03  lr: 0.000026  min_lr: 0.000026  loss: 0.7686 (0.7767)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4169 (6.4929)  time: 0.1620  data: 0.0021  max mem: 5591
Epoch: [57] (train)  [40/50]  eta: 0:00:01  lr: 0.000026  min_lr: 0.000026  loss: 0.7193 (0.7673)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5337 (6.8549)  time: 0.1517  data: 0.0027  max mem: 5591
Epoch: [57] (train)  [49/50]  eta: 0:00:00  lr: 0.000026  min_lr: 0.000026  loss: 0.7320 (0.7641)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2809 (6.7270)  time: 0.1324  data: 0.0017  max mem: 5591
Epoch: [57] (train) Total time: 0:00:08 (0.1677 s / it)
Averaged stats: lr: 0.000026  min_lr: 0.000026  loss: 0.7320 (0.7311)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2809 (6.7270)
Epoch: [57] (test)  [0/6]  eta: 0:00:02  loss: 1.0217 (1.0217)  time: 0.4903  data: 0.4562  max mem: 5591
Epoch: [57] (test)  [5/6]  eta: 0:00:00  loss: 1.0445 (1.1073)  time: 0.1458  data: 0.1118  max mem: 5591
Epoch: [57] (test) Total time: 0:00:00 (0.1560 s / it)
Averaged stats: loss: 1.0445 (1.1961)
Epoch: [58] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000026  min_lr: 0.000026  loss: 0.7345 (0.7345)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1280 (6.1280)  time: 0.9902  data: 0.3413  max mem: 5591
Epoch: [58] (train)  [10/50]  eta: 0:00:09  lr: 0.000026  min_lr: 0.000026  loss: 0.7432 (0.7572)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2899 (7.0057)  time: 0.2356  data: 0.0383  max mem: 5591
Epoch: [58] (train)  [20/50]  eta: 0:00:05  lr: 0.000026  min_lr: 0.000026  loss: 0.7444 (0.7587)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2811 (6.7344)  time: 0.1567  data: 0.0049  max mem: 5591
Epoch: [58] (train)  [30/50]  eta: 0:00:03  lr: 0.000025  min_lr: 0.000025  loss: 0.7204 (0.7409)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8745 (6.4797)  time: 0.1748  data: 0.0012  max mem: 5591
Epoch: [58] (train)  [40/50]  eta: 0:00:01  lr: 0.000025  min_lr: 0.000025  loss: 0.7204 (0.7514)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4161 (6.6782)  time: 0.1679  data: 0.0006  max mem: 5591
Epoch: [58] (train)  [49/50]  eta: 0:00:00  lr: 0.000025  min_lr: 0.000025  loss: 0.7242 (0.7426)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4187 (6.6467)  time: 0.1335  data: 0.0015  max mem: 5591
Epoch: [58] (train) Total time: 0:00:08 (0.1736 s / it)
Averaged stats: lr: 0.000025  min_lr: 0.000025  loss: 0.7242 (0.7244)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4187 (6.6467)
Epoch: [58] (test)  [0/6]  eta: 0:00:03  loss: 0.7948 (0.7948)  time: 0.5124  data: 0.4799  max mem: 5591
Epoch: [58] (test)  [5/6]  eta: 0:00:00  loss: 1.2029 (1.2385)  time: 0.1321  data: 0.1077  max mem: 5591
Epoch: [58] (test) Total time: 0:00:00 (0.1414 s / it)
Averaged stats: loss: 1.2029 (1.2334)
Epoch: [59] (train)  [ 0/50]  eta: 0:00:41  lr: 0.000025  min_lr: 0.000025  loss: 0.5902 (0.5902)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9245 (6.9245)  time: 0.8354  data: 0.7146  max mem: 5591
Epoch: [59] (train)  [10/50]  eta: 0:00:09  lr: 0.000025  min_lr: 0.000025  loss: 0.7111 (0.7419)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.2986 (7.5130)  time: 0.2445  data: 0.0870  max mem: 5591
Epoch: [59] (train)  [20/50]  eta: 0:00:05  lr: 0.000025  min_lr: 0.000025  loss: 0.7278 (0.7641)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3171 (7.6331)  time: 0.1609  data: 0.0168  max mem: 5591
Epoch: [59] (train)  [30/50]  eta: 0:00:03  lr: 0.000024  min_lr: 0.000024  loss: 0.7190 (0.7419)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.7430 (7.1865)  time: 0.1470  data: 0.0128  max mem: 5591
Epoch: [59] (train)  [40/50]  eta: 0:00:01  lr: 0.000024  min_lr: 0.000024  loss: 0.7106 (0.7486)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1612 (7.1152)  time: 0.1685  data: 0.0096  max mem: 5591
Epoch: [59] (train)  [49/50]  eta: 0:00:00  lr: 0.000024  min_lr: 0.000024  loss: 0.7107 (0.7363)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4170 (7.1411)  time: 0.1394  data: 0.0020  max mem: 5591
Epoch: [59] (train) Total time: 0:00:08 (0.1679 s / it)
Averaged stats: lr: 0.000024  min_lr: 0.000024  loss: 0.7107 (0.7310)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4170 (7.1411)
Epoch: [59] (test)  [0/6]  eta: 0:00:02  loss: 1.0833 (1.0833)  time: 0.3619  data: 0.3400  max mem: 5591
Epoch: [59] (test)  [5/6]  eta: 0:00:00  loss: 1.0833 (1.1516)  time: 0.1395  data: 0.1138  max mem: 5591
Epoch: [59] (test) Total time: 0:00:00 (0.1475 s / it)
Averaged stats: loss: 1.0833 (1.1536)
Epoch: [60] (train)  [ 0/50]  eta: 0:00:40  lr: 0.000024  min_lr: 0.000024  loss: 0.7729 (0.7729)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7406 (5.7406)  time: 0.8087  data: 0.3613  max mem: 5591
Epoch: [60] (train)  [10/50]  eta: 0:00:09  lr: 0.000024  min_lr: 0.000024  loss: 0.7210 (0.7115)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9251 (7.2632)  time: 0.2402  data: 0.0406  max mem: 5591
Epoch: [60] (train)  [20/50]  eta: 0:00:05  lr: 0.000024  min_lr: 0.000024  loss: 0.7065 (0.7236)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.9251 (7.2122)  time: 0.1621  data: 0.0045  max mem: 5591
Epoch: [60] (train)  [30/50]  eta: 0:00:03  lr: 0.000023  min_lr: 0.000023  loss: 0.7038 (0.7200)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6334 (6.9747)  time: 0.1552  data: 0.0014  max mem: 5591
Epoch: [60] (train)  [40/50]  eta: 0:00:01  lr: 0.000023  min_lr: 0.000023  loss: 0.7038 (0.7256)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4563 (6.9534)  time: 0.1668  data: 0.0026  max mem: 5591
Epoch: [60] (train)  [49/50]  eta: 0:00:00  lr: 0.000023  min_lr: 0.000023  loss: 0.7144 (0.7304)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4863 (6.9625)  time: 0.1315  data: 0.0023  max mem: 5591
Epoch: [60] (train) Total time: 0:00:08 (0.1667 s / it)
Averaged stats: lr: 0.000023  min_lr: 0.000023  loss: 0.7144 (0.7286)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4863 (6.9625)
Epoch: [60] (test)  [0/6]  eta: 0:00:03  loss: 1.0795 (1.0795)  time: 0.5974  data: 0.5755  max mem: 5591
Epoch: [60] (test)  [5/6]  eta: 0:00:00  loss: 1.0795 (1.0884)  time: 0.1343  data: 0.1138  max mem: 5591
Epoch: [60] (test) Total time: 0:00:00 (0.1428 s / it)
Averaged stats: loss: 1.0795 (1.0771)
Epoch: [61] (train)  [ 0/50]  eta: 0:00:56  lr: 0.000023  min_lr: 0.000023  loss: 0.6209 (0.6209)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 9.8697 (9.8697)  time: 1.1360  data: 0.3461  max mem: 5591
Epoch: [61] (train)  [10/50]  eta: 0:00:09  lr: 0.000023  min_lr: 0.000023  loss: 0.6827 (0.6983)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4953 (7.5663)  time: 0.2484  data: 0.0402  max mem: 5591
Epoch: [61] (train)  [20/50]  eta: 0:00:05  lr: 0.000023  min_lr: 0.000023  loss: 0.6960 (0.7230)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2993 (6.9687)  time: 0.1516  data: 0.0071  max mem: 5591
Epoch: [61] (train)  [30/50]  eta: 0:00:03  lr: 0.000022  min_lr: 0.000022  loss: 0.6960 (0.7145)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9222 (6.8490)  time: 0.1627  data: 0.0033  max mem: 5591
Epoch: [61] (train)  [40/50]  eta: 0:00:01  lr: 0.000022  min_lr: 0.000022  loss: 0.7100 (0.7132)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.4522 (7.0852)  time: 0.1792  data: 0.0028  max mem: 5591
Epoch: [61] (train)  [49/50]  eta: 0:00:00  lr: 0.000022  min_lr: 0.000022  loss: 0.7100 (0.7105)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6443 (7.1175)  time: 0.1433  data: 0.0020  max mem: 5591
Epoch: [61] (train) Total time: 0:00:08 (0.1723 s / it)
Averaged stats: lr: 0.000022  min_lr: 0.000022  loss: 0.7100 (0.7126)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.6443 (7.1175)
Epoch: [61] (test)  [0/6]  eta: 0:00:03  loss: 0.7878 (0.7878)  time: 0.5888  data: 0.5410  max mem: 5591
Epoch: [61] (test)  [5/6]  eta: 0:00:00  loss: 0.8294 (1.0065)  time: 0.1279  data: 0.1008  max mem: 5591
Epoch: [61] (test) Total time: 0:00:00 (0.1347 s / it)
Averaged stats: loss: 0.8294 (1.0720)
Epoch: [62] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000022  min_lr: 0.000022  loss: 0.6947 (0.6947)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7017 (5.7017)  time: 0.9243  data: 0.3639  max mem: 5591
Epoch: [62] (train)  [10/50]  eta: 0:00:09  lr: 0.000022  min_lr: 0.000022  loss: 0.7496 (0.7650)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.3348 (7.7062)  time: 0.2447  data: 0.0366  max mem: 5591
Epoch: [62] (train)  [20/50]  eta: 0:00:05  lr: 0.000022  min_lr: 0.000022  loss: 0.7301 (0.7251)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1058 (6.7698)  time: 0.1605  data: 0.0029  max mem: 5591
Epoch: [62] (train)  [30/50]  eta: 0:00:03  lr: 0.000021  min_lr: 0.000021  loss: 0.6899 (0.7246)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7370 (6.5963)  time: 0.1472  data: 0.0017  max mem: 5591
Epoch: [62] (train)  [40/50]  eta: 0:00:01  lr: 0.000021  min_lr: 0.000021  loss: 0.6631 (0.7085)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3458 (6.4232)  time: 0.1686  data: 0.0030  max mem: 5591
Epoch: [62] (train)  [49/50]  eta: 0:00:00  lr: 0.000021  min_lr: 0.000021  loss: 0.6754 (0.7065)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4375 (6.3164)  time: 0.1379  data: 0.0025  max mem: 5591
Epoch: [62] (train) Total time: 0:00:08 (0.1667 s / it)
Averaged stats: lr: 0.000021  min_lr: 0.000021  loss: 0.6754 (0.6967)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4375 (6.3164)
Epoch: [62] (test)  [0/6]  eta: 0:00:02  loss: 1.0398 (1.0398)  time: 0.3490  data: 0.3268  max mem: 5591
Epoch: [62] (test)  [5/6]  eta: 0:00:00  loss: 1.0398 (1.1053)  time: 0.1514  data: 0.1307  max mem: 5591
Epoch: [62] (test) Total time: 0:00:00 (0.1614 s / it)
Averaged stats: loss: 1.0398 (1.1540)
Epoch: [63] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000021  min_lr: 0.000021  loss: 0.6923 (0.6923)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6018 (5.6018)  time: 1.0347  data: 0.8555  max mem: 5591
Epoch: [63] (train)  [10/50]  eta: 0:00:09  lr: 0.000021  min_lr: 0.000021  loss: 0.6923 (0.7359)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5524 (6.7784)  time: 0.2370  data: 0.0804  max mem: 5591
Epoch: [63] (train)  [20/50]  eta: 0:00:05  lr: 0.000021  min_lr: 0.000021  loss: 0.6816 (0.7261)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5524 (6.9295)  time: 0.1567  data: 0.0017  max mem: 5591
Epoch: [63] (train)  [30/50]  eta: 0:00:03  lr: 0.000020  min_lr: 0.000020  loss: 0.6740 (0.7169)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9397 (6.8282)  time: 0.1629  data: 0.0009  max mem: 5591
Epoch: [63] (train)  [40/50]  eta: 0:00:01  lr: 0.000020  min_lr: 0.000020  loss: 0.6699 (0.7023)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.2271 (6.7232)  time: 0.1693  data: 0.0017  max mem: 5591
Epoch: [63] (train)  [49/50]  eta: 0:00:00  lr: 0.000020  min_lr: 0.000020  loss: 0.6644 (0.6939)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3503 (6.4585)  time: 0.1313  data: 0.0012  max mem: 5591
Epoch: [63] (train) Total time: 0:00:08 (0.1688 s / it)
Averaged stats: lr: 0.000020  min_lr: 0.000020  loss: 0.6644 (0.6791)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3503 (6.4585)
Epoch: [63] (test)  [0/6]  eta: 0:00:03  loss: 1.2015 (1.2015)  time: 0.5589  data: 0.5206  max mem: 5591
Epoch: [63] (test)  [5/6]  eta: 0:00:00  loss: 1.0163 (1.1318)  time: 0.1384  data: 0.1094  max mem: 5591
Epoch: [63] (test) Total time: 0:00:00 (0.1440 s / it)
Averaged stats: loss: 1.0163 (1.1496)
Epoch: [64] (train)  [ 0/50]  eta: 0:00:44  lr: 0.000020  min_lr: 0.000020  loss: 0.7585 (0.7585)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8484 (5.8484)  time: 0.8814  data: 0.7318  max mem: 5591
Epoch: [64] (train)  [10/50]  eta: 0:00:08  lr: 0.000020  min_lr: 0.000020  loss: 0.6876 (0.6871)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4902 (5.5673)  time: 0.2236  data: 0.0758  max mem: 5591
Epoch: [64] (train)  [20/50]  eta: 0:00:05  lr: 0.000020  min_lr: 0.000020  loss: 0.6361 (0.6633)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3221 (6.2081)  time: 0.1551  data: 0.0079  max mem: 5591
Epoch: [64] (train)  [30/50]  eta: 0:00:03  lr: 0.000020  min_lr: 0.000020  loss: 0.6451 (0.6736)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.8900 (6.3748)  time: 0.1651  data: 0.0047  max mem: 5591
Epoch: [64] (train)  [40/50]  eta: 0:00:01  lr: 0.000019  min_lr: 0.000019  loss: 0.6496 (0.7234)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3573 (6.8365)  time: 0.1759  data: 0.0028  max mem: 5591
Epoch: [64] (train)  [49/50]  eta: 0:00:00  lr: 0.000019  min_lr: 0.000019  loss: 0.6631 (0.7204)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3259 (6.7303)  time: 0.1388  data: 0.0012  max mem: 5591
Epoch: [64] (train) Total time: 0:00:08 (0.1677 s / it)
Averaged stats: lr: 0.000019  min_lr: 0.000019  loss: 0.6631 (0.7049)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3259 (6.7303)
Epoch: [64] (test)  [0/6]  eta: 0:00:02  loss: 0.9275 (0.9275)  time: 0.3662  data: 0.3394  max mem: 5591
Epoch: [64] (test)  [5/6]  eta: 0:00:00  loss: 1.1145 (1.0941)  time: 0.1482  data: 0.1271  max mem: 5591
Epoch: [64] (test) Total time: 0:00:00 (0.1602 s / it)
Averaged stats: loss: 1.1145 (1.1028)
Epoch: [65] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000019  min_lr: 0.000019  loss: 0.6766 (0.6766)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5601 (6.5601)  time: 0.9942  data: 0.3302  max mem: 5591
Epoch: [65] (train)  [10/50]  eta: 0:00:10  lr: 0.000019  min_lr: 0.000019  loss: 0.7234 (0.7337)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5601 (6.5961)  time: 0.2568  data: 0.0383  max mem: 5591
Epoch: [65] (train)  [20/50]  eta: 0:00:06  lr: 0.000019  min_lr: 0.000019  loss: 0.7033 (0.7073)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1045 (6.5209)  time: 0.1702  data: 0.0066  max mem: 5591
Epoch: [65] (train)  [30/50]  eta: 0:00:03  lr: 0.000019  min_lr: 0.000019  loss: 0.6419 (0.6954)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.0286 (6.2941)  time: 0.1515  data: 0.0029  max mem: 5591
Epoch: [65] (train)  [40/50]  eta: 0:00:01  lr: 0.000018  min_lr: 0.000018  loss: 0.6510 (0.6845)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.9593 (6.4053)  time: 0.1518  data: 0.0012  max mem: 5591
Epoch: [65] (train)  [49/50]  eta: 0:00:00  lr: 0.000018  min_lr: 0.000018  loss: 0.6951 (0.6879)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5016 (7.0268)  time: 0.1328  data: 0.0026  max mem: 5591
Epoch: [65] (train) Total time: 0:00:08 (0.1689 s / it)
Averaged stats: lr: 0.000018  min_lr: 0.000018  loss: 0.6951 (0.6955)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5016 (7.0268)
Epoch: [65] (test)  [0/6]  eta: 0:00:03  loss: 0.9064 (0.9064)  time: 0.6052  data: 0.5596  max mem: 5591
Epoch: [65] (test)  [5/6]  eta: 0:00:00  loss: 0.9508 (1.0092)  time: 0.1365  data: 0.1073  max mem: 5591
Epoch: [65] (test) Total time: 0:00:00 (0.1442 s / it)
Averaged stats: loss: 0.9508 (1.1624)
Epoch: [66] (train)  [ 0/50]  eta: 0:00:40  lr: 0.000018  min_lr: 0.000018  loss: 0.6527 (0.6527)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1892 (7.1892)  time: 0.8194  data: 0.3338  max mem: 5591
Epoch: [66] (train)  [10/50]  eta: 0:00:09  lr: 0.000018  min_lr: 0.000018  loss: 0.6567 (0.6958)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 7.1892 (7.9648)  time: 0.2286  data: 0.0409  max mem: 5591
Epoch: [66] (train)  [20/50]  eta: 0:00:06  lr: 0.000018  min_lr: 0.000018  loss: 0.6781 (0.7081)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.8202 (7.2717)  time: 0.1717  data: 0.0094  max mem: 5591
Epoch: [66] (train)  [30/50]  eta: 0:00:03  lr: 0.000018  min_lr: 0.000018  loss: 0.6363 (0.6829)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.6620 (6.6680)  time: 0.1669  data: 0.0053  max mem: 5591
Epoch: [66] (train)  [40/50]  eta: 0:00:01  lr: 0.000017  min_lr: 0.000017  loss: 0.6293 (0.6781)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4161 (6.4729)  time: 0.1558  data: 0.0038  max mem: 5591
Epoch: [66] (train)  [49/50]  eta: 0:00:00  lr: 0.000017  min_lr: 0.000017  loss: 0.6095 (0.6690)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1861 (6.3002)  time: 0.1317  data: 0.0036  max mem: 5591
Epoch: [66] (train) Total time: 0:00:08 (0.1674 s / it)
Averaged stats: lr: 0.000017  min_lr: 0.000017  loss: 0.6095 (0.6689)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1861 (6.3002)
Epoch: [66] (test)  [0/6]  eta: 0:00:04  loss: 1.2306 (1.2306)  time: 0.7124  data: 0.6744  max mem: 5591
Epoch: [66] (test)  [5/6]  eta: 0:00:00  loss: 1.1738 (1.1246)  time: 0.1618  data: 0.1285  max mem: 5591
Epoch: [66] (test) Total time: 0:00:01 (0.1711 s / it)
Averaged stats: loss: 1.1738 (1.1235)
Epoch: [67] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000017  min_lr: 0.000017  loss: 0.6690 (0.6690)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9417 (4.9417)  time: 1.0375  data: 0.7554  max mem: 5591
Epoch: [67] (train)  [10/50]  eta: 0:00:09  lr: 0.000017  min_lr: 0.000017  loss: 0.6293 (0.6136)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9417 (5.2636)  time: 0.2293  data: 0.0752  max mem: 5591
Epoch: [67] (train)  [20/50]  eta: 0:00:05  lr: 0.000017  min_lr: 0.000017  loss: 0.6507 (0.6433)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4980 (5.5063)  time: 0.1510  data: 0.0053  max mem: 5591
Epoch: [67] (train)  [30/50]  eta: 0:00:03  lr: 0.000017  min_lr: 0.000017  loss: 0.6442 (0.6314)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7486 (5.6808)  time: 0.1591  data: 0.0020  max mem: 5591
Epoch: [67] (train)  [40/50]  eta: 0:00:01  lr: 0.000017  min_lr: 0.000017  loss: 0.6389 (0.6353)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7222 (5.5402)  time: 0.1591  data: 0.0013  max mem: 5591
Epoch: [67] (train)  [49/50]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000016  loss: 0.6424 (0.6448)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3371 (5.6016)  time: 0.1297  data: 0.0012  max mem: 5591
Epoch: [67] (train) Total time: 0:00:08 (0.1643 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000016  loss: 0.6424 (0.6521)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3371 (5.6016)
Epoch: [67] (test)  [0/6]  eta: 0:00:03  loss: 0.7518 (0.7518)  time: 0.6112  data: 0.5845  max mem: 5591
Epoch: [67] (test)  [5/6]  eta: 0:00:00  loss: 0.9704 (1.0917)  time: 0.1515  data: 0.1302  max mem: 5591
Epoch: [67] (test) Total time: 0:00:00 (0.1602 s / it)
Averaged stats: loss: 0.9704 (1.1364)
Epoch: [68] (train)  [ 0/50]  eta: 0:00:59  lr: 0.000016  min_lr: 0.000016  loss: 0.5864 (0.5864)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5863 (6.5863)  time: 1.1868  data: 0.8683  max mem: 5591
Epoch: [68] (train)  [10/50]  eta: 0:00:09  lr: 0.000016  min_lr: 0.000016  loss: 0.6615 (0.6890)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.5863 (6.5513)  time: 0.2456  data: 0.0794  max mem: 5591
Epoch: [68] (train)  [20/50]  eta: 0:00:06  lr: 0.000016  min_lr: 0.000016  loss: 0.6615 (0.6719)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6862 (7.1348)  time: 0.1514  data: 0.0052  max mem: 5591
Epoch: [68] (train)  [30/50]  eta: 0:00:03  lr: 0.000016  min_lr: 0.000016  loss: 0.6181 (0.6701)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.3613 (6.7210)  time: 0.1571  data: 0.0057  max mem: 5591
Epoch: [68] (train)  [40/50]  eta: 0:00:01  lr: 0.000016  min_lr: 0.000016  loss: 0.6732 (0.6737)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4608 (6.4501)  time: 0.1585  data: 0.0014  max mem: 5591
Epoch: [68] (train)  [49/50]  eta: 0:00:00  lr: 0.000016  min_lr: 0.000016  loss: 0.6433 (0.6700)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1036 (6.3289)  time: 0.1421  data: 0.0009  max mem: 5591
Epoch: [68] (train) Total time: 0:00:08 (0.1669 s / it)
Averaged stats: lr: 0.000016  min_lr: 0.000016  loss: 0.6433 (0.6606)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1036 (6.3289)
Epoch: [68] (test)  [0/6]  eta: 0:00:02  loss: 1.1651 (1.1651)  time: 0.3892  data: 0.3512  max mem: 5591
Epoch: [68] (test)  [5/6]  eta: 0:00:00  loss: 0.9741 (1.1604)  time: 0.1608  data: 0.1353  max mem: 5591
Epoch: [68] (test) Total time: 0:00:01 (0.1693 s / it)
Averaged stats: loss: 0.9741 (1.1397)
Epoch: [69] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000016  min_lr: 0.000016  loss: 0.5889 (0.5889)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1764 (5.1764)  time: 0.9293  data: 0.5945  max mem: 5591
Epoch: [69] (train)  [10/50]  eta: 0:00:09  lr: 0.000015  min_lr: 0.000015  loss: 0.6423 (0.6570)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7627 (5.8614)  time: 0.2310  data: 0.0567  max mem: 5591
Epoch: [69] (train)  [20/50]  eta: 0:00:05  lr: 0.000015  min_lr: 0.000015  loss: 0.6180 (0.6320)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2589 (5.4432)  time: 0.1492  data: 0.0030  max mem: 5591
Epoch: [69] (train)  [30/50]  eta: 0:00:03  lr: 0.000015  min_lr: 0.000015  loss: 0.6080 (0.6577)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4990 (5.6785)  time: 0.1562  data: 0.0024  max mem: 5591
Epoch: [69] (train)  [40/50]  eta: 0:00:01  lr: 0.000015  min_lr: 0.000015  loss: 0.6409 (0.6608)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5322 (5.9175)  time: 0.1639  data: 0.0045  max mem: 5591
Epoch: [69] (train)  [49/50]  eta: 0:00:00  lr: 0.000015  min_lr: 0.000015  loss: 0.6343 (0.6553)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7532 (5.9408)  time: 0.1318  data: 0.0038  max mem: 5591
Epoch: [69] (train) Total time: 0:00:08 (0.1655 s / it)
Averaged stats: lr: 0.000015  min_lr: 0.000015  loss: 0.6343 (0.6502)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7532 (5.9408)
Epoch: [69] (test)  [0/6]  eta: 0:00:02  loss: 0.8954 (0.8954)  time: 0.3887  data: 0.3556  max mem: 5591
Epoch: [69] (test)  [5/6]  eta: 0:00:00  loss: 1.0111 (1.0832)  time: 0.1300  data: 0.1050  max mem: 5591
Epoch: [69] (test) Total time: 0:00:00 (0.1407 s / it)
Averaged stats: loss: 1.0111 (1.1862)
Epoch: [70] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000015  min_lr: 0.000015  loss: 0.6535 (0.6535)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7843 (5.7843)  time: 0.9045  data: 0.7206  max mem: 5591
Epoch: [70] (train)  [10/50]  eta: 0:00:09  lr: 0.000014  min_lr: 0.000014  loss: 0.6402 (0.6349)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.2934 (5.2491)  time: 0.2261  data: 0.0836  max mem: 5591
Epoch: [70] (train)  [20/50]  eta: 0:00:05  lr: 0.000014  min_lr: 0.000014  loss: 0.6379 (0.6737)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1839 (6.1108)  time: 0.1590  data: 0.0139  max mem: 5591
Epoch: [70] (train)  [30/50]  eta: 0:00:03  lr: 0.000014  min_lr: 0.000014  loss: 0.6272 (0.6664)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.1910 (6.6127)  time: 0.1543  data: 0.0067  max mem: 5591
Epoch: [70] (train)  [40/50]  eta: 0:00:01  lr: 0.000014  min_lr: 0.000014  loss: 0.6436 (0.6652)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 6.6280 (6.6989)  time: 0.1483  data: 0.0047  max mem: 5591
Epoch: [70] (train)  [49/50]  eta: 0:00:00  lr: 0.000014  min_lr: 0.000014  loss: 0.6265 (0.6544)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3233 (6.4081)  time: 0.1254  data: 0.0037  max mem: 5591
Epoch: [70] (train) Total time: 0:00:08 (0.1616 s / it)
Averaged stats: lr: 0.000014  min_lr: 0.000014  loss: 0.6265 (0.6432)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3233 (6.4081)
Epoch: [70] (test)  [0/6]  eta: 0:00:02  loss: 0.8712 (0.8712)  time: 0.3822  data: 0.3591  max mem: 5591
Epoch: [70] (test)  [5/6]  eta: 0:00:00  loss: 1.0812 (1.0768)  time: 0.1458  data: 0.1225  max mem: 5591
Epoch: [70] (test) Total time: 0:00:00 (0.1576 s / it)
Averaged stats: loss: 1.0812 (1.1067)
Epoch: [71] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000014  min_lr: 0.000014  loss: 0.6541 (0.6541)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4924 (5.4924)  time: 0.9850  data: 0.6043  max mem: 5591
Epoch: [71] (train)  [10/50]  eta: 0:00:09  lr: 0.000014  min_lr: 0.000014  loss: 0.6321 (0.6378)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.7589 (5.8644)  time: 0.2390  data: 0.0568  max mem: 5591
Epoch: [71] (train)  [20/50]  eta: 0:00:06  lr: 0.000013  min_lr: 0.000013  loss: 0.6215 (0.6342)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.0358 (5.5743)  time: 0.1681  data: 0.0020  max mem: 5591
Epoch: [71] (train)  [30/50]  eta: 0:00:03  lr: 0.000013  min_lr: 0.000013  loss: 0.6215 (0.6272)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9523 (5.5104)  time: 0.1621  data: 0.0013  max mem: 5591
Epoch: [71] (train)  [40/50]  eta: 0:00:01  lr: 0.000013  min_lr: 0.000013  loss: 0.5886 (0.6236)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9474 (5.3727)  time: 0.1552  data: 0.0013  max mem: 5591
Epoch: [71] (train)  [49/50]  eta: 0:00:00  lr: 0.000013  min_lr: 0.000013  loss: 0.5865 (0.6205)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6596 (5.2827)  time: 0.1414  data: 0.0012  max mem: 5591
Epoch: [71] (train) Total time: 0:00:08 (0.1696 s / it)
Averaged stats: lr: 0.000013  min_lr: 0.000013  loss: 0.5865 (0.6249)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6596 (5.2827)
Epoch: [71] (test)  [0/6]  eta: 0:00:02  loss: 0.8059 (0.8059)  time: 0.3683  data: 0.3297  max mem: 5591
Epoch: [71] (test)  [5/6]  eta: 0:00:00  loss: 0.9249 (1.0790)  time: 0.1421  data: 0.1174  max mem: 5591
Epoch: [71] (test) Total time: 0:00:00 (0.1463 s / it)
Averaged stats: loss: 0.9249 (1.1607)
Epoch: [72] (train)  [ 0/50]  eta: 0:00:50  lr: 0.000013  min_lr: 0.000013  loss: 0.7315 (0.7315)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.5103 (5.5103)  time: 1.0171  data: 0.7726  max mem: 5591
Epoch: [72] (train)  [10/50]  eta: 0:00:09  lr: 0.000013  min_lr: 0.000013  loss: 0.6174 (0.6237)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6268 (4.6083)  time: 0.2397  data: 0.1041  max mem: 5591
Epoch: [72] (train)  [20/50]  eta: 0:00:06  lr: 0.000013  min_lr: 0.000013  loss: 0.6071 (0.6218)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1532 (4.7633)  time: 0.1671  data: 0.0448  max mem: 5591
Epoch: [72] (train)  [30/50]  eta: 0:00:03  lr: 0.000012  min_lr: 0.000012  loss: 0.6007 (0.6157)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5630 (4.8204)  time: 0.1637  data: 0.0271  max mem: 5591
Epoch: [72] (train)  [40/50]  eta: 0:00:01  lr: 0.000012  min_lr: 0.000012  loss: 0.6072 (0.6164)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5700 (4.7715)  time: 0.1593  data: 0.0026  max mem: 5591
Epoch: [72] (train)  [49/50]  eta: 0:00:00  lr: 0.000012  min_lr: 0.000012  loss: 0.5759 (0.6137)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9620 (4.9956)  time: 0.1364  data: 0.0018  max mem: 5591
Epoch: [72] (train) Total time: 0:00:08 (0.1730 s / it)
Averaged stats: lr: 0.000012  min_lr: 0.000012  loss: 0.5759 (0.6193)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9620 (4.9956)
Epoch: [72] (test)  [0/6]  eta: 0:00:04  loss: 0.9925 (0.9925)  time: 0.6690  data: 0.6224  max mem: 5591
Epoch: [72] (test)  [5/6]  eta: 0:00:00  loss: 0.9925 (1.1771)  time: 0.1487  data: 0.1147  max mem: 5591
Epoch: [72] (test) Total time: 0:00:00 (0.1557 s / it)
Averaged stats: loss: 0.9925 (1.1184)
Epoch: [73] (train)  [ 0/50]  eta: 0:00:43  lr: 0.000012  min_lr: 0.000012  loss: 0.6758 (0.6758)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1893 (5.1893)  time: 0.8624  data: 0.5929  max mem: 5591
Epoch: [73] (train)  [10/50]  eta: 0:00:09  lr: 0.000012  min_lr: 0.000012  loss: 0.6213 (0.6307)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1893 (5.3993)  time: 0.2395  data: 0.0826  max mem: 5591
Epoch: [73] (train)  [20/50]  eta: 0:00:06  lr: 0.000012  min_lr: 0.000012  loss: 0.5979 (0.6112)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3168 (5.5693)  time: 0.1724  data: 0.0305  max mem: 5591
Epoch: [73] (train)  [30/50]  eta: 0:00:03  lr: 0.000012  min_lr: 0.000012  loss: 0.5679 (0.6124)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.3168 (5.5277)  time: 0.1542  data: 0.0188  max mem: 5591
Epoch: [73] (train)  [40/50]  eta: 0:00:01  lr: 0.000012  min_lr: 0.000012  loss: 0.5679 (0.6027)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9847 (5.3730)  time: 0.1636  data: 0.0070  max mem: 5591
Epoch: [73] (train)  [49/50]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000011  loss: 0.5959 (0.6244)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6018 (5.3542)  time: 0.1404  data: 0.0031  max mem: 5591
Epoch: [73] (train) Total time: 0:00:08 (0.1706 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 0.5959 (0.6163)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6018 (5.3542)
Epoch: [73] (test)  [0/6]  eta: 0:00:02  loss: 1.2014 (1.2014)  time: 0.3952  data: 0.3466  max mem: 5591
Epoch: [73] (test)  [5/6]  eta: 0:00:00  loss: 0.9776 (1.0704)  time: 0.1485  data: 0.1212  max mem: 5591
Epoch: [73] (test) Total time: 0:00:00 (0.1564 s / it)
Averaged stats: loss: 0.9776 (1.0878)
Epoch: [74] (train)  [ 0/50]  eta: 0:00:53  lr: 0.000011  min_lr: 0.000011  loss: 0.4755 (0.4755)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.4348 (5.4348)  time: 1.0613  data: 0.4084  max mem: 5591
Epoch: [74] (train)  [10/50]  eta: 0:00:10  lr: 0.000011  min_lr: 0.000011  loss: 0.5445 (0.5669)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6434 (4.7505)  time: 0.2537  data: 0.0433  max mem: 5591
Epoch: [74] (train)  [20/50]  eta: 0:00:06  lr: 0.000011  min_lr: 0.000011  loss: 0.5757 (0.5766)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3313 (4.6353)  time: 0.1589  data: 0.0043  max mem: 5591
Epoch: [74] (train)  [30/50]  eta: 0:00:03  lr: 0.000011  min_lr: 0.000011  loss: 0.5885 (0.5819)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3705 (4.6539)  time: 0.1534  data: 0.0012  max mem: 5591
Epoch: [74] (train)  [40/50]  eta: 0:00:01  lr: 0.000011  min_lr: 0.000011  loss: 0.5932 (0.5847)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2770 (4.5481)  time: 0.1732  data: 0.0019  max mem: 5591
Epoch: [74] (train)  [49/50]  eta: 0:00:00  lr: 0.000011  min_lr: 0.000011  loss: 0.6145 (0.5941)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1910 (4.5266)  time: 0.1420  data: 0.0024  max mem: 5591
Epoch: [74] (train) Total time: 0:00:08 (0.1718 s / it)
Averaged stats: lr: 0.000011  min_lr: 0.000011  loss: 0.6145 (0.6007)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1910 (4.5266)
Epoch: [74] (test)  [0/6]  eta: 0:00:04  loss: 1.1177 (1.1177)  time: 0.8090  data: 0.7869  max mem: 5591
Epoch: [74] (test)  [5/6]  eta: 0:00:00  loss: 0.9587 (1.0560)  time: 0.1664  data: 0.1462  max mem: 5591
Epoch: [74] (test) Total time: 0:00:01 (0.1713 s / it)
Averaged stats: loss: 0.9587 (1.0779)
Saving model at epoch 75 in /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
Epoch: [75] (train)  [ 0/50]  eta: 0:00:33  lr: 0.000011  min_lr: 0.000011  loss: 0.4967 (0.4967)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2770 (4.2770)  time: 0.6684  data: 0.5838  max mem: 5591
Epoch: [75] (train)  [10/50]  eta: 0:00:07  lr: 0.000010  min_lr: 0.000010  loss: 0.5897 (0.5912)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6976 (4.5923)  time: 0.1996  data: 0.0578  max mem: 5591
Epoch: [75] (train)  [20/50]  eta: 0:00:04  lr: 0.000010  min_lr: 0.000010  loss: 0.6059 (0.5993)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7073 (4.9987)  time: 0.1382  data: 0.0034  max mem: 5591
Epoch: [75] (train)  [30/50]  eta: 0:00:03  lr: 0.000010  min_lr: 0.000010  loss: 0.5892 (0.6041)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9130 (4.9946)  time: 0.1515  data: 0.0107  max mem: 5591
Epoch: [75] (train)  [40/50]  eta: 0:00:01  lr: 0.000010  min_lr: 0.000010  loss: 0.6023 (0.6076)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.6106 (5.0240)  time: 0.1771  data: 0.0122  max mem: 5591
Epoch: [75] (train)  [49/50]  eta: 0:00:00  lr: 0.000010  min_lr: 0.000010  loss: 0.6137 (0.6104)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4862 (4.9294)  time: 0.1349  data: 0.0026  max mem: 5591
Epoch: [75] (train) Total time: 0:00:07 (0.1571 s / it)
Averaged stats: lr: 0.000010  min_lr: 0.000010  loss: 0.6137 (0.6123)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4862 (4.9294)
Epoch: [75] (test)  [0/6]  eta: 0:00:04  loss: 0.7314 (0.7314)  time: 0.7841  data: 0.7623  max mem: 5591
Epoch: [75] (test)  [5/6]  eta: 0:00:00  loss: 0.8640 (1.0234)  time: 0.1678  data: 0.1478  max mem: 5591
Epoch: [75] (test) Total time: 0:00:01 (0.1747 s / it)
Averaged stats: loss: 0.8640 (1.1121)
Epoch: [76] (train)  [ 0/50]  eta: 0:00:44  lr: 0.000010  min_lr: 0.000010  loss: 0.6791 (0.6791)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4002 (4.4002)  time: 0.8997  data: 0.5414  max mem: 5591
Epoch: [76] (train)  [10/50]  eta: 0:00:08  lr: 0.000010  min_lr: 0.000010  loss: 0.6427 (0.6184)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3376 (4.3833)  time: 0.2185  data: 0.0544  max mem: 5591
Epoch: [76] (train)  [20/50]  eta: 0:00:05  lr: 0.000010  min_lr: 0.000010  loss: 0.5861 (0.6034)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3376 (4.5008)  time: 0.1649  data: 0.0038  max mem: 5591
Epoch: [76] (train)  [30/50]  eta: 0:00:03  lr: 0.000009  min_lr: 0.000009  loss: 0.5784 (0.5942)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9901 (4.3697)  time: 0.1696  data: 0.0026  max mem: 5591
Epoch: [76] (train)  [40/50]  eta: 0:00:01  lr: 0.000009  min_lr: 0.000009  loss: 0.5687 (0.5865)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8971 (4.2907)  time: 0.1727  data: 0.0054  max mem: 5591
Epoch: [76] (train)  [49/50]  eta: 0:00:00  lr: 0.000009  min_lr: 0.000009  loss: 0.5594 (0.5886)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2155 (4.3429)  time: 0.1396  data: 0.0046  max mem: 5591
Epoch: [76] (train) Total time: 0:00:08 (0.1703 s / it)
Averaged stats: lr: 0.000009  min_lr: 0.000009  loss: 0.5594 (0.6048)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2155 (4.3429)
Epoch: [76] (test)  [0/6]  eta: 0:00:04  loss: 1.0853 (1.0853)  time: 0.7606  data: 0.7238  max mem: 5591
Epoch: [76] (test)  [5/6]  eta: 0:00:00  loss: 1.0853 (1.1155)  time: 0.1619  data: 0.1368  max mem: 5591
Epoch: [76] (test) Total time: 0:00:01 (0.1685 s / it)
Averaged stats: loss: 1.0853 (1.1399)
Epoch: [77] (train)  [ 0/50]  eta: 0:00:54  lr: 0.000009  min_lr: 0.000009  loss: 0.5194 (0.5194)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6388 (3.6388)  time: 1.0845  data: 0.5717  max mem: 5591
Epoch: [77] (train)  [10/50]  eta: 0:00:09  lr: 0.000009  min_lr: 0.000009  loss: 0.5927 (0.6212)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2946 (4.6526)  time: 0.2356  data: 0.0565  max mem: 5591
Epoch: [77] (train)  [20/50]  eta: 0:00:06  lr: 0.000009  min_lr: 0.000009  loss: 0.5652 (0.5929)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7434 (4.7067)  time: 0.1640  data: 0.0032  max mem: 5591
Epoch: [77] (train)  [30/50]  eta: 0:00:03  lr: 0.000009  min_lr: 0.000009  loss: 0.5546 (0.5965)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.7434 (4.8328)  time: 0.1679  data: 0.0017  max mem: 5591
Epoch: [77] (train)  [40/50]  eta: 0:00:01  lr: 0.000009  min_lr: 0.000009  loss: 0.5981 (0.5970)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3908 (4.6585)  time: 0.1623  data: 0.0019  max mem: 5591
Epoch: [77] (train)  [49/50]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000008  loss: 0.5996 (0.5987)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3083 (4.6267)  time: 0.1279  data: 0.0012  max mem: 5591
Epoch: [77] (train) Total time: 0:00:08 (0.1691 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000008  loss: 0.5996 (0.6036)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3083 (4.6267)
Epoch: [77] (test)  [0/6]  eta: 0:00:03  loss: 0.8846 (0.8846)  time: 0.6225  data: 0.6001  max mem: 5591
Epoch: [77] (test)  [5/6]  eta: 0:00:00  loss: 0.8846 (0.9657)  time: 0.1494  data: 0.1291  max mem: 5591
Epoch: [77] (test) Total time: 0:00:00 (0.1563 s / it)
Averaged stats: loss: 0.8846 (1.0021)
Epoch: [78] (train)  [ 0/50]  eta: 0:00:56  lr: 0.000008  min_lr: 0.000008  loss: 0.5749 (0.5749)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.9799 (4.9799)  time: 1.1328  data: 0.6892  max mem: 5591
Epoch: [78] (train)  [10/50]  eta: 0:00:09  lr: 0.000008  min_lr: 0.000008  loss: 0.5956 (0.6227)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2855 (4.9330)  time: 0.2304  data: 0.0632  max mem: 5591
Epoch: [78] (train)  [20/50]  eta: 0:00:06  lr: 0.000008  min_lr: 0.000008  loss: 0.5840 (0.6101)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2855 (4.7059)  time: 0.1660  data: 0.0021  max mem: 5591
Epoch: [78] (train)  [30/50]  eta: 0:00:03  lr: 0.000008  min_lr: 0.000008  loss: 0.5678 (0.6068)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3193 (4.6026)  time: 0.1643  data: 0.0035  max mem: 5591
Epoch: [78] (train)  [40/50]  eta: 0:00:01  lr: 0.000008  min_lr: 0.000008  loss: 0.5709 (0.6037)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1339 (4.4746)  time: 0.1548  data: 0.0036  max mem: 5591
Epoch: [78] (train)  [49/50]  eta: 0:00:00  lr: 0.000008  min_lr: 0.000008  loss: 0.6013 (0.6016)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1532 (4.4887)  time: 0.1335  data: 0.0031  max mem: 5591
Epoch: [78] (train) Total time: 0:00:08 (0.1696 s / it)
Averaged stats: lr: 0.000008  min_lr: 0.000008  loss: 0.6013 (0.5939)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1532 (4.4887)
Epoch: [78] (test)  [0/6]  eta: 0:00:03  loss: 1.0320 (1.0320)  time: 0.5993  data: 0.5688  max mem: 5591
Epoch: [78] (test)  [5/6]  eta: 0:00:00  loss: 0.8910 (1.0468)  time: 0.1493  data: 0.1254  max mem: 5591
Epoch: [78] (test) Total time: 0:00:00 (0.1581 s / it)
Averaged stats: loss: 0.8910 (1.0567)
Epoch: [79] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000008  min_lr: 0.000008  loss: 0.5282 (0.5282)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 5.1466 (5.1466)  time: 1.0206  data: 0.5302  max mem: 5591
Epoch: [79] (train)  [10/50]  eta: 0:00:08  lr: 0.000008  min_lr: 0.000008  loss: 0.5772 (0.5809)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1029 (4.3620)  time: 0.2235  data: 0.0518  max mem: 5591
Epoch: [79] (train)  [20/50]  eta: 0:00:05  lr: 0.000008  min_lr: 0.000008  loss: 0.5633 (0.5800)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0273 (4.1873)  time: 0.1550  data: 0.0041  max mem: 5591
Epoch: [79] (train)  [30/50]  eta: 0:00:03  lr: 0.000007  min_lr: 0.000007  loss: 0.5618 (0.5893)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9013 (4.1401)  time: 0.1636  data: 0.0059  max mem: 5591
Epoch: [79] (train)  [40/50]  eta: 0:00:01  lr: 0.000007  min_lr: 0.000007  loss: 0.5813 (0.5900)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0144 (4.1400)  time: 0.1606  data: 0.0058  max mem: 5591
Epoch: [79] (train)  [49/50]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000007  loss: 0.5851 (0.5904)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0144 (4.1574)  time: 0.1284  data: 0.0030  max mem: 5591
Epoch: [79] (train) Total time: 0:00:08 (0.1651 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000007  loss: 0.5851 (0.5830)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0144 (4.1574)
Epoch: [79] (test)  [0/6]  eta: 0:00:02  loss: 0.6920 (0.6920)  time: 0.4958  data: 0.4594  max mem: 5591
Epoch: [79] (test)  [5/6]  eta: 0:00:00  loss: 0.9276 (0.9859)  time: 0.1415  data: 0.1174  max mem: 5591
Epoch: [79] (test) Total time: 0:00:00 (0.1484 s / it)
Averaged stats: loss: 0.9276 (1.0729)
Epoch: [80] (train)  [ 0/50]  eta: 0:00:48  lr: 0.000007  min_lr: 0.000007  loss: 0.5162 (0.5162)  loss_scale: 1024.0000 (1024.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4345 (3.4345)  time: 0.9720  data: 0.5372  max mem: 5591
Epoch: [80] (train)  [10/50]  eta: 0:00:09  lr: 0.000007  min_lr: 0.000007  loss: 0.5971 (0.6083)  loss_scale: 1024.0000 (1396.3636)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0609 (4.0599)  time: 0.2317  data: 0.0518  max mem: 5591
Epoch: [80] (train)  [20/50]  eta: 0:00:05  lr: 0.000007  min_lr: 0.000007  loss: 0.5901 (0.5882)  loss_scale: 2048.0000 (1706.6667)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0308 (4.6112)  time: 0.1587  data: 0.0030  max mem: 5591
Epoch: [80] (train)  [30/50]  eta: 0:00:03  lr: 0.000007  min_lr: 0.000007  loss: 0.6092 (0.6003)  loss_scale: 2048.0000 (1816.7742)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.3464 (4.5705)  time: 0.1618  data: 0.0027  max mem: 5591
Epoch: [80] (train)  [40/50]  eta: 0:00:01  lr: 0.000007  min_lr: 0.000007  loss: 0.5979 (0.5946)  loss_scale: 2048.0000 (1873.1707)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5613 (4.5700)  time: 0.1513  data: 0.0016  max mem: 5591
Epoch: [80] (train)  [49/50]  eta: 0:00:00  lr: 0.000007  min_lr: 0.000007  loss: 0.5979 (0.5970)  loss_scale: 2048.0000 (1904.6400)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5843 (4.5792)  time: 0.1214  data: 0.0005  max mem: 5591
Epoch: [80] (train) Total time: 0:00:08 (0.1633 s / it)
Averaged stats: lr: 0.000007  min_lr: 0.000007  loss: 0.5979 (0.5924)  loss_scale: 2048.0000 (1904.6400)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.5843 (4.5792)
Epoch: [80] (test)  [0/6]  eta: 0:00:03  loss: 1.1193 (1.1193)  time: 0.6625  data: 0.6408  max mem: 5591
Epoch: [80] (test)  [5/6]  eta: 0:00:00  loss: 1.0814 (1.1183)  time: 0.1651  data: 0.1388  max mem: 5591
Epoch: [80] (test) Total time: 0:00:01 (0.1750 s / it)
Averaged stats: loss: 1.0814 (1.1592)
Epoch: [81] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000007  min_lr: 0.000007  loss: 0.6577 (0.6577)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0223 (4.0223)  time: 0.9271  data: 0.3426  max mem: 5591
Epoch: [81] (train)  [10/50]  eta: 0:00:09  lr: 0.000006  min_lr: 0.000006  loss: 0.5737 (0.5839)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.4014 (4.8369)  time: 0.2328  data: 0.0355  max mem: 5591
Epoch: [81] (train)  [20/50]  eta: 0:00:05  lr: 0.000006  min_lr: 0.000006  loss: 0.5932 (0.5961)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.2088 (4.4943)  time: 0.1628  data: 0.0040  max mem: 5591
Epoch: [81] (train)  [30/50]  eta: 0:00:03  lr: 0.000006  min_lr: 0.000006  loss: 0.6078 (0.6005)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9365 (4.3289)  time: 0.1662  data: 0.0029  max mem: 5591
Epoch: [81] (train)  [40/50]  eta: 0:00:01  lr: 0.000006  min_lr: 0.000006  loss: 0.6029 (0.6014)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7878 (4.2888)  time: 0.1535  data: 0.0023  max mem: 5591
Epoch: [81] (train)  [49/50]  eta: 0:00:00  lr: 0.000006  min_lr: 0.000006  loss: 0.5770 (0.6000)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8406 (4.2798)  time: 0.1190  data: 0.0012  max mem: 5591
Epoch: [81] (train) Total time: 0:00:08 (0.1649 s / it)
Averaged stats: lr: 0.000006  min_lr: 0.000006  loss: 0.5770 (0.5814)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8406 (4.2798)
Epoch: [81] (test)  [0/6]  eta: 0:00:03  loss: 0.9327 (0.9327)  time: 0.6024  data: 0.5795  max mem: 5591
Epoch: [81] (test)  [5/6]  eta: 0:00:00  loss: 1.0665 (1.1139)  time: 0.1583  data: 0.1366  max mem: 5591
Epoch: [81] (test) Total time: 0:00:00 (0.1647 s / it)
Averaged stats: loss: 1.0665 (1.0998)
Epoch: [82] (train)  [ 0/50]  eta: 0:00:47  lr: 0.000006  min_lr: 0.000006  loss: 0.7227 (0.7227)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1170 (4.1170)  time: 0.9443  data: 0.7300  max mem: 5591
Epoch: [82] (train)  [10/50]  eta: 0:00:09  lr: 0.000006  min_lr: 0.000006  loss: 0.6034 (0.5926)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6953 (3.9633)  time: 0.2329  data: 0.0669  max mem: 5591
Epoch: [82] (train)  [20/50]  eta: 0:00:05  lr: 0.000006  min_lr: 0.000006  loss: 0.5858 (0.5789)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6953 (3.9715)  time: 0.1559  data: 0.0018  max mem: 5591
Epoch: [82] (train)  [30/50]  eta: 0:00:03  lr: 0.000006  min_lr: 0.000006  loss: 0.5901 (0.5924)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0888 (4.2300)  time: 0.1572  data: 0.0035  max mem: 5591
Epoch: [82] (train)  [40/50]  eta: 0:00:01  lr: 0.000005  min_lr: 0.000005  loss: 0.5815 (0.5902)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9239 (4.1438)  time: 0.1587  data: 0.0032  max mem: 5591
Epoch: [82] (train)  [49/50]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000005  loss: 0.5677 (0.5895)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8272 (4.1021)  time: 0.1316  data: 0.0026  max mem: 5591
Epoch: [82] (train) Total time: 0:00:08 (0.1655 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000005  loss: 0.5677 (0.5862)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8272 (4.1021)
Epoch: [82] (test)  [0/6]  eta: 0:00:04  loss: 0.8801 (0.8801)  time: 0.7573  data: 0.7354  max mem: 5591
Epoch: [82] (test)  [5/6]  eta: 0:00:00  loss: 1.0167 (1.1445)  time: 0.1634  data: 0.1434  max mem: 5591
Epoch: [82] (test) Total time: 0:00:01 (0.1702 s / it)
Averaged stats: loss: 1.0167 (1.0966)
Epoch: [83] (train)  [ 0/50]  eta: 0:00:41  lr: 0.000005  min_lr: 0.000005  loss: 0.6031 (0.6031)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1588 (4.1588)  time: 0.8271  data: 0.3307  max mem: 5591
Epoch: [83] (train)  [10/50]  eta: 0:00:09  lr: 0.000005  min_lr: 0.000005  loss: 0.5551 (0.5688)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8163 (3.9554)  time: 0.2284  data: 0.0306  max mem: 5591
Epoch: [83] (train)  [20/50]  eta: 0:00:05  lr: 0.000005  min_lr: 0.000005  loss: 0.5551 (0.5792)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8016 (3.8714)  time: 0.1663  data: 0.0022  max mem: 5591
Epoch: [83] (train)  [30/50]  eta: 0:00:03  lr: 0.000005  min_lr: 0.000005  loss: 0.5728 (0.5795)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8493 (3.9331)  time: 0.1588  data: 0.0039  max mem: 5591
Epoch: [83] (train)  [40/50]  eta: 0:00:01  lr: 0.000005  min_lr: 0.000005  loss: 0.5733 (0.5800)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.9257 (3.9297)  time: 0.1435  data: 0.0036  max mem: 5591
Epoch: [83] (train)  [49/50]  eta: 0:00:00  lr: 0.000005  min_lr: 0.000005  loss: 0.5865 (0.5804)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8666 (3.9343)  time: 0.1205  data: 0.0019  max mem: 5591
Epoch: [83] (train) Total time: 0:00:08 (0.1620 s / it)
Averaged stats: lr: 0.000005  min_lr: 0.000005  loss: 0.5865 (0.5770)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8666 (3.9343)
Epoch: [83] (test)  [0/6]  eta: 0:00:03  loss: 0.7227 (0.7227)  time: 0.5354  data: 0.5041  max mem: 5591
Epoch: [83] (test)  [5/6]  eta: 0:00:00  loss: 1.0453 (1.0433)  time: 0.1380  data: 0.1064  max mem: 5591
Epoch: [83] (test) Total time: 0:00:00 (0.1457 s / it)
Averaged stats: loss: 1.0453 (1.1017)
Epoch: [84] (train)  [ 0/50]  eta: 0:00:37  lr: 0.000005  min_lr: 0.000005  loss: 0.5695 (0.5695)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.1253 (4.1253)  time: 0.7436  data: 0.3948  max mem: 5591
Epoch: [84] (train)  [10/50]  eta: 0:00:08  lr: 0.000005  min_lr: 0.000005  loss: 0.5702 (0.5649)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0785 (4.1469)  time: 0.2227  data: 0.0397  max mem: 5591
Epoch: [84] (train)  [20/50]  eta: 0:00:05  lr: 0.000005  min_lr: 0.000005  loss: 0.5702 (0.5680)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7548 (3.9922)  time: 0.1652  data: 0.0050  max mem: 5591
Epoch: [84] (train)  [30/50]  eta: 0:00:03  lr: 0.000005  min_lr: 0.000005  loss: 0.5597 (0.5708)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6982 (3.9860)  time: 0.1489  data: 0.0039  max mem: 5591
Epoch: [84] (train)  [40/50]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000004  loss: 0.5597 (0.5713)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6535 (3.8944)  time: 0.1650  data: 0.0033  max mem: 5591
Epoch: [84] (train)  [49/50]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000004  loss: 0.5606 (0.5757)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6207 (3.9129)  time: 0.1408  data: 0.0033  max mem: 5591
Epoch: [84] (train) Total time: 0:00:08 (0.1645 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000004  loss: 0.5606 (0.5740)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6207 (3.9129)
Epoch: [84] (test)  [0/6]  eta: 0:00:02  loss: 1.1637 (1.1637)  time: 0.3693  data: 0.3310  max mem: 5591
Epoch: [84] (test)  [5/6]  eta: 0:00:00  loss: 0.8759 (0.9918)  time: 0.1568  data: 0.1338  max mem: 5591
Epoch: [84] (test) Total time: 0:00:00 (0.1646 s / it)
Averaged stats: loss: 0.8759 (1.0842)
Epoch: [85] (train)  [ 0/50]  eta: 0:00:52  lr: 0.000004  min_lr: 0.000004  loss: 0.4861 (0.4861)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2479 (3.2479)  time: 1.0410  data: 0.9135  max mem: 5591
Epoch: [85] (train)  [10/50]  eta: 0:00:10  lr: 0.000004  min_lr: 0.000004  loss: 0.5548 (0.5446)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6264 (3.5702)  time: 0.2507  data: 0.0872  max mem: 5591
Epoch: [85] (train)  [20/50]  eta: 0:00:05  lr: 0.000004  min_lr: 0.000004  loss: 0.5548 (0.5605)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6264 (3.6504)  time: 0.1564  data: 0.0032  max mem: 5591
Epoch: [85] (train)  [30/50]  eta: 0:00:03  lr: 0.000004  min_lr: 0.000004  loss: 0.5557 (0.5600)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5953 (3.6768)  time: 0.1536  data: 0.0037  max mem: 5591
Epoch: [85] (train)  [40/50]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000004  loss: 0.5722 (0.5644)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4032 (3.6622)  time: 0.1620  data: 0.0044  max mem: 5591
Epoch: [85] (train)  [49/50]  eta: 0:00:00  lr: 0.000004  min_lr: 0.000004  loss: 0.5734 (0.5644)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4857 (3.6928)  time: 0.1281  data: 0.0025  max mem: 5591
Epoch: [85] (train) Total time: 0:00:08 (0.1664 s / it)
Averaged stats: lr: 0.000004  min_lr: 0.000004  loss: 0.5734 (0.5712)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4857 (3.6928)
Epoch: [85] (test)  [0/6]  eta: 0:00:02  loss: 0.9258 (0.9258)  time: 0.3944  data: 0.3574  max mem: 5591
Epoch: [85] (test)  [5/6]  eta: 0:00:00  loss: 0.9350 (1.1421)  time: 0.1531  data: 0.1304  max mem: 5591
Epoch: [85] (test) Total time: 0:00:00 (0.1586 s / it)
Averaged stats: loss: 0.9350 (1.1588)
Epoch: [86] (train)  [ 0/50]  eta: 0:00:47  lr: 0.000004  min_lr: 0.000004  loss: 0.4824 (0.4824)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 2.9610 (2.9610)  time: 0.9414  data: 0.6245  max mem: 5591
Epoch: [86] (train)  [10/50]  eta: 0:00:10  lr: 0.000004  min_lr: 0.000004  loss: 0.5504 (0.5499)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6724 (3.5927)  time: 0.2500  data: 0.0599  max mem: 5591
Epoch: [86] (train)  [20/50]  eta: 0:00:06  lr: 0.000004  min_lr: 0.000004  loss: 0.5472 (0.5431)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6724 (3.6109)  time: 0.1694  data: 0.0027  max mem: 5591
Epoch: [86] (train)  [30/50]  eta: 0:00:03  lr: 0.000004  min_lr: 0.000004  loss: 0.5472 (0.5518)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5969 (3.5828)  time: 0.1641  data: 0.0033  max mem: 5591
Epoch: [86] (train)  [40/50]  eta: 0:00:01  lr: 0.000004  min_lr: 0.000004  loss: 0.5621 (0.5532)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6645 (3.6125)  time: 0.1658  data: 0.0032  max mem: 5591
Epoch: [86] (train)  [49/50]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000003  loss: 0.5610 (0.5539)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7406 (3.6246)  time: 0.1259  data: 0.0019  max mem: 5591
Epoch: [86] (train) Total time: 0:00:08 (0.1701 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000003  loss: 0.5610 (0.5730)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7406 (3.6246)
Epoch: [86] (test)  [0/6]  eta: 0:00:02  loss: 0.9620 (0.9620)  time: 0.3609  data: 0.3277  max mem: 5591
Epoch: [86] (test)  [5/6]  eta: 0:00:00  loss: 0.9620 (1.0571)  time: 0.1353  data: 0.1026  max mem: 5591
Epoch: [86] (test) Total time: 0:00:00 (0.1425 s / it)
Averaged stats: loss: 0.9620 (1.0947)
Epoch: [87] (train)  [ 0/50]  eta: 0:00:44  lr: 0.000003  min_lr: 0.000003  loss: 0.4967 (0.4967)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2429 (3.2429)  time: 0.8865  data: 0.6220  max mem: 5591
Epoch: [87] (train)  [10/50]  eta: 0:00:09  lr: 0.000003  min_lr: 0.000003  loss: 0.5790 (0.5782)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6606 (3.5742)  time: 0.2344  data: 0.0828  max mem: 5591
Epoch: [87] (train)  [20/50]  eta: 0:00:06  lr: 0.000003  min_lr: 0.000003  loss: 0.5692 (0.5723)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6907 (3.7137)  time: 0.1692  data: 0.0163  max mem: 5591
Epoch: [87] (train)  [30/50]  eta: 0:00:03  lr: 0.000003  min_lr: 0.000003  loss: 0.5647 (0.5650)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7234 (4.1130)  time: 0.1714  data: 0.0027  max mem: 5591
Epoch: [87] (train)  [40/50]  eta: 0:00:01  lr: 0.000003  min_lr: 0.000003  loss: 0.5539 (0.5609)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6772 (4.0980)  time: 0.1674  data: 0.0025  max mem: 5591
Epoch: [87] (train)  [49/50]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000003  loss: 0.5228 (0.5587)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8577 (4.0439)  time: 0.1282  data: 0.0025  max mem: 5591
Epoch: [87] (train) Total time: 0:00:08 (0.1700 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000003  loss: 0.5228 (0.5669)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8577 (4.0439)
Epoch: [87] (test)  [0/6]  eta: 0:00:03  loss: 1.1629 (1.1629)  time: 0.5512  data: 0.5293  max mem: 5591
Epoch: [87] (test)  [5/6]  eta: 0:00:00  loss: 0.9385 (1.0309)  time: 0.1336  data: 0.1082  max mem: 5591
Epoch: [87] (test) Total time: 0:00:00 (0.1428 s / it)
Averaged stats: loss: 0.9385 (1.0931)
Epoch: [88] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000003  min_lr: 0.000003  loss: 0.5495 (0.5495)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3488 (3.3488)  time: 0.9810  data: 0.3397  max mem: 5591
Epoch: [88] (train)  [10/50]  eta: 0:00:09  lr: 0.000003  min_lr: 0.000003  loss: 0.5966 (0.6040)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5324 (3.7593)  time: 0.2279  data: 0.0323  max mem: 5591
Epoch: [88] (train)  [20/50]  eta: 0:00:06  lr: 0.000003  min_lr: 0.000003  loss: 0.5870 (0.5963)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5324 (3.6799)  time: 0.1642  data: 0.0029  max mem: 5591
Epoch: [88] (train)  [30/50]  eta: 0:00:03  lr: 0.000003  min_lr: 0.000003  loss: 0.5746 (0.5879)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5430 (3.6833)  time: 0.1616  data: 0.0057  max mem: 5591
Epoch: [88] (train)  [40/50]  eta: 0:00:01  lr: 0.000003  min_lr: 0.000003  loss: 0.5283 (0.5708)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5816 (3.6820)  time: 0.1629  data: 0.0058  max mem: 5591
Epoch: [88] (train)  [49/50]  eta: 0:00:00  lr: 0.000003  min_lr: 0.000003  loss: 0.5283 (0.5698)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5944 (3.7052)  time: 0.1347  data: 0.0025  max mem: 5591
Epoch: [88] (train) Total time: 0:00:08 (0.1676 s / it)
Averaged stats: lr: 0.000003  min_lr: 0.000003  loss: 0.5283 (0.5663)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5944 (3.7052)
Epoch: [88] (test)  [0/6]  eta: 0:00:04  loss: 1.0989 (1.0989)  time: 0.7118  data: 0.6890  max mem: 5591
Epoch: [88] (test)  [5/6]  eta: 0:00:00  loss: 1.0898 (1.1127)  time: 0.1633  data: 0.1407  max mem: 5591
Epoch: [88] (test) Total time: 0:00:01 (0.1706 s / it)
Averaged stats: loss: 1.0898 (1.1518)
Epoch: [89] (train)  [ 0/50]  eta: 0:00:52  lr: 0.000003  min_lr: 0.000003  loss: 0.5691 (0.5691)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6021 (3.6021)  time: 1.0531  data: 0.5524  max mem: 5591
Epoch: [89] (train)  [10/50]  eta: 0:00:10  lr: 0.000003  min_lr: 0.000003  loss: 0.5875 (0.5779)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6021 (3.6268)  time: 0.2513  data: 0.0529  max mem: 5591
Epoch: [89] (train)  [20/50]  eta: 0:00:05  lr: 0.000003  min_lr: 0.000003  loss: 0.5659 (0.5672)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4728 (3.6193)  time: 0.1509  data: 0.0018  max mem: 5591
Epoch: [89] (train)  [30/50]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000002  loss: 0.5663 (0.5696)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4518 (3.6015)  time: 0.1536  data: 0.0024  max mem: 5591
Epoch: [89] (train)  [40/50]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000002  loss: 0.5725 (0.5801)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5578 (3.6286)  time: 0.1761  data: 0.0042  max mem: 5591
Epoch: [89] (train)  [49/50]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000002  loss: 0.5461 (0.5704)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3702 (3.5842)  time: 0.1314  data: 0.0027  max mem: 5591
Epoch: [89] (train) Total time: 0:00:08 (0.1691 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000002  loss: 0.5461 (0.5651)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3702 (3.5842)
Epoch: [89] (test)  [0/6]  eta: 0:00:03  loss: 0.8515 (0.8515)  time: 0.5912  data: 0.5455  max mem: 5591
Epoch: [89] (test)  [5/6]  eta: 0:00:00  loss: 0.9939 (1.0469)  time: 0.1428  data: 0.1109  max mem: 5591
Epoch: [89] (test) Total time: 0:00:00 (0.1511 s / it)
Averaged stats: loss: 0.9939 (1.0875)
Epoch: [90] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000002  min_lr: 0.000002  loss: 0.5563 (0.5563)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7425 (3.7425)  time: 1.0326  data: 0.3280  max mem: 5591
Epoch: [90] (train)  [10/50]  eta: 0:00:09  lr: 0.000002  min_lr: 0.000002  loss: 0.5679 (0.5939)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5433 (3.6785)  time: 0.2365  data: 0.0323  max mem: 5591
Epoch: [90] (train)  [20/50]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000002  loss: 0.5615 (0.5927)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5433 (3.5982)  time: 0.1574  data: 0.0023  max mem: 5591
Epoch: [90] (train)  [30/50]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000002  loss: 0.5465 (0.5813)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3801 (3.5399)  time: 0.1604  data: 0.0039  max mem: 5591
Epoch: [90] (train)  [40/50]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000002  loss: 0.5435 (0.5811)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2851 (3.5164)  time: 0.1554  data: 0.0043  max mem: 5591
Epoch: [90] (train)  [49/50]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000002  loss: 0.5438 (0.5751)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4915 (3.5044)  time: 0.1248  data: 0.0016  max mem: 5591
Epoch: [90] (train) Total time: 0:00:08 (0.1644 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000002  loss: 0.5438 (0.5619)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4915 (3.5044)
Epoch: [90] (test)  [0/6]  eta: 0:00:02  loss: 0.7520 (0.7520)  time: 0.3684  data: 0.3461  max mem: 5591
Epoch: [90] (test)  [5/6]  eta: 0:00:00  loss: 0.9144 (1.0797)  time: 0.1464  data: 0.1232  max mem: 5591
Epoch: [90] (test) Total time: 0:00:00 (0.1530 s / it)
Averaged stats: loss: 0.9144 (1.1220)
Epoch: [91] (train)  [ 0/50]  eta: 0:00:39  lr: 0.000002  min_lr: 0.000002  loss: 0.4741 (0.4741)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0697 (4.0697)  time: 0.7834  data: 0.5749  max mem: 5591
Epoch: [91] (train)  [10/50]  eta: 0:00:09  lr: 0.000002  min_lr: 0.000002  loss: 0.5827 (0.5733)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5706 (3.7011)  time: 0.2376  data: 0.0563  max mem: 5591
Epoch: [91] (train)  [20/50]  eta: 0:00:06  lr: 0.000002  min_lr: 0.000002  loss: 0.5827 (0.5736)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5172 (3.6002)  time: 0.1755  data: 0.0025  max mem: 5591
Epoch: [91] (train)  [30/50]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000002  loss: 0.5754 (0.5719)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4050 (3.5504)  time: 0.1565  data: 0.0012  max mem: 5591
Epoch: [91] (train)  [40/50]  eta: 0:00:01  lr: 0.000002  min_lr: 0.000002  loss: 0.5589 (0.5660)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3631 (3.4929)  time: 0.1546  data: 0.0018  max mem: 5591
Epoch: [91] (train)  [49/50]  eta: 0:00:00  lr: 0.000002  min_lr: 0.000002  loss: 0.5691 (0.5721)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3643 (3.5220)  time: 0.1377  data: 0.0011  max mem: 5591
Epoch: [91] (train) Total time: 0:00:08 (0.1670 s / it)
Averaged stats: lr: 0.000002  min_lr: 0.000002  loss: 0.5691 (0.5626)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3643 (3.5220)
Epoch: [91] (test)  [0/6]  eta: 0:00:03  loss: 0.9795 (0.9795)  time: 0.6249  data: 0.5910  max mem: 5591
Epoch: [91] (test)  [5/6]  eta: 0:00:00  loss: 1.0483 (1.1319)  time: 0.1534  data: 0.1243  max mem: 5591
Epoch: [91] (test) Total time: 0:00:00 (0.1621 s / it)
Averaged stats: loss: 1.0483 (1.1326)
Epoch: [92] (train)  [ 0/50]  eta: 0:00:51  lr: 0.000002  min_lr: 0.000002  loss: 0.6944 (0.6944)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7507 (3.7507)  time: 1.0203  data: 0.3478  max mem: 5591
Epoch: [92] (train)  [10/50]  eta: 0:00:09  lr: 0.000002  min_lr: 0.000002  loss: 0.5729 (0.5846)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.7376 (3.7477)  time: 0.2424  data: 0.0418  max mem: 5591
Epoch: [92] (train)  [20/50]  eta: 0:00:05  lr: 0.000002  min_lr: 0.000002  loss: 0.5613 (0.5686)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4239 (3.5826)  time: 0.1552  data: 0.0104  max mem: 5591
Epoch: [92] (train)  [30/50]  eta: 0:00:03  lr: 0.000002  min_lr: 0.000002  loss: 0.5569 (0.5680)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3705 (3.5026)  time: 0.1613  data: 0.0083  max mem: 5591
Epoch: [92] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5592 (0.5702)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3654 (3.5125)  time: 0.1721  data: 0.0060  max mem: 5591
Epoch: [92] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5610 (0.5676)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3654 (3.4970)  time: 0.1285  data: 0.0033  max mem: 5591
Epoch: [92] (train) Total time: 0:00:08 (0.1679 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5610 (0.5602)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3654 (3.4970)
Epoch: [92] (test)  [0/6]  eta: 0:00:02  loss: 0.9006 (0.9006)  time: 0.3353  data: 0.3131  max mem: 5591
Epoch: [92] (test)  [5/6]  eta: 0:00:00  loss: 1.0132 (1.1207)  time: 0.1117  data: 0.0834  max mem: 5591
Epoch: [92] (test) Total time: 0:00:00 (0.1207 s / it)
Averaged stats: loss: 1.0132 (1.0405)
Epoch: [93] (train)  [ 0/50]  eta: 0:00:49  lr: 0.000001  min_lr: 0.000001  loss: 0.6886 (0.6886)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8697 (3.8697)  time: 0.9893  data: 0.7688  max mem: 5591
Epoch: [93] (train)  [10/50]  eta: 0:00:09  lr: 0.000001  min_lr: 0.000001  loss: 0.5776 (0.5924)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2830 (3.4291)  time: 0.2288  data: 0.0713  max mem: 5591
Epoch: [93] (train)  [20/50]  eta: 0:00:05  lr: 0.000001  min_lr: 0.000001  loss: 0.5604 (0.5834)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2830 (3.4564)  time: 0.1524  data: 0.0012  max mem: 5591
Epoch: [93] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5472 (0.5641)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3723 (3.4583)  time: 0.1655  data: 0.0026  max mem: 5591
Epoch: [93] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5273 (0.5641)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6037 (3.4922)  time: 0.1649  data: 0.0053  max mem: 5591
Epoch: [93] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5255 (0.5577)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3975 (3.4385)  time: 0.1280  data: 0.0034  max mem: 5591
Epoch: [93] (train) Total time: 0:00:08 (0.1662 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5255 (0.5594)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3975 (3.4385)
Epoch: [93] (test)  [0/6]  eta: 0:00:03  loss: 0.9062 (0.9062)  time: 0.6062  data: 0.5583  max mem: 5591
Epoch: [93] (test)  [5/6]  eta: 0:00:00  loss: 0.9912 (1.0854)  time: 0.1485  data: 0.1169  max mem: 5591
Epoch: [93] (test) Total time: 0:00:00 (0.1556 s / it)
Averaged stats: loss: 0.9912 (1.1541)
Epoch: [94] (train)  [ 0/50]  eta: 0:00:40  lr: 0.000001  min_lr: 0.000001  loss: 0.5800 (0.5800)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1448 (3.1448)  time: 0.8044  data: 0.6690  max mem: 5591
Epoch: [94] (train)  [10/50]  eta: 0:00:09  lr: 0.000001  min_lr: 0.000001  loss: 0.5561 (0.5610)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3032 (3.3113)  time: 0.2363  data: 0.0685  max mem: 5591
Epoch: [94] (train)  [20/50]  eta: 0:00:06  lr: 0.000001  min_lr: 0.000001  loss: 0.5546 (0.5548)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3032 (3.2721)  time: 0.1714  data: 0.0059  max mem: 5591
Epoch: [94] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5534 (0.5550)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4044 (3.4007)  time: 0.1599  data: 0.0032  max mem: 5591
Epoch: [94] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5330 (0.5545)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4631 (3.3948)  time: 0.1488  data: 0.0032  max mem: 5591
Epoch: [94] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5335 (0.5587)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3104 (3.4149)  time: 0.1241  data: 0.0029  max mem: 5591
Epoch: [94] (train) Total time: 0:00:08 (0.1640 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5335 (0.5573)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3104 (3.4149)
Epoch: [94] (test)  [0/6]  eta: 0:00:04  loss: 0.9739 (0.9739)  time: 0.7197  data: 0.6258  max mem: 5591
Epoch: [94] (test)  [5/6]  eta: 0:00:00  loss: 1.0560 (1.1781)  time: 0.1613  data: 0.1206  max mem: 5591
Epoch: [94] (test) Total time: 0:00:01 (0.1694 s / it)
Averaged stats: loss: 1.0560 (1.0979)
Epoch: [95] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000001  min_lr: 0.000001  loss: 0.5413 (0.5413)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.6486 (3.6486)  time: 0.9313  data: 0.5554  max mem: 5591
Epoch: [95] (train)  [10/50]  eta: 0:00:08  lr: 0.000001  min_lr: 0.000001  loss: 0.5413 (0.5567)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5090 (3.4644)  time: 0.2246  data: 0.0527  max mem: 5591
Epoch: [95] (train)  [20/50]  eta: 0:00:05  lr: 0.000001  min_lr: 0.000001  loss: 0.5298 (0.5511)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3435 (3.3761)  time: 0.1625  data: 0.0034  max mem: 5591
Epoch: [95] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5128 (0.5419)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2512 (3.3690)  time: 0.1670  data: 0.0061  max mem: 5591
Epoch: [95] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5573 (0.5591)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3531 (3.3884)  time: 0.1547  data: 0.0049  max mem: 5591
Epoch: [95] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5819 (0.5609)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5332 (3.3922)  time: 0.1298  data: 0.0015  max mem: 5591
Epoch: [95] (train) Total time: 0:00:08 (0.1669 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5819 (0.5560)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5332 (3.3922)
Epoch: [95] (test)  [0/6]  eta: 0:00:04  loss: 1.2092 (1.2092)  time: 0.7331  data: 0.6778  max mem: 5591
Epoch: [95] (test)  [5/6]  eta: 0:00:00  loss: 1.1547 (1.1182)  time: 0.1593  data: 0.1327  max mem: 5591
Epoch: [95] (test) Total time: 0:00:00 (0.1649 s / it)
Averaged stats: loss: 1.1547 (1.0428)
Epoch: [96] (train)  [ 0/50]  eta: 0:00:50  lr: 0.000001  min_lr: 0.000001  loss: 0.5370 (0.5370)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3555 (3.3555)  time: 1.0102  data: 0.3478  max mem: 5591
Epoch: [96] (train)  [10/50]  eta: 0:00:09  lr: 0.000001  min_lr: 0.000001  loss: 0.5656 (0.5513)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3555 (3.4296)  time: 0.2399  data: 0.0350  max mem: 5591
Epoch: [96] (train)  [20/50]  eta: 0:00:05  lr: 0.000001  min_lr: 0.000001  loss: 0.5233 (0.5399)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3933 (3.5025)  time: 0.1571  data: 0.0036  max mem: 5591
Epoch: [96] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5144 (0.5323)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4854 (3.4838)  time: 0.1669  data: 0.0033  max mem: 5591
Epoch: [96] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5440 (0.5446)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3384 (3.4850)  time: 0.1744  data: 0.0028  max mem: 5591
Epoch: [96] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5922 (0.5537)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3384 (3.4816)  time: 0.1284  data: 0.0018  max mem: 5591
Epoch: [96] (train) Total time: 0:00:08 (0.1697 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5922 (0.5596)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3384 (3.4816)
Epoch: [96] (test)  [0/6]  eta: 0:00:04  loss: 0.7160 (0.7160)  time: 0.7470  data: 0.7087  max mem: 5591
Epoch: [96] (test)  [5/6]  eta: 0:00:00  loss: 0.9794 (1.1024)  time: 0.1581  data: 0.1351  max mem: 5591
Epoch: [96] (test) Total time: 0:00:00 (0.1637 s / it)
Averaged stats: loss: 0.9794 (1.0938)
Epoch: [97] (train)  [ 0/50]  eta: 0:00:46  lr: 0.000001  min_lr: 0.000001  loss: 0.4771 (0.4771)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3426 (3.3426)  time: 0.9218  data: 0.5463  max mem: 5591
Epoch: [97] (train)  [10/50]  eta: 0:00:09  lr: 0.000001  min_lr: 0.000001  loss: 0.5780 (0.5778)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3953 (3.4057)  time: 0.2356  data: 0.0584  max mem: 5591
Epoch: [97] (train)  [20/50]  eta: 0:00:06  lr: 0.000001  min_lr: 0.000001  loss: 0.5728 (0.5652)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3953 (3.4220)  time: 0.1670  data: 0.0051  max mem: 5591
Epoch: [97] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5704 (0.5714)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3407 (3.4565)  time: 0.1661  data: 0.0029  max mem: 5591
Epoch: [97] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5633 (0.5663)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.2353 (3.3961)  time: 0.1626  data: 0.0050  max mem: 5591
Epoch: [97] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5645 (0.5677)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1140 (3.4128)  time: 0.1311  data: 0.0032  max mem: 5591
Epoch: [97] (train) Total time: 0:00:08 (0.1686 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5645 (0.5566)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.1140 (3.4128)
Epoch: [97] (test)  [0/6]  eta: 0:00:03  loss: 1.0522 (1.0522)  time: 0.6551  data: 0.6208  max mem: 5591
Epoch: [97] (test)  [5/6]  eta: 0:00:00  loss: 1.0522 (1.1723)  time: 0.1541  data: 0.1245  max mem: 5591
Epoch: [97] (test) Total time: 0:00:00 (0.1610 s / it)
Averaged stats: loss: 1.0522 (1.1457)
Epoch: [98] (train)  [ 0/50]  eta: 0:00:45  lr: 0.000001  min_lr: 0.000001  loss: 0.4899 (0.4899)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.8612 (3.8612)  time: 0.9184  data: 0.6831  max mem: 5591
Epoch: [98] (train)  [10/50]  eta: 0:00:09  lr: 0.000001  min_lr: 0.000001  loss: 0.5703 (0.5537)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.5255 (3.4659)  time: 0.2344  data: 0.0677  max mem: 5591
Epoch: [98] (train)  [20/50]  eta: 0:00:06  lr: 0.000001  min_lr: 0.000001  loss: 0.5676 (0.5661)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4427 (3.4910)  time: 0.1713  data: 0.0060  max mem: 5591
Epoch: [98] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5264 (0.5423)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4228 (3.4026)  time: 0.1686  data: 0.0054  max mem: 5591
Epoch: [98] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5163 (0.5453)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4228 (3.4225)  time: 0.1593  data: 0.0034  max mem: 5591
Epoch: [98] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5419 (0.5429)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4642 (3.4120)  time: 0.1294  data: 0.0019  max mem: 5591
Epoch: [98] (train) Total time: 0:00:08 (0.1700 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5419 (0.5586)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4642 (3.4120)
Epoch: [98] (test)  [0/6]  eta: 0:00:02  loss: 0.8179 (0.8179)  time: 0.3798  data: 0.3427  max mem: 5591
Epoch: [98] (test)  [5/6]  eta: 0:00:00  loss: 1.0921 (1.1253)  time: 0.1461  data: 0.1110  max mem: 5591
Epoch: [98] (test) Total time: 0:00:00 (0.1521 s / it)
Averaged stats: loss: 1.0921 (1.1605)
Epoch: [99] (train)  [ 0/50]  eta: 0:00:48  lr: 0.000001  min_lr: 0.000001  loss: 0.5352 (0.5352)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 4.0431 (4.0431)  time: 0.9739  data: 0.3420  max mem: 5591
Epoch: [99] (train)  [10/50]  eta: 0:00:09  lr: 0.000001  min_lr: 0.000001  loss: 0.5603 (0.5461)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4617 (3.5740)  time: 0.2377  data: 0.0460  max mem: 5591
Epoch: [99] (train)  [20/50]  eta: 0:00:06  lr: 0.000001  min_lr: 0.000001  loss: 0.5258 (0.5375)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.4053 (3.5014)  time: 0.1688  data: 0.0113  max mem: 5591
Epoch: [99] (train)  [30/50]  eta: 0:00:03  lr: 0.000001  min_lr: 0.000001  loss: 0.5229 (0.5511)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3139 (3.4848)  time: 0.1629  data: 0.0045  max mem: 5591
Epoch: [99] (train)  [40/50]  eta: 0:00:01  lr: 0.000001  min_lr: 0.000001  loss: 0.5365 (0.5511)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3139 (3.4441)  time: 0.1477  data: 0.0023  max mem: 5591
Epoch: [99] (train)  [49/50]  eta: 0:00:00  lr: 0.000001  min_lr: 0.000001  loss: 0.5431 (0.5553)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3724 (3.4382)  time: 0.1288  data: 0.0012  max mem: 5591
Epoch: [99] (train) Total time: 0:00:08 (0.1674 s / it)
Averaged stats: lr: 0.000001  min_lr: 0.000001  loss: 0.5431 (0.5520)  loss_scale: 2048.0000 (2048.0000)  weight_decay: 0.0500 (0.0500)  grad_norm: 3.3724 (3.4382)
Epoch: [99] (test)  [0/6]  eta: 0:00:03  loss: 0.9880 (0.9880)  time: 0.6304  data: 0.6084  max mem: 5591
Epoch: [99] (test)  [5/6]  eta: 0:00:00  loss: 0.9880 (0.9993)  time: 0.1435  data: 0.1126  max mem: 5591
Epoch: [99] (test) Total time: 0:00:00 (0.1540 s / it)
Averaged stats: loss: 0.9880 (1.1738)
Saving model at epoch 100 in /mnt/home/bregaldosaintblancard/Projects/Foundation Models/VideoMAE_comparison/ceph/pdebench_finetuning/k400_s/k400_s_turb_224_test/
Training time 0:15:42
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:              epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:       n_parameters ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:          test_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    train_grad_norm  ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         train_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train_loss_scale ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           train_lr ‚ñÇ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train_min_lr ‚ñÇ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_weight_decay ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:              epoch 99
wandb:       n_parameters 24029376
wandb:          test_loss 1.17377
wandb:    train_grad_norm 3.43819
wandb:         train_loss 0.552
wandb:   train_loss_scale 2048.0
wandb:           train_lr 0.0
wandb:       train_min_lr 0.0
wandb: train_weight_decay 0.05
wandb: 
wandb: üöÄ View run k400_s_turb_224_test at: https://wandb.ai/flatiron-scipt/videomae_finetuning/runs/gsc5kaof
wandb: Ô∏è‚ö° View job at https://wandb.ai/flatiron-scipt/videomae_finetuning/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0MjEyMTE0/version_details/v4
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230902_123755-gsc5kaof/logs
